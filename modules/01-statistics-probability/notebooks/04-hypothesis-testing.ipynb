{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/buildLittleWorlds/ml-math-with-densworld/blob/main/modules/01-statistics-probability/notebooks/04-hypothesis-testing.ipynb)\n",
    "\n",
    "# Lesson 4: Hypothesis Testing\n",
    "\n",
    "*\"In the Archives, victory is not decided by truth, but by the margin of doubt. A Stone School philosopher may seem undefeated—but sample fifty of their debates, and you'll find that chance alone could explain their record.\"*  \n",
    "— Mink Pavar, testimony before the Senate Inquiry\n",
    "\n",
    "---\n",
    "\n",
    "## The Core Problem\n",
    "\n",
    "The Capital Archives preserve records of 256 formal debates between philosophical schools. Looking at the raw numbers, the Stone School appears dominant—their scholars seem to win more often. But is this difference **real**, or could it be explained by random chance?\n",
    "\n",
    "This is the fundamental question of **hypothesis testing**: when we see a pattern in our data, is it signal or noise?\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this lesson, you will:\n",
    "1. Understand the null hypothesis as a \"default assumption\"\n",
    "2. Interpret p-values correctly (and avoid common misconceptions)\n",
    "3. Recognize Type I and Type II errors and their tradeoffs\n",
    "4. Detect and avoid the multiple comparisons trap (p-hacking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Nice plotting defaults\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "# Colab-ready data loading\n",
    "BASE_URL = \"https://raw.githubusercontent.com/buildLittleWorlds/ml-math-with-densworld/main/data/\"\n",
    "\n",
    "# Load the scholar debates dataset\n",
    "debates = pd.read_csv(BASE_URL + \"scholar_debates.csv\")\n",
    "\n",
    "print(f\"Loaded {len(debates)} debate records\")\n",
    "print(f\"Years covered: {debates['year'].min()} - {debates['year'].max()}\")\n",
    "debates.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: The Three Schools\n",
    "\n",
    "The Capital's intellectual life is dominated by three philosophical schools:\n",
    "\n",
    "- **Stone School**: Emphasizes permanence, structure, and tradition\n",
    "- **Water School**: Values flexibility, adaptation, and flow\n",
    "- **Pebble School**: Seeks compromise, often dismissed as \"fence-sitters\"\n",
    "\n",
    "Let's examine the win rates by school:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to determine if a school won a debate\n",
    "def calculate_win_rate(df, school):\n",
    "    \"\"\"Calculate the win rate for a given school.\"\"\"\n",
    "    # Debates where this school participated as scholar_a\n",
    "    as_a = df[df['scholar_a_school'] == school]\n",
    "    wins_as_a = (as_a['outcome'] == 'victory_a').sum()\n",
    "    \n",
    "    # Debates where this school participated as scholar_b\n",
    "    as_b = df[df['scholar_b_school'] == school]\n",
    "    wins_as_b = (as_b['outcome'] == 'victory_b').sum()\n",
    "    \n",
    "    total_debates = len(as_a) + len(as_b)\n",
    "    total_wins = wins_as_a + wins_as_b\n",
    "    \n",
    "    # Exclude draws for win rate calculation\n",
    "    draws_as_a = (as_a['outcome'] == 'draw').sum()\n",
    "    draws_as_b = (as_b['outcome'] == 'draw').sum()\n",
    "    decisive_debates = total_debates - draws_as_a - draws_as_b\n",
    "    \n",
    "    return {\n",
    "        'school': school,\n",
    "        'total_debates': total_debates,\n",
    "        'wins': total_wins,\n",
    "        'decisive_debates': decisive_debates,\n",
    "        'win_rate': total_wins / decisive_debates if decisive_debates > 0 else 0\n",
    "    }\n",
    "\n",
    "# Calculate win rates for each school\n",
    "schools = ['stone_school', 'water_school', 'pebble_school']\n",
    "win_rates = pd.DataFrame([calculate_win_rate(debates, school) for school in schools])\n",
    "\n",
    "print(\"Scholar Debate Win Rates by School\")\n",
    "print(\"=\" * 60)\n",
    "for _, row in win_rates.iterrows():\n",
    "    school_name = row['school'].replace('_', ' ').title()\n",
    "    print(f\"{school_name:20} | {row['wins']:3d} wins / {row['decisive_debates']:3d} decisive debates | Win Rate: {row['win_rate']:.1%}\")\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "colors = ['#8B4513', '#4169E1', '#808080']  # Brown, Blue, Gray\n",
    "bars = ax.bar(win_rates['school'].str.replace('_', ' ').str.title(), \n",
    "              win_rates['win_rate'], color=colors, edgecolor='black')\n",
    "ax.axhline(0.5, color='red', linestyle='--', linewidth=2, label='50% (fair odds)')\n",
    "ax.set_ylabel('Win Rate', fontsize=12)\n",
    "ax.set_title('Scholar Debate Win Rates by Philosophical School', fontsize=14)\n",
    "ax.set_ylim(0, 0.7)\n",
    "ax.legend()\n",
    "\n",
    "# Add counts on bars\n",
    "for bar, (_, row) in zip(bars, win_rates.iterrows()):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, \n",
    "            f\"n={row['decisive_debates']}\", ha='center', fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: The Null Hypothesis\n",
    "\n",
    "Looking at the data, one school appears to outperform the others. But before we declare a winner, we must ask: **could this difference arise by chance alone?**\n",
    "\n",
    "This is where the **null hypothesis (H₀)** comes in. The null hypothesis represents the \"boring\" or \"default\" assumption—typically that there is no real effect.\n",
    "\n",
    "### For our question:\n",
    "- **H₀ (Null)**: The school has a 50% win rate (no advantage)\n",
    "- **H₁ (Alternative)**: The school has a win rate ≠ 50% (real advantage or disadvantage)\n",
    "\n",
    "We assume H₀ is true, then calculate how likely we'd be to observe our data under that assumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's test: Is Stone School's win rate significantly different from 50%?\n",
    "stone_stats = win_rates[win_rates['school'] == 'stone_school'].iloc[0]\n",
    "n_debates = int(stone_stats['decisive_debates'])\n",
    "n_wins = int(stone_stats['wins'])\n",
    "observed_rate = stone_stats['win_rate']\n",
    "\n",
    "print(\"Testing Stone School Dominance\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Observed: {n_wins} wins out of {n_debates} decisive debates\")\n",
    "print(f\"Observed win rate: {observed_rate:.1%}\")\n",
    "print(f\"\\nH₀: True win rate = 50%\")\n",
    "print(f\"H₁: True win rate ≠ 50%\")\n",
    "\n",
    "# Under H₀, what's the probability of seeing this many or more wins?\n",
    "# This is a binomial test\n",
    "# Use scipy.stats.binom_test (or binomtest in newer scipy)\n",
    "try:\n",
    "    from scipy.stats import binomtest\n",
    "    result = binomtest(n_wins, n_debates, 0.5, alternative='two-sided')\n",
    "    p_value = result.pvalue\n",
    "except ImportError:\n",
    "    # Fallback for older scipy\n",
    "    p_value = stats.binom_test(n_wins, n_debates, 0.5)\n",
    "\n",
    "print(f\"\\nP-value: {p_value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: The P-Value — What It Really Means\n",
    "\n",
    "The **p-value** is perhaps the most misunderstood concept in statistics. Let's be clear about what it is and isn't:\n",
    "\n",
    "### What the p-value IS:\n",
    "> The probability of observing data as extreme as (or more extreme than) what we saw, **assuming the null hypothesis is true**.\n",
    "\n",
    "### What the p-value is NOT:\n",
    "- ❌ NOT the probability that H₀ is true\n",
    "- ❌ NOT the probability that H₁ is true\n",
    "- ❌ NOT the probability that the result is due to chance\n",
    "\n",
    "### Intuition: The Surprise Interpretation\n",
    "\n",
    "Think of the p-value as a measure of **surprise**. If the null hypothesis were true, how surprised would we be to see this data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the p-value with a simulation\n",
    "# If Stone School truly had a 50% win rate, what would their win counts look like?\n",
    "\n",
    "n_simulations = 10000\n",
    "simulated_wins = np.random.binomial(n_debates, 0.5, n_simulations)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Histogram of simulated wins under H₀\n",
    "counts, bins, _ = ax.hist(simulated_wins, bins=range(0, n_debates+2), \n",
    "                          color='steelblue', edgecolor='black', alpha=0.7,\n",
    "                          density=True, align='left')\n",
    "\n",
    "# Mark the observed value and region more extreme\n",
    "ax.axvline(n_wins, color='red', linewidth=3, linestyle='--', \n",
    "           label=f'Observed: {n_wins} wins')\n",
    "\n",
    "# Shade the \"as extreme or more\" regions\n",
    "extreme_low = n_debates - n_wins  # Mirror for two-sided test\n",
    "for i, count in enumerate(counts):\n",
    "    if bins[i] >= n_wins or bins[i] <= extreme_low:\n",
    "        ax.bar(bins[i], count, color='red', alpha=0.5, edgecolor='black', width=0.8)\n",
    "\n",
    "ax.set_xlabel('Number of Wins (out of {})'.format(n_debates), fontsize=12)\n",
    "ax.set_ylabel('Probability', fontsize=12)\n",
    "ax.set_title('Distribution of Wins Under H₀ (50% Win Rate)\\nRed region = p-value', fontsize=14)\n",
    "ax.legend(fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate empirical p-value from simulation\n",
    "empirical_p = np.mean((simulated_wins >= n_wins) | (simulated_wins <= extreme_low))\n",
    "print(f\"Empirical p-value (from simulation): {empirical_p:.4f}\")\n",
    "print(f\"Exact p-value (from binomial test): {p_value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Statistical Significance and Alpha\n",
    "\n",
    "By convention, we compare the p-value to a threshold called **α (alpha)**, typically set at 0.05.\n",
    "\n",
    "- If p < α: **Reject H₀** — the result is \"statistically significant\"\n",
    "- If p ≥ α: **Fail to reject H₀** — we don't have enough evidence\n",
    "\n",
    "### Important Caveats:\n",
    "\n",
    "1. **α = 0.05 is arbitrary** — it's just a convention from the 1920s\n",
    "2. **\"Not significant\" ≠ \"No effect\"** — it means we can't tell\n",
    "3. **\"Significant\" ≠ \"Important\"** — a tiny effect can be statistically significant with enough data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test all schools\n",
    "print(\"Hypothesis Tests for Each School\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'School':<20} {'Win Rate':<12} {'p-value':<12} {'Significant?':<12}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "alpha = 0.05\n",
    "\n",
    "for _, row in win_rates.iterrows():\n",
    "    n = int(row['decisive_debates'])\n",
    "    wins = int(row['wins'])\n",
    "    \n",
    "    try:\n",
    "        result = binomtest(wins, n, 0.5, alternative='two-sided')\n",
    "        p = result.pvalue\n",
    "    except:\n",
    "        p = stats.binom_test(wins, n, 0.5)\n",
    "    \n",
    "    sig = \"Yes\" if p < alpha else \"No\"\n",
    "    school_name = row['school'].replace('_', ' ').title()\n",
    "    print(f\"{school_name:<20} {row['win_rate']:<12.1%} {p:<12.4f} {sig:<12}\")\n",
    "\n",
    "print(f\"\\n(Using α = {alpha})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Type I and Type II Errors\n",
    "\n",
    "When making decisions based on hypothesis tests, we can make two kinds of mistakes:\n",
    "\n",
    "| | H₀ is Actually True | H₀ is Actually False |\n",
    "|---|---|---|\n",
    "| **Reject H₀** | Type I Error (False Positive) | Correct (True Positive) |\n",
    "| **Fail to Reject H₀** | Correct (True Negative) | Type II Error (False Negative) |\n",
    "\n",
    "### In the Archives context:\n",
    "\n",
    "- **Type I Error**: Declaring Stone School superior when they're actually just lucky\n",
    "- **Type II Error**: Failing to detect a real advantage due to too little data\n",
    "\n",
    "### The Tradeoff:\n",
    "- Lower α → Fewer false positives, but more false negatives\n",
    "- Higher α → Fewer false negatives, but more false positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the tradeoff with simulation\n",
    "# True scenario: Water School actually has a 55% win rate (small real advantage)\n",
    "\n",
    "TRUE_RATE = 0.55  # Small real advantage\n",
    "n_debates_sim = 50  # Sample size\n",
    "n_experiments = 10000\n",
    "\n",
    "# Track decisions at different alpha levels\n",
    "alphas = [0.01, 0.05, 0.10, 0.20]\n",
    "results = {alpha: {'reject': 0, 'fail_reject': 0} for alpha in alphas}\n",
    "\n",
    "for _ in range(n_experiments):\n",
    "    # Simulate debates with true 55% win rate\n",
    "    wins = np.random.binomial(n_debates_sim, TRUE_RATE)\n",
    "    \n",
    "    # Calculate p-value\n",
    "    try:\n",
    "        p = binomtest(wins, n_debates_sim, 0.5, alternative='two-sided').pvalue\n",
    "    except:\n",
    "        p = stats.binom_test(wins, n_debates_sim, 0.5)\n",
    "    \n",
    "    # Make decision at each alpha\n",
    "    for alpha in alphas:\n",
    "        if p < alpha:\n",
    "            results[alpha]['reject'] += 1\n",
    "        else:\n",
    "            results[alpha]['fail_reject'] += 1\n",
    "\n",
    "print(f\"True win rate: {TRUE_RATE:.0%} (there IS a real effect)\")\n",
    "print(f\"Sample size: {n_debates_sim} debates per experiment\")\n",
    "print(f\"\\nDetection rates (statistical power) at different α levels:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Alpha':<10} {'Detect Effect':<20} {'Miss Effect (Type II)':<20}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for alpha in alphas:\n",
    "    power = results[alpha]['reject'] / n_experiments\n",
    "    miss_rate = results[alpha]['fail_reject'] / n_experiments\n",
    "    print(f\"{alpha:<10} {power:<20.1%} {miss_rate:<20.1%}\")\n",
    "\n",
    "print(f\"\\n⚠️  With a small effect (55% vs 50%) and limited data (n={n_debates_sim}),\")\n",
    "print(f\"   we often fail to detect the real difference!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: The Multiple Comparisons Trap\n",
    "\n",
    "### P-Hacking: The Scholar's Temptation\n",
    "\n",
    "Mink Pavar, the forger, was also known for his statistical manipulations. He understood that if you test enough hypotheses, some will appear \"significant\" by pure chance.\n",
    "\n",
    "**The problem**: If you test 20 independent hypotheses at α = 0.05, you expect 1 false positive even if nothing is real.\n",
    "\n",
    "Let's demonstrate this with the debate data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test many hypotheses on the debate data\n",
    "# Most of these we'd expect to find no effect\n",
    "\n",
    "hypotheses = []\n",
    "\n",
    "# 1. Does venue affect outcome?\n",
    "for venue in debates['venue'].unique():\n",
    "    venue_data = debates[debates['venue'] == venue]\n",
    "    a_wins = (venue_data['outcome'] == 'victory_a').sum()\n",
    "    total = len(venue_data[venue_data['outcome'] != 'draw'])\n",
    "    if total > 10:\n",
    "        try:\n",
    "            p = binomtest(a_wins, total, 0.5).pvalue\n",
    "        except:\n",
    "            p = stats.binom_test(a_wins, total, 0.5)\n",
    "        hypotheses.append(('Venue: ' + venue, a_wins/total, total, p))\n",
    "\n",
    "# 2. Does topic affect outcome?\n",
    "for topic in debates['topic_category'].unique():\n",
    "    topic_data = debates[debates['topic_category'] == topic]\n",
    "    a_wins = (topic_data['outcome'] == 'victory_a').sum()\n",
    "    total = len(topic_data[topic_data['outcome'] != 'draw'])\n",
    "    if total > 10:\n",
    "        try:\n",
    "            p = binomtest(a_wins, total, 0.5).pvalue\n",
    "        except:\n",
    "            p = stats.binom_test(a_wins, total, 0.5)\n",
    "        hypotheses.append(('Topic: ' + topic, a_wins/total, total, p))\n",
    "\n",
    "# 3. Does judge count affect outcome?\n",
    "for judges in debates['judge_count'].unique():\n",
    "    judge_data = debates[debates['judge_count'] == judges]\n",
    "    a_wins = (judge_data['outcome'] == 'victory_a').sum()\n",
    "    total = len(judge_data[judge_data['outcome'] != 'draw'])\n",
    "    if total > 10:\n",
    "        try:\n",
    "            p = binomtest(a_wins, total, 0.5).pvalue\n",
    "        except:\n",
    "            p = stats.binom_test(a_wins, total, 0.5)\n",
    "        hypotheses.append((f'{judges} judges', a_wins/total, total, p))\n",
    "\n",
    "# 4. Does year period affect outcome?\n",
    "for period in [(850, 860), (860, 870), (870, 880)]:\n",
    "    period_data = debates[(debates['year'] >= period[0]) & (debates['year'] < period[1])]\n",
    "    a_wins = (period_data['outcome'] == 'victory_a').sum()\n",
    "    total = len(period_data[period_data['outcome'] != 'draw'])\n",
    "    if total > 10:\n",
    "        try:\n",
    "            p = binomtest(a_wins, total, 0.5).pvalue\n",
    "        except:\n",
    "            p = stats.binom_test(a_wins, total, 0.5)\n",
    "        hypotheses.append((f'Years {period[0]}-{period[1]}', a_wins/total, total, p))\n",
    "\n",
    "# Sort by p-value\n",
    "hypotheses.sort(key=lambda x: x[3])\n",
    "\n",
    "print(f\"Tested {len(hypotheses)} hypotheses\")\n",
    "print(\"\\nResults sorted by p-value:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Hypothesis':<30} {'Rate':<10} {'n':<8} {'p-value':<12} {'Sig?':<6}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for hyp, rate, n, p in hypotheses:\n",
    "    sig = \"*\" if p < 0.05 else \"\"\n",
    "    print(f\"{hyp:<30} {rate:<10.1%} {n:<8} {p:<12.4f} {sig:<6}\")\n",
    "\n",
    "n_significant = sum(1 for h in hypotheses if h[3] < 0.05)\n",
    "print(f\"\\n{n_significant} hypotheses are 'significant' at α = 0.05\")\n",
    "print(f\"Expected by chance alone: {len(hypotheses) * 0.05:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Bonferroni Correction\n",
    "\n",
    "One solution to the multiple comparisons problem is the **Bonferroni correction**: divide α by the number of tests.\n",
    "\n",
    "If testing m hypotheses:\n",
    "$$\\alpha_{\\text{corrected}} = \\frac{\\alpha}{m}$$\n",
    "\n",
    "This is conservative—it reduces Type I errors at the cost of more Type II errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Bonferroni correction\n",
    "n_tests = len(hypotheses)\n",
    "alpha_corrected = 0.05 / n_tests\n",
    "\n",
    "print(f\"Bonferroni Correction\")\n",
    "print(f\"=\" * 50)\n",
    "print(f\"Number of tests: {n_tests}\")\n",
    "print(f\"Original α: 0.05\")\n",
    "print(f\"Corrected α: {alpha_corrected:.4f}\")\n",
    "\n",
    "print(f\"\\nHypotheses significant after Bonferroni correction:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "any_significant = False\n",
    "for hyp, rate, n, p in hypotheses:\n",
    "    if p < alpha_corrected:\n",
    "        print(f\"{hyp}: p = {p:.4f}\")\n",
    "        any_significant = True\n",
    "\n",
    "if not any_significant:\n",
    "    print(\"None! All 'significant' findings were likely false positives.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Effect Size — Beyond Significance\n",
    "\n",
    "Statistical significance tells us whether an effect exists. **Effect size** tells us how big it is.\n",
    "\n",
    "For proportions, a simple effect size is the difference from 50%:\n",
    "\n",
    "$$\\text{Effect Size} = |p - 0.5|$$\n",
    "\n",
    "A school with 52% win rate has a small effect. A school with 70% win rate has a large effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Effect sizes for each school\n",
    "print(\"Effect Sizes by School\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"{'School':<20} {'Win Rate':<12} {'Effect Size':<15} {'Interpretation':<15}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for _, row in win_rates.iterrows():\n",
    "    effect = abs(row['win_rate'] - 0.5)\n",
    "    if effect < 0.05:\n",
    "        interp = \"Negligible\"\n",
    "    elif effect < 0.10:\n",
    "        interp = \"Small\"\n",
    "    elif effect < 0.20:\n",
    "        interp = \"Medium\"\n",
    "    else:\n",
    "        interp = \"Large\"\n",
    "    \n",
    "    school_name = row['school'].replace('_', ' ').title()\n",
    "    print(f\"{school_name:<20} {row['win_rate']:<12.1%} {effect:<15.1%} {interp:<15}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Comparing Two Groups\n",
    "\n",
    "Often we want to compare two groups directly. For example: Is there a difference between Stone School and Water School win rates?\n",
    "\n",
    "### Chi-Square Test for Independence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a contingency table: Stone School vs Water School debates\n",
    "# Only look at debates where these two schools faced each other\n",
    "\n",
    "stone_vs_water = debates[\n",
    "    ((debates['scholar_a_school'] == 'stone_school') & (debates['scholar_b_school'] == 'water_school')) |\n",
    "    ((debates['scholar_a_school'] == 'water_school') & (debates['scholar_b_school'] == 'stone_school'))\n",
    "]\n",
    "\n",
    "# Count wins for each school\n",
    "stone_wins = (\n",
    "    ((stone_vs_water['scholar_a_school'] == 'stone_school') & (stone_vs_water['outcome'] == 'victory_a')).sum() +\n",
    "    ((stone_vs_water['scholar_b_school'] == 'stone_school') & (stone_vs_water['outcome'] == 'victory_b')).sum()\n",
    ")\n",
    "\n",
    "water_wins = (\n",
    "    ((stone_vs_water['scholar_a_school'] == 'water_school') & (stone_vs_water['outcome'] == 'victory_a')).sum() +\n",
    "    ((stone_vs_water['scholar_b_school'] == 'water_school') & (stone_vs_water['outcome'] == 'victory_b')).sum()\n",
    ")\n",
    "\n",
    "draws = (stone_vs_water['outcome'] == 'draw').sum()\n",
    "total_decisive = stone_wins + water_wins\n",
    "\n",
    "print(\"Stone School vs Water School Head-to-Head\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Total debates: {len(stone_vs_water)}\")\n",
    "print(f\"Draws: {draws}\")\n",
    "print(f\"Stone School wins: {stone_wins}\")\n",
    "print(f\"Water School wins: {water_wins}\")\n",
    "print(f\"\\nStone School win rate: {stone_wins/total_decisive:.1%}\")\n",
    "print(f\"Water School win rate: {water_wins/total_decisive:.1%}\")\n",
    "\n",
    "# Binomial test\n",
    "try:\n",
    "    p = binomtest(stone_wins, total_decisive, 0.5).pvalue\n",
    "except:\n",
    "    p = stats.binom_test(stone_wins, total_decisive, 0.5)\n",
    "\n",
    "print(f\"\\nP-value (test if different from 50/50): {p:.4f}\")\n",
    "print(f\"Significant at α=0.05? {'Yes' if p < 0.05 else 'No'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "| Concept | Key Insight | Common Mistake |\n",
    "|---------|-------------|----------------|\n",
    "| Null Hypothesis (H₀) | Default assumption of \"no effect\" | Thinking it's what we want to prove |\n",
    "| P-value | P(data \\| H₀), not P(H₀ \\| data) | Interpreting as \"probability H₀ is true\" |\n",
    "| Significance (α) | Arbitrary threshold, usually 0.05 | Treating as magical cutoff |\n",
    "| Type I Error | False positive (rejecting true H₀) | Ignoring when p-hacking |\n",
    "| Type II Error | False negative (missing real effect) | Concluding \"no effect\" from high p |\n",
    "| Multiple Comparisons | Test enough, something will be \"significant\" | Not correcting for many tests |\n",
    "| Effect Size | How big is the effect? | Confusing significance with importance |\n",
    "\n",
    "---\n",
    "\n",
    "## Exercises\n",
    "\n",
    "### Exercise 1: The Age Effect\n",
    "\n",
    "Test whether older scholars (age > 50) have a different win rate than younger scholars (age ≤ 50). \n",
    "1. Calculate win rates for each group\n",
    "2. Perform an appropriate hypothesis test\n",
    "3. Calculate the effect size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Your code here\n",
    "# Hint: Create columns for winner_age, then compare groups\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Publication Power\n",
    "\n",
    "Does having more publications help? Test whether scholars with above-median publications win more debates.\n",
    "1. Find the median publication count\n",
    "2. Compare win rates above vs below median\n",
    "3. What's the p-value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: The Controversy Connection\n",
    "\n",
    "Are close debates (margin = 'narrow' or outcome = 'draw') more controversial (higher `controversy_score`)?\n",
    "1. Calculate mean controversy score for close vs decisive debates\n",
    "2. Perform a t-test to compare the means\n",
    "3. Interpret the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Your code here\n",
    "# Hint: Use stats.ttest_ind() for comparing two means\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: P-Hacking Simulation\n",
    "\n",
    "Simulate the p-hacking problem:\n",
    "1. Generate 20 random \"coin flip\" experiments (each with n=50 trials, true p=0.5)\n",
    "2. Test each for deviation from 0.5\n",
    "3. How many are \"significant\" at α=0.05?\n",
    "4. Repeat this simulation 1000 times and show the distribution of \"significant\" findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4: Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Lesson\n",
    "\n",
    "In **Lesson 5: Bayesian Classification**, we'll move from \"is there an effect?\" to \"what should we believe?\" We'll investigate manuscript forgeries in the Archives, using evidence to update our beliefs about whether a document is genuine.\n",
    "\n",
    "*\"The frequentist asks: if Mink were innocent, how often would we see this evidence? The Bayesian asks: given this evidence, how likely is Mink innocent?\"*  \n",
    "— From the Senate Inquiry transcripts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
