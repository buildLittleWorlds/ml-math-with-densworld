{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/buildLittleWorlds/ml-math-with-densworld/blob/main/modules/01-statistics-probability/notebooks/03-central-limit-theorem.ipynb)\n",
    "\n",
    "# Lesson 3: The Central Limit Theorem\n",
    "\n",
    "*\"One mapmaker lies. Two mapmakers argue. Ten mapmakers approach the truth.\"*  \n",
    "— Proverb of the Capital Survey Office\n",
    "\n",
    "---\n",
    "\n",
    "## The Core Problem\n",
    "\n",
    "In the Dens, where the boundary between solid ground and densmuck shifts like a fever dream, mapmakers face an impossible task: survey terrain that refuses to stay still. Each individual survey contains error—from instruments, from weather, from fatigue, from the land itself.\n",
    "\n",
    "Yet somehow, when the Capital Survey Office combines reports from fifteen different mapmakers at the same location, their averaged estimate is remarkably accurate. **How can averaging wrong answers give you the right one?**\n",
    "\n",
    "The answer lies in the most important theorem in statistics: the **Central Limit Theorem** (CLT).\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this lesson, you will:\n",
    "1. Understand why sample means follow a normal distribution—even when the data doesn't\n",
    "2. Apply the CLT to quantify uncertainty in estimates\n",
    "3. Know the √n rule: why 4× the data gives only 2× the precision\n",
    "4. Recognize when CLT assumptions break down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Nice plotting defaults\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "# Colab-ready data loading\n",
    "BASE_URL = \"https://raw.githubusercontent.com/buildLittleWorlds/ml-math-with-densworld/main/data/\"\n",
    "\n",
    "# Load the datasets\n",
    "expeditions = pd.read_csv(BASE_URL + \"expedition_outcomes.csv\")\n",
    "dens_boundary = pd.read_csv(BASE_URL + \"dens_boundary_observations.csv\")\n",
    "\n",
    "print(f\"Loaded {len(expeditions)} expedition records\")\n",
    "print(f\"Loaded {len(dens_boundary)} boundary observations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: The Magic of Averaging\n",
    "\n",
    "### One Mapmaker vs. Many\n",
    "\n",
    "Imagine a single location on the Dens boundary—grid coordinate (50, 50). The true ground stability at this point is 0.65 (meaning 65% stable, 35% densmuck risk).\n",
    "\n",
    "Mapmakers survey this point, but their measurements are noisy. Let's see what happens when we send one mapmaker vs. averaging ten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate mapmaker observations at a single point\n",
    "TRUE_STABILITY = 0.65\n",
    "MEASUREMENT_STD = 0.10  # Typical mapmaker precision\n",
    "\n",
    "# Simulate 1000 scenarios where we send ONE mapmaker\n",
    "n_simulations = 1000\n",
    "single_observations = np.random.normal(TRUE_STABILITY, MEASUREMENT_STD, n_simulations)\n",
    "\n",
    "# Simulate 1000 scenarios where we send TEN mapmakers and average\n",
    "averaged_observations = []\n",
    "for _ in range(n_simulations):\n",
    "    ten_observations = np.random.normal(TRUE_STABILITY, MEASUREMENT_STD, 10)\n",
    "    averaged_observations.append(ten_observations.mean())\n",
    "averaged_observations = np.array(averaged_observations)\n",
    "\n",
    "# Visualize the difference\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Single mapmaker\n",
    "axes[0].hist(single_observations, bins=40, color='coral', edgecolor='black', alpha=0.7)\n",
    "axes[0].axvline(TRUE_STABILITY, color='green', linewidth=3, linestyle='--', label=f'Truth = {TRUE_STABILITY}')\n",
    "axes[0].axvline(single_observations.mean(), color='red', linewidth=2, label=f'Mean = {single_observations.mean():.3f}')\n",
    "axes[0].set_xlabel('Observed Stability')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title(f'One Mapmaker\\nStd = {single_observations.std():.3f}')\n",
    "axes[0].legend()\n",
    "axes[0].set_xlim(0.2, 1.1)\n",
    "\n",
    "# Ten mapmakers averaged\n",
    "axes[1].hist(averaged_observations, bins=40, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "axes[1].axvline(TRUE_STABILITY, color='green', linewidth=3, linestyle='--', label=f'Truth = {TRUE_STABILITY}')\n",
    "axes[1].axvline(averaged_observations.mean(), color='red', linewidth=2, label=f'Mean = {averaged_observations.mean():.3f}')\n",
    "axes[1].set_xlabel('Averaged Stability Estimate')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title(f'Average of 10 Mapmakers\\nStd = {averaged_observations.std():.3f}')\n",
    "axes[1].legend()\n",
    "axes[1].set_xlim(0.2, 1.1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Precision improvement: {single_observations.std() / averaged_observations.std():.2f}x\")\n",
    "print(f\"Theory predicts: √10 = {np.sqrt(10):.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Key Insight\n",
    "\n",
    "Notice two things:\n",
    "\n",
    "1. **The average of 10 mapmakers is more concentrated around the truth** — the histogram is narrower\n",
    "2. **The improvement is approximately √10 ≈ 3.16×** — not 10×!\n",
    "\n",
    "This is the √n rule in action. More mapmakers help, but with diminishing returns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: The Central Limit Theorem Explained\n",
    "\n",
    "The CLT states:\n",
    "\n",
    "> If you take a sample of n observations from ANY distribution with mean μ and standard deviation σ, the distribution of the sample mean approaches a normal distribution with:\n",
    "> - Mean = μ\n",
    "> - Standard deviation = σ/√n\n",
    "\n",
    "The remarkable part: **\"any distribution.\"** It doesn't matter if the original data is skewed, bimodal, or weirdly shaped. The sample mean will still be approximately normal.\n",
    "\n",
    "### Demonstration with Non-Normal Data\n",
    "\n",
    "Let's prove this with creature market prices—which we know are heavily right-skewed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load creature market data (heavily skewed)\n",
    "creature_market = pd.read_csv(BASE_URL + \"creature_market.csv\")\n",
    "prices = creature_market['price_per_unit'].values\n",
    "\n",
    "# Show the original skewed distribution\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "# Original distribution\n",
    "axes[0, 0].hist(prices, bins=50, color='goldenrod', edgecolor='black', alpha=0.7)\n",
    "axes[0, 0].set_title(f'Original Price Distribution\\n(Skewness = {stats.skew(prices):.2f})')\n",
    "axes[0, 0].set_xlabel('Price')\n",
    "\n",
    "# Sample means for various n\n",
    "sample_sizes = [2, 5, 10, 30, 100]\n",
    "n_samples = 2000\n",
    "\n",
    "for idx, n in enumerate(sample_sizes):\n",
    "    row = (idx + 1) // 3\n",
    "    col = (idx + 1) % 3\n",
    "    \n",
    "    sample_means = [np.random.choice(prices, size=n, replace=True).mean() for _ in range(n_samples)]\n",
    "    sample_means = np.array(sample_means)\n",
    "    \n",
    "    axes[row, col].hist(sample_means, bins=40, color='steelblue', edgecolor='black', alpha=0.7, density=True)\n",
    "    \n",
    "    # Overlay theoretical normal\n",
    "    theoretical_mean = prices.mean()\n",
    "    theoretical_std = prices.std() / np.sqrt(n)\n",
    "    x = np.linspace(sample_means.min(), sample_means.max(), 100)\n",
    "    axes[row, col].plot(x, stats.norm.pdf(x, theoretical_mean, theoretical_std), 'r-', linewidth=2)\n",
    "    \n",
    "    axes[row, col].set_title(f'Sample Mean (n={n})\\nSkewness = {stats.skew(sample_means):.2f}')\n",
    "    axes[row, col].set_xlabel('Mean Price')\n",
    "\n",
    "plt.suptitle('Central Limit Theorem: Skewed Data → Normal Sample Means', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"As sample size increases:\")\n",
    "print(\"  - The distribution of sample means becomes more symmetric\")\n",
    "print(\"  - The spread (standard deviation) decreases\")\n",
    "print(\"  - The shape approaches normal (red curve)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mathematical Formulation\n",
    "\n",
    "For a sample of n observations with sample mean $\\bar{X}$:\n",
    "\n",
    "$$\\bar{X} \\sim N\\left(\\mu, \\frac{\\sigma}{\\sqrt{n}}\\right)$$\n",
    "\n",
    "Or equivalently, the standardized version:\n",
    "\n",
    "$$Z = \\frac{\\bar{X} - \\mu}{\\sigma/\\sqrt{n}} \\sim N(0, 1)$$\n",
    "\n",
    "The quantity $\\sigma/\\sqrt{n}$ is called the **Standard Error** of the mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: The √n Rule in Practice\n",
    "\n",
    "### How Much Data Do You Need?\n",
    "\n",
    "The √n relationship has profound practical implications:\n",
    "\n",
    "| To improve precision by... | You need... |\n",
    "|---------------------------|-------------|\n",
    "| 2× | 4× the data |\n",
    "| 3× | 9× the data |\n",
    "| 10× | 100× the data |\n",
    "\n",
    "This is why the Capital Survey Office sends teams of mapmakers rather than waiting for one perfect measurement. Perfection is impossible; averaging is practical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the √n relationship with real boundary data\n",
    "true_mean = dens_boundary['observed_stability'].mean()  # Treat full dataset as \"population\"\n",
    "true_std = dens_boundary['observed_stability'].std()\n",
    "\n",
    "sample_sizes = [5, 10, 25, 50, 100, 200, 400]\n",
    "n_simulations = 1000\n",
    "\n",
    "observed_stds = []\n",
    "theoretical_stds = []\n",
    "\n",
    "for n in sample_sizes:\n",
    "    sample_means = []\n",
    "    for _ in range(n_simulations):\n",
    "        sample = dens_boundary['observed_stability'].sample(n=n, replace=True)\n",
    "        sample_means.append(sample.mean())\n",
    "    observed_stds.append(np.std(sample_means))\n",
    "    theoretical_stds.append(true_std / np.sqrt(n))\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "ax.plot(sample_sizes, observed_stds, 'bo-', markersize=10, linewidth=2, label='Observed (simulation)')\n",
    "ax.plot(sample_sizes, theoretical_stds, 'r--', linewidth=2, label='Theoretical (σ/√n)')\n",
    "\n",
    "ax.set_xlabel('Sample Size (n)', fontsize=12)\n",
    "ax.set_ylabel('Standard Error of Mean', fontsize=12)\n",
    "ax.set_title('The √n Rule: More Data = Less Uncertainty\\n(but with diminishing returns)', fontsize=14)\n",
    "ax.legend(fontsize=11)\n",
    "ax.set_xscale('log')\n",
    "ax.set_yscale('log')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nPrecision at different sample sizes:\")\n",
    "print(f\"{'n':<10} {'Std Error':<15} {'Relative to n=5':<20}\")\n",
    "print(\"-\" * 45)\n",
    "base_se = theoretical_stds[0]\n",
    "for n, se in zip(sample_sizes, theoretical_stds):\n",
    "    print(f\"{n:<10} {se:<15.4f} {base_se/se:.2f}x better\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practical Application: Survey Planning\n",
    "\n",
    "The Capital Survey Office needs to estimate the average stability of a region to within ±0.02 (with 95% confidence). How many observations do they need?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required sample size calculation\n",
    "# 95% CI half-width = 1.96 * (σ/√n)\n",
    "# We want: 1.96 * (σ/√n) ≤ 0.02\n",
    "# So: n ≥ (1.96 * σ / 0.02)²\n",
    "\n",
    "sigma = dens_boundary['measurement_error'].std()  # Use error std as proxy\n",
    "desired_precision = 0.02\n",
    "confidence_level = 0.95\n",
    "z_score = stats.norm.ppf(1 - (1 - confidence_level) / 2)  # 1.96\n",
    "\n",
    "required_n = (z_score * sigma / desired_precision) ** 2\n",
    "\n",
    "print(\"Sample Size Planning for Boundary Survey\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Measurement std (σ): {sigma:.4f}\")\n",
    "print(f\"Desired precision: ±{desired_precision}\")\n",
    "print(f\"Confidence level: {confidence_level:.0%}\")\n",
    "print(f\"\\nRequired sample size: n ≥ {required_n:.0f}\")\n",
    "\n",
    "# Show precision at various sample sizes\n",
    "print(f\"\\n{'n':<10} {'95% CI half-width':<20} {'Meets target?':<15}\")\n",
    "print(\"-\" * 45)\n",
    "for n in [10, 25, 50, 100, 200, 500]:\n",
    "    half_width = z_score * sigma / np.sqrt(n)\n",
    "    meets = \"✓\" if half_width <= desired_precision else \"✗\"\n",
    "    print(f\"{n:<10} ±{half_width:<18.4f} {meets}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Why the CLT Matters for Machine Learning\n",
    "\n",
    "The Central Limit Theorem is the foundation for many statistical techniques:\n",
    "\n",
    "### 1. Confidence Intervals\n",
    "Because sample means are approximately normal, we can construct confidence intervals using the normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confidence interval for expedition success rate\n",
    "success_rate = expeditions['success'].mean()\n",
    "n = len(expeditions)\n",
    "\n",
    "# For proportions, σ = √(p(1-p))\n",
    "se = np.sqrt(success_rate * (1 - success_rate) / n)\n",
    "\n",
    "ci_95 = (success_rate - 1.96 * se, success_rate + 1.96 * se)\n",
    "ci_99 = (success_rate - 2.576 * se, success_rate + 2.576 * se)\n",
    "\n",
    "print(\"Expedition Success Rate Estimation\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Sample proportion: {success_rate:.1%}\")\n",
    "print(f\"Sample size: {n}\")\n",
    "print(f\"Standard error: {se:.4f}\")\n",
    "print(f\"\\n95% CI: ({ci_95[0]:.1%}, {ci_95[1]:.1%})\")\n",
    "print(f\"99% CI: ({ci_99[0]:.1%}, {ci_99[1]:.1%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Hypothesis Testing\n",
    "Because we know the distribution of sample means, we can test whether observed differences are statistically significant.\n",
    "\n",
    "### 3. Regression Coefficients\n",
    "In linear regression, the estimated coefficients are averages of sorts—and thus approximately normally distributed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: When the CLT Fails\n",
    "\n",
    "The CLT requires certain conditions. It may fail when:\n",
    "\n",
    "### 1. Sample Size Is Too Small\n",
    "For highly skewed distributions, you may need n > 30 (or even n > 100) for the CLT to apply."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate CLT failure with small samples from skewed data\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "for idx, n in enumerate([3, 10, 50]):\n",
    "    sample_means = [np.random.choice(prices, size=n, replace=True).mean() for _ in range(2000)]\n",
    "    \n",
    "    axes[idx].hist(sample_means, bins=40, color='steelblue', edgecolor='black', alpha=0.7, density=True)\n",
    "    \n",
    "    # Try to fit normal\n",
    "    theoretical_mean = np.mean(sample_means)\n",
    "    theoretical_std = np.std(sample_means)\n",
    "    x = np.linspace(min(sample_means), max(sample_means), 100)\n",
    "    axes[idx].plot(x, stats.norm.pdf(x, theoretical_mean, theoretical_std), 'r-', linewidth=2)\n",
    "    \n",
    "    skewness = stats.skew(sample_means)\n",
    "    axes[idx].set_title(f'n = {n}\\nSkewness = {skewness:.2f}\\n{\"✓ CLT applies\" if abs(skewness) < 0.5 else \"⚠ Still skewed\"}')\n",
    "    axes[idx].set_xlabel('Sample Mean Price')\n",
    "\n",
    "plt.suptitle('CLT Convergence Depends on Sample Size (and Original Distribution)', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"For highly skewed data like prices:\")\n",
    "print(\"  - n=3: CLT definitely doesn't apply\")\n",
    "print(\"  - n=10: Marginal, still shows skewness\")\n",
    "print(\"  - n=50: CLT is reasonably applicable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Infinite Variance (Fat Tails)\n",
    "\n",
    "Some distributions have such heavy tails that the variance is technically infinite. The CLT doesn't apply to these. Fortunately, our Densworld data doesn't exhibit this extreme behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Dependent Observations\n",
    "\n",
    "The CLT assumes observations are independent. If mapmaker observations are correlated (e.g., they all survey on the same day with the same weather), the effective sample size is smaller than it appears."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for potential dependence: do observations cluster by weather?\n",
    "weather_stability = dens_boundary.groupby('weather_conditions')['observed_stability'].agg(['mean', 'std', 'count'])\n",
    "\n",
    "print(\"Stability Observations by Weather Condition\")\n",
    "print(\"=\" * 50)\n",
    "print(weather_stability.round(3))\n",
    "print(f\"\\nIf observations are clustered by weather, they may not be independent.\")\n",
    "print(f\"This could make our CLT-based confidence intervals too narrow.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Practical Exercise — Combining Mapmaker Surveys\n",
    "\n",
    "Let's simulate what the Capital Survey Office does: combine multiple mapmaker observations to estimate true stability at a location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to a single approximate location (grid cell)\n",
    "location_data = dens_boundary[\n",
    "    (dens_boundary['location_x'].between(45, 55)) & \n",
    "    (dens_boundary['location_y'].between(45, 55))\n",
    "]\n",
    "\n",
    "print(f\"Found {len(location_data)} observations in grid cell (45-55, 45-55)\")\n",
    "print(f\"\\nObservers who surveyed this area:\")\n",
    "print(location_data['observer_name'].value_counts())\n",
    "\n",
    "# Calculate group estimate and confidence interval\n",
    "stability_values = location_data['observed_stability']\n",
    "group_mean = stability_values.mean()\n",
    "group_se = stability_values.std() / np.sqrt(len(stability_values))\n",
    "\n",
    "print(f\"\\nCombined estimate (CLT-based):\")\n",
    "print(f\"  Mean stability: {group_mean:.3f}\")\n",
    "print(f\"  Standard error: {group_se:.4f}\")\n",
    "print(f\"  95% CI: ({group_mean - 1.96*group_se:.3f}, {group_mean + 1.96*group_se:.3f})\")\n",
    "\n",
    "# Compare individual mapmaker estimates\n",
    "print(f\"\\nIndividual mapmaker estimates:\")\n",
    "individual = location_data.groupby('observer_name')['observed_stability'].mean()\n",
    "for name, value in individual.items():\n",
    "    print(f\"  {name}: {value:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "| Concept | Key Insight | Formula |\n",
    "|---------|-------------|--------|\n",
    "| Central Limit Theorem | Sample means are normal (regardless of original distribution) | $\\bar{X} \\sim N(\\mu, \\sigma/\\sqrt{n})$ |\n",
    "| Standard Error | Uncertainty of sample mean decreases with √n | $SE = \\sigma / \\sqrt{n}$ |\n",
    "| √n Rule | 4× data → 2× precision | Precision ∝ √n |\n",
    "| Sample Size Planning | Required n = (z × σ / precision)² | Solve for n |\n",
    "| CLT Requirements | Large n, finite variance, independence | Rules of thumb vary |\n",
    "\n",
    "---\n",
    "\n",
    "## Exercises\n",
    "\n",
    "### Exercise 1: Bootstrap vs. CLT\n",
    "\n",
    "For the expedition `catch_value` variable:\n",
    "1. Calculate a 95% confidence interval using the CLT (normal approximation)\n",
    "2. Calculate a 95% confidence interval using bootstrap (10,000 resamples)\n",
    "3. How different are they? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Your code here\n",
    "# CLT-based CI: mean ± 1.96 * (std / sqrt(n))\n",
    "# Bootstrap CI: np.percentile(bootstrap_means, [2.5, 97.5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Sample Size for Creature Market Analysis\n",
    "\n",
    "A merchant wants to estimate the average price of `mammal` category creatures to within ±5 gold pieces with 95% confidence. Based on the current data, how many sales records would they need?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Your code here\n",
    "# First, calculate the standard deviation of mammal prices\n",
    "# Then use: n = (1.96 * sigma / precision)^2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Testing CLT Assumptions\n",
    "\n",
    "Check whether the `casualties` variable from expeditions data satisfies CLT requirements:\n",
    "1. Calculate skewness and kurtosis\n",
    "2. Simulate the distribution of sample means for n=5, n=20, n=50\n",
    "3. At what sample size does the distribution become approximately normal?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Comparing Mapmaker Precision\n",
    "\n",
    "Using the `dens_boundary` data:\n",
    "1. Group observations by `instrument_type`\n",
    "2. Calculate the standard error of `observed_stability` for each instrument type\n",
    "3. How many observations with `pacing` would you need to match the precision of 10 observations with `theodolite`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4: Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Lesson\n",
    "\n",
    "In **Lesson 4: Hypothesis Testing**, we'll learn how to determine whether observed differences are real or just random noise. We'll analyze the Scholar Debates to ask: Do Stone School philosophers really win more debates, or is it just luck?\n",
    "\n",
    "*\"Correlation is not causation. The Stone School's victories may say more about the judges than the philosophy.\"*  \n",
    "— Yasho Krent, in defense at the Senate Inquiry"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
