{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/buildLittleWorlds/ml-math-with-densworld/blob/main/modules/01-statistics-probability/notebooks/05-bayesian-classification.ipynb)\n",
    "\n",
    "# Lesson 5: Bayesian Classification\n",
    "\n",
    "*\"The Archives hold 300 manuscripts attributed to Grigsu Haldo, the great Stone School philosopher. But how many did he actually write? Mink Pavar's forgeries are legendary—even experts cannot always tell them apart. To convict Mink, we must ask not 'Is this a forgery?' but 'Given what we observe, how likely is it to be a forgery?'\"*  \n",
    "— Opening statement, Senate Inquiry into the Pavar Forgeries\n",
    "\n",
    "---\n",
    "\n",
    "## The Core Problem\n",
    "\n",
    "The Capital Archives have discovered that some manuscripts attributed to famous philosophers are actually forgeries, likely created by Mink Pavar. We have a labeled training set of 300 manuscripts—some genuine, some known forgeries—with measurable features like vocabulary richness, sentence length, and philosophical term density.\n",
    "\n",
    "Given a new manuscript, how should we combine these features with our prior beliefs to estimate the probability it's a forgery?\n",
    "\n",
    "This is the domain of **Bayesian classification**.\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this lesson, you will:\n",
    "1. Understand Bayes' Theorem and its components (prior, likelihood, posterior)\n",
    "2. Build intuition for how evidence updates beliefs\n",
    "3. Evaluate classifiers using confusion matrices, precision, and recall\n",
    "4. Implement Naive Bayes classification from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Nice plotting defaults\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "# Colab-ready data loading\n",
    "BASE_URL = \"https://raw.githubusercontent.com/buildLittleWorlds/ml-math-with-densworld/main/data/\"\n",
    "\n",
    "# Load the manuscript features dataset\n",
    "manuscripts = pd.read_csv(BASE_URL + \"manuscript_features.csv\")\n",
    "\n",
    "print(f\"Loaded {len(manuscripts)} manuscript records\")\n",
    "print(f\"Forgeries: {manuscripts['is_forgery'].sum()} ({manuscripts['is_forgery'].mean():.1%})\")\n",
    "manuscripts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: The Forgery Problem\n",
    "\n",
    "### Exploring the Data\n",
    "\n",
    "Mink Pavar was a brilliant forger who created manuscripts in the style of famous philosophers, then sold them to collectors and archives. Let's examine the features that might distinguish forgeries from genuine manuscripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare features between genuine and forged manuscripts\n",
    "genuine = manuscripts[manuscripts['is_forgery'] == False]\n",
    "forged = manuscripts[manuscripts['is_forgery'] == True]\n",
    "\n",
    "features = ['avg_sentence_length', 'vocabulary_richness', 'philosophical_term_density', \n",
    "            'stylometric_variance', 'era_marker_score']\n",
    "\n",
    "print(\"Feature Comparison: Genuine vs Forged Manuscripts\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Feature':<30} {'Genuine Mean':<15} {'Forged Mean':<15} {'Difference':<10}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for feature in features:\n",
    "    gen_mean = genuine[feature].mean()\n",
    "    forg_mean = forged[feature].mean()\n",
    "    diff = forg_mean - gen_mean\n",
    "    print(f\"{feature:<30} {gen_mean:<15.4f} {forg_mean:<15.4f} {diff:+.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the distributions\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, feature in enumerate(features):\n",
    "    ax = axes[idx]\n",
    "    ax.hist(genuine[feature], bins=20, alpha=0.6, label='Genuine', color='steelblue', density=True)\n",
    "    ax.hist(forged[feature], bins=20, alpha=0.6, label='Forged', color='coral', density=True)\n",
    "    ax.set_xlabel(feature.replace('_', ' ').title())\n",
    "    ax.set_ylabel('Density')\n",
    "    ax.legend()\n",
    "    ax.set_title(f'Distribution of {feature}')\n",
    "\n",
    "# Hide the 6th subplot if we only have 5 features\n",
    "if len(features) < 6:\n",
    "    axes[5].axis('off')\n",
    "\n",
    "plt.suptitle('Feature Distributions: Genuine vs Forged Manuscripts', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey observations:\")\n",
    "print(\"  - Some features show clear separation (good for classification)\")\n",
    "print(\"  - Others overlap substantially (less discriminative)\")\n",
    "print(\"  - No single feature perfectly separates the classes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Bayes' Theorem\n",
    "\n",
    "### The Foundation of Bayesian Thinking\n",
    "\n",
    "Bayes' Theorem tells us how to update our beliefs in light of new evidence:\n",
    "\n",
    "$$P(\\text{Forgery} | \\text{Evidence}) = \\frac{P(\\text{Evidence} | \\text{Forgery}) \\times P(\\text{Forgery})}{P(\\text{Evidence})}$$\n",
    "\n",
    "Let's break down each component:\n",
    "\n",
    "| Term | Name | Meaning |\n",
    "|------|------|--------|\n",
    "| P(Forgery \\| Evidence) | **Posterior** | Probability it's a forgery, given what we observed |\n",
    "| P(Evidence \\| Forgery) | **Likelihood** | How likely would we see this evidence if it IS a forgery? |\n",
    "| P(Forgery) | **Prior** | Our initial belief before seeing evidence |\n",
    "| P(Evidence) | **Normalizing constant** | Probability of seeing this evidence under any scenario |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Simple Example\n",
    "\n",
    "Let's start with one feature: `stylometric_variance`. Mink Pavar was brilliant but inconsistent—forged manuscripts tend to have higher variance in writing style."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate class statistics for stylometric_variance\n",
    "feature = 'stylometric_variance'\n",
    "\n",
    "# Prior probabilities\n",
    "p_forgery = manuscripts['is_forgery'].mean()\n",
    "p_genuine = 1 - p_forgery\n",
    "\n",
    "print(\"Prior Probabilities (before seeing any evidence)\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"P(Forgery) = {p_forgery:.3f}  ({p_forgery:.1%})\")\n",
    "print(f\"P(Genuine) = {p_genuine:.3f}  ({p_genuine:.1%})\")\n",
    "\n",
    "# Class-conditional distributions (assuming Gaussian)\n",
    "genuine_mean = genuine[feature].mean()\n",
    "genuine_std = genuine[feature].std()\n",
    "forged_mean = forged[feature].mean()\n",
    "forged_std = forged[feature].std()\n",
    "\n",
    "print(f\"\\nClass-Conditional Distributions for '{feature}'\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Genuine: μ = {genuine_mean:.4f}, σ = {genuine_std:.4f}\")\n",
    "print(f\"Forged:  μ = {forged_mean:.4f}, σ = {forged_std:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Bayesian updating for a specific observation\n",
    "test_value = 0.15  # A manuscript with stylometric_variance = 0.15\n",
    "\n",
    "# Calculate likelihoods P(Evidence | Class)\n",
    "likelihood_genuine = stats.norm.pdf(test_value, genuine_mean, genuine_std)\n",
    "likelihood_forged = stats.norm.pdf(test_value, forged_mean, forged_std)\n",
    "\n",
    "# Calculate unnormalized posteriors\n",
    "unnorm_genuine = likelihood_genuine * p_genuine\n",
    "unnorm_forged = likelihood_forged * p_forgery\n",
    "\n",
    "# Normalize\n",
    "normalizer = unnorm_genuine + unnorm_forged\n",
    "posterior_genuine = unnorm_genuine / normalizer\n",
    "posterior_forged = unnorm_forged / normalizer\n",
    "\n",
    "print(f\"Bayesian Classification for a manuscript with {feature} = {test_value}\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\n1. Prior probabilities:\")\n",
    "print(f\"   P(Genuine) = {p_genuine:.3f}\")\n",
    "print(f\"   P(Forgery) = {p_forgery:.3f}\")\n",
    "\n",
    "print(f\"\\n2. Likelihoods P(Evidence | Class):\")\n",
    "print(f\"   P({feature}={test_value} | Genuine) = {likelihood_genuine:.4f}\")\n",
    "print(f\"   P({feature}={test_value} | Forgery) = {likelihood_forged:.4f}\")\n",
    "\n",
    "print(f\"\\n3. Posterior probabilities (after seeing evidence):\")\n",
    "print(f\"   P(Genuine | Evidence) = {posterior_genuine:.3f}  ({posterior_genuine:.1%})\")\n",
    "print(f\"   P(Forgery | Evidence) = {posterior_forged:.3f}  ({posterior_forged:.1%})\")\n",
    "\n",
    "print(f\"\\n4. Classification: {'FORGERY' if posterior_forged > 0.5 else 'GENUINE'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize how posterior changes with feature value\n",
    "x_range = np.linspace(0, 0.4, 200)\n",
    "\n",
    "posteriors = []\n",
    "for x in x_range:\n",
    "    lik_gen = stats.norm.pdf(x, genuine_mean, genuine_std)\n",
    "    lik_forg = stats.norm.pdf(x, forged_mean, forged_std)\n",
    "    unnorm_forg = lik_forg * p_forgery\n",
    "    unnorm_gen = lik_gen * p_genuine\n",
    "    post_forg = unnorm_forg / (unnorm_forg + unnorm_gen)\n",
    "    posteriors.append(post_forg)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: Class-conditional distributions\n",
    "x_pdf = np.linspace(0, 0.4, 200)\n",
    "axes[0].plot(x_pdf, stats.norm.pdf(x_pdf, genuine_mean, genuine_std), \n",
    "             'b-', linewidth=2, label='P(x | Genuine)')\n",
    "axes[0].plot(x_pdf, stats.norm.pdf(x_pdf, forged_mean, forged_std), \n",
    "             'r-', linewidth=2, label='P(x | Forgery)')\n",
    "axes[0].axvline(test_value, color='green', linestyle='--', linewidth=2, label=f'x = {test_value}')\n",
    "axes[0].set_xlabel('Stylometric Variance')\n",
    "axes[0].set_ylabel('Likelihood')\n",
    "axes[0].set_title('Class-Conditional Distributions')\n",
    "axes[0].legend()\n",
    "\n",
    "# Right: Posterior probability\n",
    "axes[1].plot(x_range, posteriors, 'purple', linewidth=2)\n",
    "axes[1].axhline(0.5, color='gray', linestyle='--', label='Decision boundary')\n",
    "axes[1].axvline(test_value, color='green', linestyle='--', linewidth=2, label=f'x = {test_value}')\n",
    "axes[1].fill_between(x_range, posteriors, 0.5, where=np.array(posteriors) > 0.5, \n",
    "                     alpha=0.3, color='red', label='Classify as Forgery')\n",
    "axes[1].fill_between(x_range, posteriors, 0.5, where=np.array(posteriors) <= 0.5, \n",
    "                     alpha=0.3, color='blue', label='Classify as Genuine')\n",
    "axes[1].set_xlabel('Stylometric Variance')\n",
    "axes[1].set_ylabel('P(Forgery | x)')\n",
    "axes[1].set_title('Posterior Probability of Forgery')\n",
    "axes[1].legend(loc='upper left')\n",
    "axes[1].set_ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Confusion Matrix and Performance Metrics\n",
    "\n",
    "Before building a full classifier, let's understand how to evaluate one.\n",
    "\n",
    "### The Confusion Matrix\n",
    "\n",
    "| | Predicted Genuine | Predicted Forgery |\n",
    "|---|---|---|\n",
    "| **Actually Genuine** | True Negative (TN) | False Positive (FP) |\n",
    "| **Actually Forgery** | False Negative (FN) | True Positive (TP) |\n",
    "\n",
    "### Key Metrics\n",
    "\n",
    "- **Accuracy**: (TP + TN) / Total — Overall correctness\n",
    "- **Precision**: TP / (TP + FP) — When we say \"forgery,\" how often are we right?\n",
    "- **Recall**: TP / (TP + FN) — Of all forgeries, how many did we catch?\n",
    "- **F1 Score**: Harmonic mean of precision and recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classifier(y_true, y_pred):\n",
    "    \"\"\"Calculate and display classification metrics.\"\"\"\n",
    "    \n",
    "    # Confusion matrix components\n",
    "    tp = np.sum((y_true == True) & (y_pred == True))\n",
    "    tn = np.sum((y_true == False) & (y_pred == False))\n",
    "    fp = np.sum((y_true == False) & (y_pred == True))\n",
    "    fn = np.sum((y_true == True) & (y_pred == False))\n",
    "    \n",
    "    # Metrics\n",
    "    accuracy = (tp + tn) / len(y_true)\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    # Display confusion matrix\n",
    "    print(\"Confusion Matrix\")\n",
    "    print(\"=\" * 40)\n",
    "    print(f\"{'':20} {'Pred Genuine':>12} {'Pred Forgery':>12}\")\n",
    "    print(f\"{'Actual Genuine':20} {tn:>12} {fp:>12}\")\n",
    "    print(f\"{'Actual Forgery':20} {fn:>12} {tp:>12}\")\n",
    "    \n",
    "    print(f\"\\nMetrics\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"Accuracy:  {accuracy:.3f}  (Overall correctness)\")\n",
    "    print(f\"Precision: {precision:.3f}  (When we say 'forgery', are we right?)\")\n",
    "    print(f\"Recall:    {recall:.3f}  (Of all forgeries, how many caught?)\")\n",
    "    print(f\"F1 Score:  {f1:.3f}  (Harmonic mean of precision & recall)\")\n",
    "    \n",
    "    return {'accuracy': accuracy, 'precision': precision, 'recall': recall, 'f1': f1}\n",
    "\n",
    "# Test with our single-feature classifier\n",
    "def classify_single_feature(x, feature='stylometric_variance'):\n",
    "    \"\"\"Classify using a single feature with Gaussian Naive Bayes.\"\"\"\n",
    "    gen_mean = genuine[feature].mean()\n",
    "    gen_std = genuine[feature].std()\n",
    "    forg_mean = forged[feature].mean()\n",
    "    forg_std = forged[feature].std()\n",
    "    \n",
    "    lik_gen = stats.norm.pdf(x, gen_mean, gen_std)\n",
    "    lik_forg = stats.norm.pdf(x, forg_mean, forg_std)\n",
    "    \n",
    "    unnorm_gen = lik_gen * p_genuine\n",
    "    unnorm_forg = lik_forg * p_forgery\n",
    "    \n",
    "    post_forg = unnorm_forg / (unnorm_forg + unnorm_gen)\n",
    "    return post_forg > 0.5\n",
    "\n",
    "# Apply to all manuscripts\n",
    "predictions = manuscripts['stylometric_variance'].apply(classify_single_feature)\n",
    "\n",
    "print(\"Single-Feature Classifier (stylometric_variance)\")\n",
    "print(\"\" + \"=\"*50 + \"\\n\")\n",
    "metrics = evaluate_classifier(manuscripts['is_forgery'].values, predictions.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Naive Bayes with Multiple Features\n",
    "\n",
    "### The \"Naive\" Assumption\n",
    "\n",
    "Naive Bayes assumes that features are **conditionally independent** given the class. This means:\n",
    "\n",
    "$$P(x_1, x_2, ..., x_n | \\text{Class}) = P(x_1 | \\text{Class}) \\times P(x_2 | \\text{Class}) \\times ... \\times P(x_n | \\text{Class})$$\n",
    "\n",
    "This assumption is almost always wrong! But it often works surprisingly well in practice.\n",
    "\n",
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayesClassifier:\n",
    "    \"\"\"Gaussian Naive Bayes classifier implemented from scratch.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.class_priors = {}\n",
    "        self.class_means = {}\n",
    "        self.class_stds = {}\n",
    "        self.classes = None\n",
    "        self.features = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Learn class-conditional distributions from training data.\"\"\"\n",
    "        self.features = X.columns.tolist()\n",
    "        self.classes = y.unique()\n",
    "        \n",
    "        for cls in self.classes:\n",
    "            # Prior probability\n",
    "            self.class_priors[cls] = (y == cls).mean()\n",
    "            \n",
    "            # Class-conditional means and stds\n",
    "            cls_data = X[y == cls]\n",
    "            self.class_means[cls] = cls_data.mean()\n",
    "            self.class_stds[cls] = cls_data.std() + 1e-6  # Add small value to avoid division by zero\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Return posterior probabilities for each class.\"\"\"\n",
    "        posteriors = []\n",
    "        \n",
    "        for idx in range(len(X)):\n",
    "            row = X.iloc[idx]\n",
    "            class_posteriors = {}\n",
    "            \n",
    "            for cls in self.classes:\n",
    "                # Start with log prior\n",
    "                log_posterior = np.log(self.class_priors[cls])\n",
    "                \n",
    "                # Add log likelihoods for each feature\n",
    "                for feature in self.features:\n",
    "                    mean = self.class_means[cls][feature]\n",
    "                    std = self.class_stds[cls][feature]\n",
    "                    log_likelihood = stats.norm.logpdf(row[feature], mean, std)\n",
    "                    log_posterior += log_likelihood\n",
    "                \n",
    "                class_posteriors[cls] = log_posterior\n",
    "            \n",
    "            # Convert from log to probability and normalize\n",
    "            max_log = max(class_posteriors.values())\n",
    "            exp_posteriors = {cls: np.exp(lp - max_log) for cls, lp in class_posteriors.items()}\n",
    "            total = sum(exp_posteriors.values())\n",
    "            normalized = {cls: p / total for cls, p in exp_posteriors.items()}\n",
    "            posteriors.append(normalized)\n",
    "        \n",
    "        return posteriors\n",
    "    \n",
    "    def predict(self, X, threshold=0.5):\n",
    "        \"\"\"Predict class labels.\"\"\"\n",
    "        posteriors = self.predict_proba(X)\n",
    "        predictions = []\n",
    "        for p in posteriors:\n",
    "            # For binary classification, use threshold\n",
    "            if len(self.classes) == 2:\n",
    "                predictions.append(p[True] > threshold)\n",
    "            else:\n",
    "                predictions.append(max(p, key=p.get))\n",
    "        return np.array(predictions)\n",
    "\n",
    "# Train the classifier\n",
    "X = manuscripts[features]\n",
    "y = manuscripts['is_forgery']\n",
    "\n",
    "nb_classifier = NaiveBayesClassifier()\n",
    "nb_classifier.fit(X, y)\n",
    "\n",
    "print(\"Naive Bayes Classifier Trained\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Features used: {features}\")\n",
    "print(f\"\\nPrior probabilities:\")\n",
    "for cls, prior in nb_classifier.class_priors.items():\n",
    "    label = \"Forgery\" if cls else \"Genuine\"\n",
    "    print(f\"  P({label}) = {prior:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate multi-feature classifier\n",
    "predictions_multi = nb_classifier.predict(X)\n",
    "\n",
    "print(\"Multi-Feature Naive Bayes Classifier\")\n",
    "print(\"=\" * 50 + \"\\n\")\n",
    "metrics_multi = evaluate_classifier(y.values, predictions_multi)\n",
    "\n",
    "print(f\"\\nComparison with single-feature classifier:\")\n",
    "print(f\"  Single feature accuracy: {metrics['accuracy']:.3f}\")\n",
    "print(f\"  Multi-feature accuracy:  {metrics_multi['accuracy']:.3f}\")\n",
    "print(f\"  Improvement: {(metrics_multi['accuracy'] - metrics['accuracy'])*100:.1f} percentage points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: The Precision-Recall Tradeoff\n",
    "\n",
    "In the forgery investigation, there's a fundamental tension:\n",
    "\n",
    "- **High Precision**: Only accuse manuscripts we're very confident are forgeries (risk: miss some forgeries)\n",
    "- **High Recall**: Catch all forgeries (risk: falsely accuse genuine manuscripts)\n",
    "\n",
    "By adjusting our decision threshold, we can trade off between these goals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate precision and recall at various thresholds\n",
    "thresholds = np.linspace(0.1, 0.9, 17)\n",
    "posteriors = nb_classifier.predict_proba(X)\n",
    "p_forgery = np.array([p[True] for p in posteriors])\n",
    "\n",
    "precisions = []\n",
    "recalls = []\n",
    "\n",
    "for thresh in thresholds:\n",
    "    preds = p_forgery > thresh\n",
    "    tp = np.sum((y == True) & (preds == True))\n",
    "    fp = np.sum((y == False) & (preds == True))\n",
    "    fn = np.sum((y == True) & (preds == False))\n",
    "    \n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    \n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n",
    "\n",
    "# Plot precision-recall tradeoff\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: Precision and Recall vs Threshold\n",
    "axes[0].plot(thresholds, precisions, 'b-', linewidth=2, label='Precision')\n",
    "axes[0].plot(thresholds, recalls, 'r-', linewidth=2, label='Recall')\n",
    "axes[0].axvline(0.5, color='gray', linestyle='--', label='Default threshold')\n",
    "axes[0].set_xlabel('Decision Threshold', fontsize=12)\n",
    "axes[0].set_ylabel('Score', fontsize=12)\n",
    "axes[0].set_title('Precision and Recall vs. Threshold', fontsize=14)\n",
    "axes[0].legend()\n",
    "axes[0].set_ylim(0, 1)\n",
    "\n",
    "# Right: Precision-Recall Curve\n",
    "axes[1].plot(recalls, precisions, 'purple', linewidth=2)\n",
    "axes[1].scatter([recalls[len(thresholds)//2]], [precisions[len(thresholds)//2]], \n",
    "                color='red', s=100, zorder=5, label='Threshold = 0.5')\n",
    "axes[1].set_xlabel('Recall', fontsize=12)\n",
    "axes[1].set_ylabel('Precision', fontsize=12)\n",
    "axes[1].set_title('Precision-Recall Curve', fontsize=14)\n",
    "axes[1].legend()\n",
    "axes[1].set_xlim(0, 1)\n",
    "axes[1].set_ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Interpretation:\")\n",
    "print(\"  - Higher threshold → Higher precision, lower recall\")\n",
    "print(\"  - Lower threshold → Lower precision, higher recall\")\n",
    "print(\"  - The 'ideal' point depends on the costs of each error type\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Case Study — A Specific Manuscript\n",
    "\n",
    "Let's walk through the Bayesian classification of a specific manuscript that the Senate is investigating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a manuscript to investigate\n",
    "suspect_manuscript = manuscripts[manuscripts['attributed_author'] == 'Grigsu Haldo'].iloc[0]\n",
    "\n",
    "print(\"Senate Investigation: Manuscript Analysis\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Manuscript ID: {suspect_manuscript['manuscript_id']}\")\n",
    "print(f\"Attributed to: {suspect_manuscript['attributed_author']}\")\n",
    "print(f\"School: {suspect_manuscript['attributed_school']}\")\n",
    "print(f\"Composition Year: {suspect_manuscript['composition_year']}\")\n",
    "print(f\"\\nActual status: {'FORGERY' if suspect_manuscript['is_forgery'] else 'GENUINE'}\")\n",
    "if suspect_manuscript['is_forgery']:\n",
    "    print(f\"True author: {suspect_manuscript['true_author']}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Feature Analysis\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Calculate contribution of each feature\n",
    "X_suspect = suspect_manuscript[features].to_frame().T\n",
    "\n",
    "print(f\"\\n{'Feature':<30} {'Value':<10} {'Gen Prob':<10} {'Forg Prob':<10} {'Favors':<10}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "log_odds_contributions = []\n",
    "\n",
    "for feature in features:\n",
    "    val = suspect_manuscript[feature]\n",
    "    \n",
    "    gen_prob = stats.norm.pdf(val, genuine[feature].mean(), genuine[feature].std())\n",
    "    forg_prob = stats.norm.pdf(val, forged[feature].mean(), forged[feature].std())\n",
    "    \n",
    "    log_ratio = np.log(forg_prob / gen_prob) if gen_prob > 0 else 0\n",
    "    log_odds_contributions.append(log_ratio)\n",
    "    \n",
    "    favors = \"Forgery\" if forg_prob > gen_prob else \"Genuine\"\n",
    "    print(f\"{feature:<30} {val:<10.4f} {gen_prob:<10.4f} {forg_prob:<10.4f} {favors:<10}\")\n",
    "\n",
    "# Final posterior\n",
    "posteriors = nb_classifier.predict_proba(X_suspect)\n",
    "p_forg = posteriors[0][True]\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Bayesian Conclusion\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Prior P(Forgery): {p_forgery:.3f}\")\n",
    "print(f\"Posterior P(Forgery | Evidence): {p_forg:.3f}\")\n",
    "print(f\"\\nClassification: {'FORGERY' if p_forg > 0.5 else 'GENUINE'}\")\n",
    "print(f\"Confidence: {max(p_forg, 1-p_forg):.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Cross-Validation\n",
    "\n",
    "We've been evaluating on the same data we trained on, which can lead to overly optimistic results. Let's use cross-validation to get a more honest estimate of performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# 5-fold cross-validation\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "cv_results = {'accuracy': [], 'precision': [], 'recall': [], 'f1': []}\n",
    "\n",
    "print(\"5-Fold Cross-Validation Results\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Fold':<6} {'Accuracy':<12} {'Precision':<12} {'Recall':<12} {'F1':<12}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(kfold.split(X, y), 1):\n",
    "    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "    \n",
    "    # Train\n",
    "    clf = NaiveBayesClassifier()\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict\n",
    "    preds = clf.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    tp = np.sum((y_test == True) & (preds == True))\n",
    "    tn = np.sum((y_test == False) & (preds == False))\n",
    "    fp = np.sum((y_test == False) & (preds == True))\n",
    "    fn = np.sum((y_test == True) & (preds == False))\n",
    "    \n",
    "    accuracy = (tp + tn) / len(y_test)\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    cv_results['accuracy'].append(accuracy)\n",
    "    cv_results['precision'].append(precision)\n",
    "    cv_results['recall'].append(recall)\n",
    "    cv_results['f1'].append(f1)\n",
    "    \n",
    "    print(f\"{fold:<6} {accuracy:<12.3f} {precision:<12.3f} {recall:<12.3f} {f1:<12.3f}\")\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Mean':<6} {np.mean(cv_results['accuracy']):<12.3f} {np.mean(cv_results['precision']):<12.3f} \"\n",
    "      f\"{np.mean(cv_results['recall']):<12.3f} {np.mean(cv_results['f1']):<12.3f}\")\n",
    "print(f\"{'Std':<6} {np.std(cv_results['accuracy']):<12.3f} {np.std(cv_results['precision']):<12.3f} \"\n",
    "      f\"{np.std(cv_results['recall']):<12.3f} {np.std(cv_results['f1']):<12.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "| Concept | Key Insight | Application |\n",
    "|---------|-------------|-------------|\n",
    "| Bayes' Theorem | Posterior ∝ Likelihood × Prior | Update beliefs with evidence |\n",
    "| Prior | Initial belief before seeing evidence | Base rate of forgeries |\n",
    "| Likelihood | P(evidence \\| class) | How typical is this manuscript for each class? |\n",
    "| Posterior | P(class \\| evidence) | Final probability of forgery |\n",
    "| Naive Bayes | Assume feature independence | Multiply likelihoods |\n",
    "| Confusion Matrix | TP, TN, FP, FN | Understanding error types |\n",
    "| Precision | TP / (TP + FP) | When we accuse, are we right? |\n",
    "| Recall | TP / (TP + FN) | How many forgeries do we catch? |\n",
    "\n",
    "---\n",
    "\n",
    "## Exercises\n",
    "\n",
    "### Exercise 1: Feature Importance\n",
    "\n",
    "Which feature contributes most to the classification? Calculate the average absolute log-odds contribution for each feature across all manuscripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Your code here\n",
    "# Hint: For each feature, calculate log(P(x|Forgery) / P(x|Genuine)) for each manuscript\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Threshold Optimization\n",
    "\n",
    "The Senate wants to minimize false accusations (false positives) while still catching at least 80% of forgeries (recall ≥ 0.80). What threshold should we use?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Your code here\n",
    "# Hint: Search through thresholds to find the highest precision that achieves recall ≥ 0.80\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Prior Sensitivity\n",
    "\n",
    "How sensitive is our classifier to the prior? Suppose a skeptic believes only 5% of manuscripts are forgeries, while an accuser believes 40% are. How would their posterior probabilities differ for the same manuscript?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Your code here\n",
    "# Hint: Modify the prior and recalculate posteriors for a specific manuscript\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Author-Specific Classification\n",
    "\n",
    "Build a classifier specifically for manuscripts attributed to Grigsu Haldo. Does focusing on one author improve performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4: Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Module Complete!\n",
    "\n",
    "Congratulations! You've completed Module 1: Statistics & Probability. You now understand:\n",
    "\n",
    "1. **Uncertainty Intuition**: Populations, samples, and random variables\n",
    "2. **Distributions as Terrain**: Normal, skewed, and fat-tailed distributions\n",
    "3. **Central Limit Theorem**: Why averaging works\n",
    "4. **Hypothesis Testing**: Signal vs. noise, p-values, multiple comparisons\n",
    "5. **Bayesian Classification**: Updating beliefs with evidence\n",
    "\n",
    "In Module 2, we'll explore **Linear Algebra**—the mathematics of vectors and matrices—through the lens of creature similarity, map projections, and feature engineering.\n",
    "\n",
    "*\"The Archives are full of numbers. But numbers without interpretation are just ink on parchment. What we've learned today is how to make those numbers speak.\"*  \n",
    "— Closing statement, Senate Inquiry into the Pavar Forgeries"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
