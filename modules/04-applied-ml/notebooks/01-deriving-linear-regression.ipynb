{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/buildLittleWorlds/ml-math-with-densworld/blob/main/modules/04-applied-ml/notebooks/01-deriving-linear-regression.ipynb)\n",
    "\n",
    "# Lesson 1: Deriving Linear Regression from First Principles\n",
    "\n",
    "*\"The Tribunal demanded evidence. 'Show us,' they said, 'how you know this manuscript is false.' I could not simply declare it—I needed mathematics that would stand before the court. I needed a method that derived its authority from pure logic, not from the intuition of scholars who might be bribed or mistaken.\"*  \n",
    "— Mink Pavar, testimony at the Great Forgery Trial of 912\n",
    "\n",
    "---\n",
    "\n",
    "## The Forgery Trial Begins\n",
    "\n",
    "In the year 912, the Capital Archives faced a crisis. Dozens of manuscripts attributed to the great philosopher Grigsu Haldo had been called into question. The accusations came from an unlikely source: Mink Pavar, the reclusive scholar of the Water School, who claimed that someone had been forging Haldo's works for decades.\n",
    "\n",
    "The Tribunal convened. Careers hung in the balance. Fortunes had been spent on manuscripts now suspected to be worthless. Mink Pavar stood before them with a radical proposition:\n",
    "\n",
    "> *\"Let the numbers speak. Each manuscript has measurable features—sentence length, vocabulary richness, philosophical term density. A true Haldo manuscript will have patterns. A forgery will deviate. I propose we derive a mathematical rule that separates authentic from false.\"*\n",
    "\n",
    "The Tribunal was skeptical. \"And how do we know your rule is correct?\"\n",
    "\n",
    "Mink Pavar smiled. \"Because we will derive it from first principles. We will not assume the answer—we will prove it.\"\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this lesson, you will:\n",
    "1. Understand what \"best fit\" actually means mathematically\n",
    "2. Derive the loss function from intuitive requirements\n",
    "3. Use calculus to find the optimal parameters\n",
    "4. Implement linear regression from scratch on Densworld data\n",
    "5. Connect linear regression to the forgery detection problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Nice plotting defaults\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "# Colab-ready data loading\n",
    "BASE_URL = \"https://raw.githubusercontent.com/buildLittleWorlds/ml-math-with-densworld/main/data/\"\n",
    "\n",
    "# Load the manuscript features data (for forgery detection)\n",
    "manuscripts = pd.read_csv(BASE_URL + \"manuscript_features.csv\")\n",
    "# Load creature market data (for price prediction)\n",
    "market = pd.read_csv(BASE_URL + \"creature_market.csv\")\n",
    "# Load expedition data\n",
    "expeditions = pd.read_csv(BASE_URL + \"expedition_outcomes.csv\")\n",
    "\n",
    "print(f\"Loaded {len(manuscripts)} manuscript records\")\n",
    "print(f\"Loaded {len(market)} market transactions\")\n",
    "print(f\"Loaded {len(expeditions)} expedition records\")\n",
    "print(f\"\\nForgeries in manuscript data: {manuscripts['is_forgery'].sum()} ({manuscripts['is_forgery'].mean()*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: The Problem Statement\n",
    "\n",
    "*\"Before we can find the answer, we must precisely state the question. The Tribunal wanted me to identify forgeries. But I realized: the first step is simpler. Can we predict one measurable feature from another?\"*  \n",
    "— Mink Pavar\n",
    "\n",
    "### The Miasto Trappers Guild's Question\n",
    "\n",
    "Before tackling the complex forgery problem, let's start with a simpler question from the Miasto Trappers Guild:\n",
    "\n",
    "> \"Can we predict a creature's market price from its danger rating?\"\n",
    "\n",
    "The Guild believes: *\"Dangerous creatures fetch higher prices.\"*\n",
    "\n",
    "Let's see if this holds—and find the \"best\" line to describe the relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by creature to get average price per danger level\n",
    "creature_stats = market.groupby('creature_id').agg({\n",
    "    'creature_name': 'first',\n",
    "    'danger_rating': 'first',\n",
    "    'rarity_rating': 'first',\n",
    "    'price_per_unit': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "creature_stats.columns = ['id', 'name', 'danger', 'rarity', 'avg_price']\n",
    "\n",
    "# Plot danger vs price\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(creature_stats['danger'], creature_stats['avg_price'], \n",
    "            s=100, c='darkred', edgecolor='black', alpha=0.7)\n",
    "\n",
    "for _, row in creature_stats.iterrows():\n",
    "    plt.annotate(row['name'].split()[0], \n",
    "                 xy=(row['danger'], row['avg_price']),\n",
    "                 xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "\n",
    "plt.xlabel('Danger Rating', fontsize=12)\n",
    "plt.ylabel('Average Price', fontsize=12)\n",
    "plt.title('Creature Danger vs. Market Price\\nWhat is the \"Best\" Line?', fontsize=13)\n",
    "plt.show()\n",
    "\n",
    "print(\"The Miasto Guild's hypothesis seems plausible.\")\n",
    "print(\"But what exactly is the relationship? And what is 'best'?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Defining \"Best\" — Building the Loss Function\n",
    "\n",
    "*\"The Tribunal asked me: 'What makes one line better than another?' I told them: a good line makes small errors. But then they asked: 'How do you combine many small errors into a single measure of goodness?' This was the key question.\"*  \n",
    "— Mink Pavar\n",
    "\n",
    "Let's think step by step about what we want.\n",
    "\n",
    "### Step 1: What are we fitting?\n",
    "\n",
    "A line: $\\hat{y} = w \\cdot x + b$\n",
    "\n",
    "where:\n",
    "- $x$ = danger rating (input)\n",
    "- $\\hat{y}$ = predicted price (output)\n",
    "- $w$ = slope (how much does price increase per danger point?)\n",
    "- $b$ = intercept (base price for a harmless creature)\n",
    "\n",
    "### Step 2: What is an error?\n",
    "\n",
    "For each creature, the **residual** is the vertical distance between actual and predicted price:\n",
    "\n",
    "$$\\text{residual}_i = y_i - \\hat{y}_i = \\text{actual price} - \\text{predicted price}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract our data\n",
    "X = creature_stats['danger'].values\n",
    "y = creature_stats['avg_price'].values\n",
    "\n",
    "# Let's try a guess: w=50, b=0 (price increases 50 per danger point)\n",
    "w_guess = 50\n",
    "b_guess = 0\n",
    "\n",
    "y_pred = w_guess * X + b_guess\n",
    "residuals = y - y_pred\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X, y, s=100, c='darkred', edgecolor='black', label='Actual prices')\n",
    "plt.plot(X, y_pred, 'b-', linewidth=2, label=f'Guess: price = {w_guess}×danger + {b_guess}')\n",
    "\n",
    "# Draw residuals as vertical lines\n",
    "for i in range(len(X)):\n",
    "    color = 'green' if residuals[i] > 0 else 'red'\n",
    "    plt.vlines(X[i], y_pred[i], y[i], colors=color, linestyles='dashed', linewidth=2)\n",
    "\n",
    "plt.xlabel('Danger Rating', fontsize=12)\n",
    "plt.ylabel('Price', fontsize=12)\n",
    "plt.title('Residuals: The Vertical Errors\\n(Green = underpredicted, Red = overpredicted)', fontsize=13)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"Residuals (actual - predicted):\")\n",
    "for name, res in zip(creature_stats['name'], residuals):\n",
    "    sign = '+' if res > 0 else ''\n",
    "    print(f\"  {name:25}: {sign}{res:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: The Summing Problem\n",
    "\n",
    "*\"My first instinct was to add up all the errors. But I quickly realized my mistake...\"*  \n",
    "— Mink Pavar\n",
    "\n",
    "We might think: \"Just minimize the sum of residuals!\"\n",
    "\n",
    "**Problem**: Positive and negative residuals cancel out!\n",
    "\n",
    "A line could be terribly wrong in both directions, but have a sum near zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Sum of residuals: {residuals.sum():.1f}\")\n",
    "print(f\"But total absolute error: {np.abs(residuals).sum():.1f}\")\n",
    "print(\"\\nThe sum can be misleadingly small due to cancellation!\")\n",
    "print(\"\\nMink Pavar's insight: 'We need all errors to count positively.'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: The Solution — Square the Residuals\n",
    "\n",
    "*\"I considered taking absolute values—but the absolute value function has sharp corners, making calculus difficult. Squaring is smooth everywhere. And it has another virtue: large errors are penalized more heavily. A line that makes one huge mistake is worse than a line that makes several small ones.\"*  \n",
    "— Mink Pavar\n",
    "\n",
    "**Why squaring wins:**\n",
    "- Squaring is differentiable everywhere (important for calculus!)\n",
    "- Squaring penalizes large errors more heavily\n",
    "- The math works out cleanly\n",
    "\n",
    "### The Loss Function: Mean Squared Error (MSE)\n",
    "\n",
    "$$\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - wx_i - b)^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mse(w, b, X, y):\n",
    "    \"\"\"Compute Mean Squared Error for price prediction.\"\"\"\n",
    "    y_pred = w * X + b\n",
    "    return np.mean((y - y_pred)**2)\n",
    "\n",
    "# Our guess\n",
    "mse_guess = compute_mse(w_guess, b_guess, X, y)\n",
    "print(f\"MSE for our guess (w={w_guess}, b={b_guess}): {mse_guess:.1f}\")\n",
    "\n",
    "# Try a different line\n",
    "mse_alt = compute_mse(40, 20, X, y)\n",
    "print(f\"MSE for alternative (w=40, b=20): {mse_alt:.1f}\")\n",
    "\n",
    "print(f\"\\nLower MSE = Better fit\")\n",
    "print(f\"The alternative is {'better' if mse_alt < mse_guess else 'worse'}!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: The Loss Landscape\n",
    "\n",
    "*\"Imagine a vast terrain where every point represents a choice of parameters. The height at each point is the error. We seek the lowest valley—the parameters that minimize error. This is optimization.\"*  \n",
    "— Mink Pavar, addressing the Tribunal\n",
    "\n",
    "Now we have a loss function. How do we find the $w$ and $b$ that minimize it?\n",
    "\n",
    "Let's visualize the \"loss landscape\"—how MSE changes with different parameter values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a grid of w and b values\n",
    "w_range = np.linspace(-20, 80, 100)\n",
    "b_range = np.linspace(-100, 200, 100)\n",
    "W, B = np.meshgrid(w_range, b_range)\n",
    "\n",
    "# Compute MSE for each combination\n",
    "MSE = np.zeros_like(W)\n",
    "for i in range(len(b_range)):\n",
    "    for j in range(len(w_range)):\n",
    "        MSE[i, j] = compute_mse(W[i, j], B[i, j], X, y)\n",
    "\n",
    "# Plot the loss landscape\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "contour = ax.contourf(W, B, np.log10(MSE + 1), levels=50, cmap='viridis')\n",
    "ax.contour(W, B, np.log10(MSE + 1), levels=15, colors='white', alpha=0.3, linewidths=0.5)\n",
    "plt.colorbar(contour, label='log10(MSE)')\n",
    "\n",
    "ax.set_xlabel('Weight (w) — Price Increase per Danger Point', fontsize=12)\n",
    "ax.set_ylabel('Bias (b) — Base Price', fontsize=12)\n",
    "ax.set_title('The Loss Landscape\\nDarker = Lower MSE = Better Fit', fontsize=13)\n",
    "\n",
    "# Mark our guess\n",
    "ax.plot(w_guess, b_guess, 'ro', markersize=12, label='Our guess')\n",
    "ax.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"The dark valley is where the optimal parameters live.\")\n",
    "print(\"Our goal: find the lowest point in this landscape.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: The Analytical Solution\n",
    "\n",
    "*\"The Tribunal was impressed by the landscape visualization, but they demanded more. 'Can you prove that your solution is truly optimal?' I showed them that at the minimum, the derivative equals zero—this is the condition for optimality.\"*  \n",
    "— Mink Pavar\n",
    "\n",
    "For simple linear regression, we can solve directly by setting the derivatives to zero.\n",
    "\n",
    "### The Derivation (from Module 3: Calculus)\n",
    "\n",
    "We want to minimize:\n",
    "$$L(w, b) = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - wx_i - b)^2$$\n",
    "\n",
    "Setting $\\frac{\\partial L}{\\partial w} = 0$ and $\\frac{\\partial L}{\\partial b} = 0$ gives us:\n",
    "\n",
    "$$w^* = \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum (x_i - \\bar{x})^2}$$\n",
    "\n",
    "$$b^* = \\bar{y} - w^*\\bar{x}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_linear_regression(X, y):\n",
    "    \"\"\"Compute optimal w and b using the closed-form solution.\"\"\"\n",
    "    x_mean = np.mean(X)\n",
    "    y_mean = np.mean(y)\n",
    "    \n",
    "    # Optimal weight\n",
    "    numerator = np.sum((X - x_mean) * (y - y_mean))\n",
    "    denominator = np.sum((X - x_mean)**2)\n",
    "    w = numerator / denominator\n",
    "    \n",
    "    # Optimal bias\n",
    "    b = y_mean - w * x_mean\n",
    "    \n",
    "    return w, b\n",
    "\n",
    "w_optimal, b_optimal = fit_linear_regression(X, y)\n",
    "mse_optimal = compute_mse(w_optimal, b_optimal, X, y)\n",
    "\n",
    "print(\"Optimal Parameters for Creature Pricing:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"  w = {w_optimal:.2f} (price increases by ~{w_optimal:.0f} per danger point)\")\n",
    "print(f\"  b = {b_optimal:.2f} (base price for danger=0 creature)\")\n",
    "print(f\"  MSE = {mse_optimal:.1f}\")\n",
    "print(f\"\\nOur guess MSE was {mse_guess:.1f} — optimal is {mse_guess/mse_optimal:.1f}× better!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the optimal line\n",
    "x_line = np.linspace(0, 10, 100)\n",
    "y_optimal = w_optimal * x_line + b_optimal\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X, y, s=100, c='darkred', edgecolor='black', label='Actual prices')\n",
    "plt.plot(x_line, y_optimal, 'b-', linewidth=2, \n",
    "         label=f'Best fit: price = {w_optimal:.1f}×danger + {b_optimal:.1f}')\n",
    "\n",
    "# Draw residuals\n",
    "y_pred_optimal = w_optimal * X + b_optimal\n",
    "for i in range(len(X)):\n",
    "    plt.vlines(X[i], y_pred_optimal[i], y[i], colors='green', \n",
    "               linestyles='dashed', alpha=0.5)\n",
    "\n",
    "# Add creature names\n",
    "for _, row in creature_stats.iterrows():\n",
    "    plt.annotate(row['name'].split()[0], \n",
    "                 xy=(row['danger'], row['avg_price']),\n",
    "                 xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "\n",
    "plt.xlabel('Danger Rating', fontsize=12)\n",
    "plt.ylabel('Average Price', fontsize=12)\n",
    "plt.title('The Optimal Line Minimizes Squared Residuals', fontsize=13)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nInterpretation for the Miasto Guild:\")\n",
    "print(f\"  - A danger-0 creature is worth ~{b_optimal:.0f} on average\")\n",
    "print(f\"  - Each point of danger adds ~{w_optimal:.0f} to the price\")\n",
    "print(f\"  - A Stakdur (danger=9) should fetch ~{w_optimal*9 + b_optimal:.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Gradient Descent — The General-Purpose Algorithm\n",
    "\n",
    "*\"The closed-form solution works for simple problems. But what of complex ones—with many features, many parameters? The Tribunal would need a more general method. I showed them the algorithm of descent: follow the slope downhill until you reach the valley floor.\"*  \n",
    "— Mink Pavar\n",
    "\n",
    "The closed-form solution works for simple linear regression. But for more complex models (neural networks, etc.), there's no closed form.\n",
    "\n",
    "**Gradient Descent** is the general-purpose optimization algorithm:\n",
    "\n",
    "1. Start with random w, b\n",
    "2. Compute gradient (which direction increases loss?)\n",
    "3. Take a step in the opposite direction\n",
    "4. Repeat until convergence\n",
    "\n",
    "This connects back to Module 3—the Colonel's siege was gradient descent in the fog of war!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, learning_rate=0.001, n_iterations=1000):\n",
    "    \"\"\"Find optimal w, b using gradient descent.\"\"\"\n",
    "    # Initialize with a guess\n",
    "    w = 10.0  # Start with a low estimate\n",
    "    b = 50.0\n",
    "    n = len(X)\n",
    "    \n",
    "    history = {'w': [w], 'b': [b], 'mse': [compute_mse(w, b, X, y)]}\n",
    "    \n",
    "    for iteration in range(n_iterations):\n",
    "        # Compute predictions\n",
    "        y_pred = w * X + b\n",
    "        \n",
    "        # Compute gradients (partial derivatives of MSE)\n",
    "        dw = -2/n * np.sum(X * (y - y_pred))\n",
    "        db = -2/n * np.sum(y - y_pred)\n",
    "        \n",
    "        # Update parameters (move opposite to gradient)\n",
    "        w = w - learning_rate * dw\n",
    "        b = b - learning_rate * db\n",
    "        \n",
    "        # Record history\n",
    "        history['w'].append(w)\n",
    "        history['b'].append(b)\n",
    "        history['mse'].append(compute_mse(w, b, X, y))\n",
    "    \n",
    "    return w, b, history\n",
    "\n",
    "# Run gradient descent\n",
    "w_gd, b_gd, history = gradient_descent(X, y, learning_rate=0.005, n_iterations=500)\n",
    "\n",
    "print(\"Gradient Descent Results:\")\n",
    "print(f\"  w = {w_gd:.2f} (optimal: {w_optimal:.2f})\")\n",
    "print(f\"  b = {b_gd:.2f} (optimal: {b_optimal:.2f})\")\n",
    "print(f\"  MSE = {history['mse'][-1]:.1f} (optimal: {mse_optimal:.1f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the gradient descent journey\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Path on loss landscape\n",
    "contour = axes[0].contourf(W, B, np.log10(MSE + 1), levels=50, cmap='viridis')\n",
    "axes[0].plot(history['w'], history['b'], 'r.-', markersize=2, linewidth=1, alpha=0.7)\n",
    "axes[0].plot(history['w'][0], history['b'][0], 'ro', markersize=10, label='Start')\n",
    "axes[0].plot(w_optimal, b_optimal, 'g*', markersize=15, label='Optimal')\n",
    "axes[0].set_xlabel('Weight (w)', fontsize=11)\n",
    "axes[0].set_ylabel('Bias (b)', fontsize=11)\n",
    "axes[0].set_title('Gradient Descent Path on Loss Landscape', fontsize=12)\n",
    "axes[0].legend()\n",
    "\n",
    "# MSE over iterations\n",
    "axes[1].plot(history['mse'], 'b-', linewidth=2)\n",
    "axes[1].axhline(mse_optimal, color='green', linestyle='--', label='Optimal MSE')\n",
    "axes[1].set_xlabel('Iteration', fontsize=11)\n",
    "axes[1].set_ylabel('MSE', fontsize=11)\n",
    "axes[1].set_title('Loss Decreasing During Training', fontsize=12)\n",
    "axes[1].legend()\n",
    "axes[1].set_yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Left: The red path shows gradient descent 'walking downhill'\")\n",
    "print(\"Right: MSE decreases rapidly at first, then plateaus\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Applying to the Forgery Problem\n",
    "\n",
    "*\"The Tribunal saw that I could predict prices from danger ratings. 'But what of manuscripts?' they asked. 'Can your method detect forgeries?' I told them: first we must find features that correlate with authenticity.\"*  \n",
    "— Mink Pavar\n",
    "\n",
    "Let's apply linear regression to the manuscript data. One hypothesis: stylometric variance (inconsistency in writing style) might predict whether a manuscript is a forgery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the manuscript features\n",
    "print(\"Manuscript Features for Forgery Detection:\")\n",
    "print(manuscripts[['manuscript_id', 'attributed_author', 'is_forgery', \n",
    "                   'stylometric_variance', 'era_marker_score']].head(10))\n",
    "\n",
    "# Compare authentic vs forgery\n",
    "authentic = manuscripts[manuscripts['is_forgery'] == False]\n",
    "forgeries = manuscripts[manuscripts['is_forgery'] == True]\n",
    "\n",
    "print(f\"\\nAuthentic manuscripts: {len(authentic)}\")\n",
    "print(f\"Forgeries: {len(forgeries)}\")\n",
    "print(f\"\\nMean stylometric variance:\")\n",
    "print(f\"  Authentic: {authentic['stylometric_variance'].mean():.4f}\")\n",
    "print(f\"  Forgeries: {forgeries['stylometric_variance'].mean():.4f}\")\n",
    "print(f\"\\nMean era marker score:\")\n",
    "print(f\"  Authentic: {authentic['era_marker_score'].mean():.4f}\")\n",
    "print(f\"  Forgeries: {forgeries['era_marker_score'].mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the relationship\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Stylometric variance vs forgery\n",
    "axes[0].scatter(authentic['stylometric_variance'], [0]*len(authentic), \n",
    "                alpha=0.5, label='Authentic', c='green', s=50)\n",
    "axes[0].scatter(forgeries['stylometric_variance'], [1]*len(forgeries), \n",
    "                alpha=0.5, label='Forgery', c='red', s=50)\n",
    "axes[0].set_xlabel('Stylometric Variance', fontsize=11)\n",
    "axes[0].set_ylabel('Is Forgery (0=No, 1=Yes)', fontsize=11)\n",
    "axes[0].set_title('Stylometric Variance vs. Forgery Status', fontsize=12)\n",
    "axes[0].legend()\n",
    "\n",
    "# Era marker score vs forgery\n",
    "axes[1].scatter(authentic['era_marker_score'], [0]*len(authentic), \n",
    "                alpha=0.5, label='Authentic', c='green', s=50)\n",
    "axes[1].scatter(forgeries['era_marker_score'], [1]*len(forgeries), \n",
    "                alpha=0.5, label='Forgery', c='red', s=50)\n",
    "axes[1].set_xlabel('Era Marker Score', fontsize=11)\n",
    "axes[1].set_ylabel('Is Forgery (0=No, 1=Yes)', fontsize=11)\n",
    "axes[1].set_title('Era Marker Score vs. Forgery Status', fontsize=12)\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Observation: Forgeries tend to have higher stylometric variance\")\n",
    "print(\"and higher era marker scores (anachronistic terms).\")\n",
    "print(\"\\nMink Pavar: 'The forger's hand betrays inconsistency.'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit linear regression to predict forgery probability from stylometric variance\n",
    "X_ms = manuscripts['stylometric_variance'].values\n",
    "y_ms = manuscripts['is_forgery'].astype(int).values\n",
    "\n",
    "w_ms, b_ms = fit_linear_regression(X_ms, y_ms)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_ms, y_ms + np.random.normal(0, 0.02, len(y_ms)), \n",
    "            alpha=0.5, c=['green' if y==0 else 'red' for y in y_ms], s=50)\n",
    "\n",
    "x_line = np.linspace(0, X_ms.max(), 100)\n",
    "y_line = w_ms * x_line + b_ms\n",
    "plt.plot(x_line, y_line, 'b-', linewidth=2, label=f'Prediction = {w_ms:.2f}×variance + {b_ms:.2f}')\n",
    "plt.axhline(0.5, color='orange', linestyle='--', label='Decision boundary (0.5)')\n",
    "\n",
    "plt.xlabel('Stylometric Variance', fontsize=12)\n",
    "plt.ylabel('Forgery Probability', fontsize=12)\n",
    "plt.title('Linear Regression for Forgery Detection\\n(Note: This is a preview—logistic regression works better!)', fontsize=13)\n",
    "plt.legend()\n",
    "plt.ylim(-0.2, 1.2)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Model: forgery_probability = {w_ms:.4f} × stylometric_variance + {b_ms:.4f}\")\n",
    "print(f\"\\nFor a manuscript with variance = 0.1: probability = {w_ms*0.1 + b_ms:.2f}\")\n",
    "print(f\"For a manuscript with variance = 0.3: probability = {w_ms*0.3 + b_ms:.2f}\")\n",
    "print(\"\\nNote: Linear regression can give probabilities outside [0,1]!\")\n",
    "print(\"We'll fix this with logistic regression in a later course.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Expedition Data — Multiple Features Preview\n",
    "\n",
    "*\"One feature is a start. But the world is multidimensional. True prediction requires many features working together.\"*  \n",
    "— Mink Pavar\n",
    "\n",
    "Let's apply regression to expedition data, previewing multiple features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple regression: days in field vs catch value\n",
    "X_exp = expeditions['days_in_field'].values\n",
    "y_exp = expeditions['catch_value'].values\n",
    "\n",
    "# Fit regression\n",
    "w_exp, b_exp = fit_linear_regression(X_exp, y_exp)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_exp, y_exp, alpha=0.3, s=20, c='steelblue')\n",
    "\n",
    "x_line = np.linspace(X_exp.min(), X_exp.max(), 100)\n",
    "y_line = w_exp * x_line + b_exp\n",
    "plt.plot(x_line, y_line, 'r-', linewidth=2, \n",
    "         label=f'catch = {w_exp:.1f}×days + {b_exp:.1f}')\n",
    "\n",
    "plt.xlabel('Days in Field', fontsize=12)\n",
    "plt.ylabel('Catch Value', fontsize=12)\n",
    "plt.title('Expedition Duration vs. Catch Value', fontsize=13)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Model: catch_value = {w_exp:.2f} × days + {b_exp:.2f}\")\n",
    "print(f\"\\nInterpretation: Each additional day adds ~{w_exp:.1f} to catch value\")\n",
    "\n",
    "# Calculate R-squared\n",
    "y_pred_exp = w_exp * X_exp + b_exp\n",
    "ss_res = np.sum((y_exp - y_pred_exp)**2)\n",
    "ss_tot = np.sum((y_exp - y_exp.mean())**2)\n",
    "r_squared = 1 - ss_res / ss_tot\n",
    "print(f\"R² = {r_squared:.3f} ({r_squared*100:.1f}% of variance explained)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercises\n",
    "\n",
    "### Exercise 1: Multi-feature Pricing\n",
    "\n",
    "The Miasto Guild suspects that **rarity** also affects price, not just danger. Fit a model using both features:\n",
    "\n",
    "$$\\text{price} = w_1 \\cdot \\text{danger} + w_2 \\cdot \\text{rarity} + b$$\n",
    "\n",
    "*Hint: Use `np.linalg.lstsq` for the matrix solution.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Multi-feature model\n",
    "# Build feature matrix with danger, rarity, and bias term\n",
    "\n",
    "X_multi = np.column_stack([\n",
    "    creature_stats['danger'].values,\n",
    "    creature_stats['rarity'].values,\n",
    "    np.ones(len(creature_stats))  # bias term\n",
    "])\n",
    "y_multi = creature_stats['avg_price'].values\n",
    "\n",
    "# Solve using least squares\n",
    "# YOUR CODE HERE\n",
    "# weights, residuals, rank, s = np.linalg.lstsq(X_multi, y_multi, rcond=None)\n",
    "# print(f\"Danger weight: {weights[0]:.2f}\")\n",
    "# print(f\"Rarity weight: {weights[1]:.2f}\")\n",
    "# print(f\"Bias: {weights[2]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Learning Rate Experiments\n",
    "\n",
    "The Tribunal asks: \"What happens if we step too fast or too slow?\"\n",
    "\n",
    "Try gradient descent with:\n",
    "- `learning_rate = 0.1` (too fast?)\n",
    "- `learning_rate = 0.0001` (too slow?)\n",
    "\n",
    "What happens in each case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Learning rate experiments\n",
    "# Try different learning rates and observe the behavior\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# w_fast, b_fast, history_fast = gradient_descent(X, y, learning_rate=0.1, n_iterations=100)\n",
    "# w_slow, b_slow, history_slow = gradient_descent(X, y, learning_rate=0.0001, n_iterations=500)\n",
    "\n",
    "# Plot comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Outlier Analysis\n",
    "\n",
    "The Golden Amalgam Snail fetches extreme prices. Mink Pavar warns: \"Outliers can distort our models.\"\n",
    "\n",
    "Remove the Golden Amalgam Snail and refit. How do the parameters change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Outlier analysis\n",
    "# Remove the outlier and compare\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# no_outlier = creature_stats[~creature_stats['name'].str.contains('Golden Amalgam')]\n",
    "# X_clean = no_outlier['danger'].values\n",
    "# y_clean = no_outlier['avg_price'].values\n",
    "# w_clean, b_clean = fit_linear_regression(X_clean, y_clean)\n",
    "# print(f\"Original: w={w_optimal:.2f}, b={b_optimal:.2f}\")\n",
    "# print(f\"Without outlier: w={w_clean:.2f}, b={b_clean:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Forgery Features\n",
    "\n",
    "Mink Pavar wants to know: which manuscript feature best predicts forgery?\n",
    "\n",
    "Compare R² values for:\n",
    "- `stylometric_variance`\n",
    "- `era_marker_score`\n",
    "- `vocabulary_richness`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4: Compare forgery predictors\n",
    "# Calculate R² for each feature\n",
    "\n",
    "features_to_test = ['stylometric_variance', 'era_marker_score', 'vocabulary_richness']\n",
    "y_forgery = manuscripts['is_forgery'].astype(int).values\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# for feature in features_to_test:\n",
    "#     X_feat = manuscripts[feature].values\n",
    "#     w_feat, b_feat = fit_linear_regression(X_feat, y_forgery)\n",
    "#     y_pred = w_feat * X_feat + b_feat\n",
    "#     ss_res = np.sum((y_forgery - y_pred)**2)\n",
    "#     ss_tot = np.sum((y_forgery - y_forgery.mean())**2)\n",
    "#     r2 = 1 - ss_res / ss_tot\n",
    "#     print(f\"{feature}: R² = {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: The Forgery Trial Challenge\n",
    "\n",
    "The Tribunal gives you 5 manuscripts. Based on `stylometric_variance` alone, predict which are forgeries (probability > 0.5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 5: Predict on new manuscripts\n",
    "# Use the model from Part 6\n",
    "\n",
    "test_variances = [0.05, 0.12, 0.25, 0.08, 0.35]\n",
    "test_ids = ['MS-TEST-1', 'MS-TEST-2', 'MS-TEST-3', 'MS-TEST-4', 'MS-TEST-5']\n",
    "\n",
    "print(\"Forgery Predictions:\")\n",
    "print(\"=\" * 50)\n",
    "# YOUR CODE HERE\n",
    "# for ms_id, variance in zip(test_ids, test_variances):\n",
    "#     prob = w_ms * variance + b_ms\n",
    "#     prediction = \"FORGERY\" if prob > 0.5 else \"Authentic\"\n",
    "#     print(f\"{ms_id}: variance={variance:.2f} -> prob={prob:.3f} -> {prediction}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "| Concept | Key Insight | Forgery Trial Application |\n",
    "|---------|-------------|---------------------------|\n",
    "| **Loss Function** | MSE measures average squared error | How far are predictions from truth? |\n",
    "| **Why Squared?** | Differentiable, penalizes outliers, no cancellation | Ensures all errors count positively |\n",
    "| **Closed-Form Solution** | Set derivative = 0, solve directly | Works for simple linear regression |\n",
    "| **Gradient Descent** | Iteratively follow the downhill slope | General method for any model |\n",
    "| **R²** | Proportion of variance explained | How good is our forgery predictor? |\n",
    "\n",
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **\"Best fit\" means minimize squared errors** — we derived this from first principles, just as Mink Pavar did for the Tribunal\n",
    "\n",
    "2. **The loss landscape is smooth** — this is why optimization works; we can always find which direction is downhill\n",
    "\n",
    "3. **Two ways to find the minimum**: closed-form (when available) or gradient descent (general purpose)\n",
    "\n",
    "4. **Linear regression is a building block** — it's the foundation for more complex models\n",
    "\n",
    "5. **The Forgery Trial continues** — we can detect forgeries, but our method has limitations (probabilities outside [0,1])\n",
    "\n",
    "---\n",
    "\n",
    "## Next Lesson\n",
    "\n",
    "In **Lesson 2: The Bias-Variance Trade-off**, we'll discover the most important theoretical concept in machine learning. \n",
    "\n",
    "*\"The Tribunal asked me a dangerous question: 'If we give you more data, more features, more complexity—will your predictions improve?' I told them the truth: not necessarily. There is a hidden tension between simplicity and complexity, between fitting the past and predicting the future. This tension is the bias-variance trade-off, and understanding it is the key to all of machine learning.\"*  \n",
    "— Mink Pavar"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
