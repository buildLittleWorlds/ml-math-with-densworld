{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/buildLittleWorlds/ml-math-with-densworld/blob/main/modules/04-applied-ml/notebooks/04-model-selection-capstone.ipynb)\n",
    "\n",
    "# Lesson 4: Model Selection & Cross-Validation (Capstone)\n",
    "\n",
    "*\"The final day of testimony arrived. The Tribunal demanded a verdict: could my methods reliably identify forgeries? I proposed a test that would satisfy even the most skeptical judgeâ€”cross-validation. We would repeatedly hide some data, train on the rest, and measure performance on what was hidden. Only a model that consistently performed well across all such tests could be trusted.\"*  \n",
    "â€” Mink Pavar, final day of the Great Forgery Trial\n",
    "\n",
    "---\n",
    "\n",
    "## The Final Test\n",
    "\n",
    "The Great Forgery Trial of 912 reached its climax. Mink Pavar had demonstrated linear regression, explained the bias-variance trade-off, and shown how regularization could control complexity. But the Tribunal had one final question:\n",
    "\n",
    "> *\"How do we know your model will work on manuscripts we haven't examined yet? You've shown us training error and test errorâ€”but what if your test set was unusually easy or hard?\"*\n",
    "\n",
    "Mink Pavar smiled. This was the question he had been waiting for.\n",
    "\n",
    "> *\"We will use cross-validationâ€”the most rigorous test of a model's true ability. We will train and test not once, but many times, each time holding out different data. The average performance across all these trials will tell us what to expect on truly unseen manuscripts.\"*\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this capstone lesson, you will:\n",
    "1. Understand why a single train/test split is insufficient\n",
    "2. Implement K-Fold Cross-Validation from scratch\n",
    "3. Use cross-validation to select hyperparameters\n",
    "4. Build a complete ML pipeline for the forgery detection task\n",
    "5. Make principled decisions about model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Nice plotting defaults\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "# Colab-ready data loading\n",
    "BASE_URL = \"https://raw.githubusercontent.com/buildLittleWorlds/ml-math-with-densworld/main/data/\"\n",
    "\n",
    "# Load all datasets\n",
    "manuscripts = pd.read_csv(BASE_URL + \"manuscript_features.csv\")\n",
    "expeditions = pd.read_csv(BASE_URL + \"expedition_outcomes.csv\")\n",
    "creature_market = pd.read_csv(BASE_URL + \"creature_market.csv\")\n",
    "\n",
    "print(f\"Loaded {len(manuscripts)} manuscripts\")\n",
    "print(f\"Loaded {len(expeditions)} expeditions\")\n",
    "print(f\"Loaded {len(creature_market)} market transactions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: The Problem with a Single Split\n",
    "\n",
    "*\"A single test set is like a single witness. It may be truthful, but it may also be unrepresentative. What if, by chance, we happened to hide the easiest manuscripts? Or the hardest? Our confidence in the model would be misplaced.\"*  \n",
    "â€” Mink Pavar\n",
    "\n",
    "### Demonstrating Variability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare manuscript data\n",
    "feature_cols = ['stylometric_variance', 'era_marker_score', 'vocabulary_richness', \n",
    "                'avg_sentence_length', 'philosophical_term_density']\n",
    "\n",
    "X = manuscripts[feature_cols].values\n",
    "y = manuscripts['is_forgery'].astype(int).values\n",
    "\n",
    "# Show how test error varies with different random splits\n",
    "test_errors = []\n",
    "for seed in range(50):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.3, random_state=seed\n",
    "    )\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    model = Ridge(alpha=0.1)\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    test_errors.append(mse)\n",
    "\n",
    "# Plot the variability\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(range(50), test_errors, color='steelblue', alpha=0.7)\n",
    "plt.axhline(np.mean(test_errors), color='red', linestyle='--', linewidth=2, \n",
    "            label=f'Mean: {np.mean(test_errors):.4f}')\n",
    "plt.xlabel('Random Seed', fontsize=11)\n",
    "plt.ylabel('Test MSE', fontsize=11)\n",
    "plt.title('Test Error Varies with Random Split!', fontsize=12)\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(test_errors, bins=15, color='coral', edgecolor='black', alpha=0.7)\n",
    "plt.axvline(np.mean(test_errors), color='red', linestyle='--', linewidth=2, label=f'Mean: {np.mean(test_errors):.4f}')\n",
    "plt.xlabel('Test MSE', fontsize=11)\n",
    "plt.ylabel('Frequency', fontsize=11)\n",
    "plt.title(f'Distribution of Test Errors\\nStd: {np.std(test_errors):.4f}', fontsize=12)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Test MSE range: {min(test_errors):.4f} to {max(test_errors):.4f}\")\n",
    "print(f\"This {max(test_errors)/min(test_errors):.1f}x difference is just from random chance!\")\n",
    "print(\"\\nA single split cannot give us reliable performance estimates.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: K-Fold Cross-Validation\n",
    "\n",
    "*\"Instead of one test, we perform K tests. We divide our data into K equal partsâ€”called folds. Each fold takes a turn being the test set while the others train the model. The average across all K experiments is our estimate.\"*  \n",
    "â€” Mink Pavar\n",
    "\n",
    "### Visualizing K-Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize 5-fold cross-validation\n",
    "n_samples = 20  # Simplified for visualization\n",
    "k_folds = 5\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "\n",
    "colors = ['steelblue', 'coral', 'green', 'purple', 'orange']\n",
    "fold_size = n_samples // k_folds\n",
    "\n",
    "for fold in range(k_folds):\n",
    "    for sample in range(n_samples):\n",
    "        sample_fold = sample // fold_size\n",
    "        if sample_fold == fold:\n",
    "            color = 'red'  # Test\n",
    "            label = 'Test' if sample == fold * fold_size else None\n",
    "        else:\n",
    "            color = 'lightblue'  # Train\n",
    "            label = 'Train' if sample == 0 and fold == 0 else None\n",
    "        \n",
    "        rect = plt.Rectangle((sample, k_folds - fold - 1), 0.9, 0.8, \n",
    "                              facecolor=color, edgecolor='black')\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "ax.set_xlim(-0.5, n_samples + 0.5)\n",
    "ax.set_ylim(-0.5, k_folds + 0.5)\n",
    "ax.set_xlabel('Sample Index', fontsize=12)\n",
    "ax.set_ylabel('Fold Number', fontsize=12)\n",
    "ax.set_yticks(np.arange(k_folds) + 0.4)\n",
    "ax.set_yticklabels([f'Fold {k_folds-i}' for i in range(k_folds)])\n",
    "ax.set_title('5-Fold Cross-Validation\\n(Red = Test, Blue = Train)', fontsize=13)\n",
    "\n",
    "# Legend\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [Patch(facecolor='red', edgecolor='black', label='Test'),\n",
    "                   Patch(facecolor='lightblue', edgecolor='black', label='Train')]\n",
    "ax.legend(handles=legend_elements, loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Each row shows one fold's train/test split.\")\n",
    "print(\"Every sample gets to be in the test set exactly once.\")\n",
    "print(\"We train 5 models and average their performance.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement K-Fold cross-validation from scratch\n",
    "def manual_kfold_cv(X, y, model_class, model_params, k=5):\n",
    "    \"\"\"Perform K-fold cross-validation manually.\"\"\"\n",
    "    n_samples = len(X)\n",
    "    fold_size = n_samples // k\n",
    "    indices = np.arange(n_samples)\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    fold_scores = []\n",
    "    \n",
    "    for fold in range(k):\n",
    "        # Define test indices for this fold\n",
    "        test_start = fold * fold_size\n",
    "        test_end = test_start + fold_size if fold < k-1 else n_samples\n",
    "        test_indices = indices[test_start:test_end]\n",
    "        train_indices = np.concatenate([indices[:test_start], indices[test_end:]])\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_test = X[train_indices], X[test_indices]\n",
    "        y_train, y_test = y[train_indices], y[test_indices]\n",
    "        \n",
    "        # Scale features\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        \n",
    "        # Train model\n",
    "        model = model_class(**model_params)\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        # Evaluate\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        fold_scores.append(mse)\n",
    "        \n",
    "        print(f\"  Fold {fold+1}: MSE = {mse:.4f}\")\n",
    "    \n",
    "    return fold_scores\n",
    "\n",
    "print(\"Manual 5-Fold Cross-Validation for Forgery Detection:\")\n",
    "print(\"=\" * 50)\n",
    "scores = manual_kfold_cv(X, y, Ridge, {'alpha': 0.1})\n",
    "print(\"-\" * 50)\n",
    "print(f\"Mean MSE: {np.mean(scores):.4f} (+/- {np.std(scores):.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare to sklearn's implementation\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Create a pipeline (scaler + model)\n",
    "pipeline = make_pipeline(StandardScaler(), Ridge(alpha=0.1))\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_scores = cross_val_score(pipeline, X, y, cv=5, scoring='neg_mean_squared_error')\n",
    "\n",
    "print(\"\\nSklearn 5-Fold Cross-Validation:\")\n",
    "print(f\"MSE per fold: {-cv_scores}\")\n",
    "print(f\"Mean MSE: {-cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Using CV for Hyperparameter Selection\n",
    "\n",
    "*\"The Tribunal asked: 'How do you choose the regularization strength Î»?' I explained: we use cross-validation to test each candidate value. The Î» that gives the best average performance across all folds is our choice.\"*  \n",
    "â€” Mink Pavar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use CV to select optimal lambda for Ridge\n",
    "lambdas = np.logspace(-4, 2, 20)\n",
    "cv_results = []\n",
    "\n",
    "for lam in lambdas:\n",
    "    pipeline = make_pipeline(StandardScaler(), Ridge(alpha=lam))\n",
    "    scores = cross_val_score(pipeline, X, y, cv=5, scoring='neg_mean_squared_error')\n",
    "    cv_results.append({\n",
    "        'lambda': lam,\n",
    "        'mean_mse': -scores.mean(),\n",
    "        'std_mse': scores.std()\n",
    "    })\n",
    "\n",
    "cv_df = pd.DataFrame(cv_results)\n",
    "\n",
    "# Find best lambda\n",
    "best_idx = cv_df['mean_mse'].idxmin()\n",
    "best_lambda = cv_df.loc[best_idx, 'lambda']\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.errorbar(cv_df['lambda'], cv_df['mean_mse'], yerr=cv_df['std_mse'], \n",
    "             fmt='o-', capsize=3, capthick=1, linewidth=2, markersize=6)\n",
    "plt.axvline(best_lambda, color='green', linestyle='--', linewidth=2,\n",
    "            label=f'Best Î» = {best_lambda:.4f}')\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Î» (Regularization Strength)', fontsize=12)\n",
    "plt.ylabel('Mean CV MSE (+/- std)', fontsize=12)\n",
    "plt.title('Cross-Validation for Hyperparameter Selection', fontsize=13)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Optimal Î»: {best_lambda:.4f}\")\n",
    "print(f\"CV MSE at optimal Î»: {cv_df.loc[best_idx, 'mean_mse']:.4f} (+/- {cv_df.loc[best_idx, 'std_mse']:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Grid Search â€” Systematic Hyperparameter Tuning\n",
    "\n",
    "*\"When we have multiple hyperparameters, we search over a grid of all combinations. This is exhaustive but thorough.\"*  \n",
    "â€” Mink Pavar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search over multiple hyperparameters\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'elasticnet__alpha': [0.001, 0.01, 0.1, 1.0],\n",
    "    'elasticnet__l1_ratio': [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "}\n",
    "\n",
    "# Create pipeline\n",
    "pipeline = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    ElasticNet(max_iter=10000)\n",
    ")\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline, \n",
    "    param_grid, \n",
    "    cv=5, \n",
    "    scoring='neg_mean_squared_error',\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Results\n",
    "print(\"Grid Search Results:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best CV MSE: {-grid_search.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize grid search results\n",
    "results = pd.DataFrame(grid_search.cv_results_)\n",
    "\n",
    "# Pivot for heatmap\n",
    "alphas = param_grid['elasticnet__alpha']\n",
    "l1_ratios = param_grid['elasticnet__l1_ratio']\n",
    "\n",
    "scores_matrix = np.zeros((len(alphas), len(l1_ratios)))\n",
    "for i, alpha in enumerate(alphas):\n",
    "    for j, l1_ratio in enumerate(l1_ratios):\n",
    "        mask = (results['param_elasticnet__alpha'] == alpha) & \\\n",
    "               (results['param_elasticnet__l1_ratio'] == l1_ratio)\n",
    "        scores_matrix[i, j] = -results.loc[mask, 'mean_test_score'].values[0]\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(10, 6))\n",
    "im = plt.imshow(scores_matrix, cmap='viridis_r', aspect='auto')\n",
    "plt.colorbar(im, label='Mean CV MSE')\n",
    "\n",
    "plt.xticks(range(len(l1_ratios)), [f'{r:.1f}' for r in l1_ratios])\n",
    "plt.yticks(range(len(alphas)), [f'{a:.3f}' for a in alphas])\n",
    "plt.xlabel('L1 Ratio (0=Ridge, 1=Lasso)', fontsize=12)\n",
    "plt.ylabel('Alpha (Regularization Strength)', fontsize=12)\n",
    "plt.title('Grid Search: Elastic Net Hyperparameter Tuning\\n(Darker = Better)', fontsize=13)\n",
    "\n",
    "# Mark best\n",
    "best_alpha = grid_search.best_params_['elasticnet__alpha']\n",
    "best_l1 = grid_search.best_params_['elasticnet__l1_ratio']\n",
    "best_i = alphas.index(best_alpha)\n",
    "best_j = l1_ratios.index(best_l1)\n",
    "plt.plot(best_j, best_i, 'r*', markersize=20, label='Best')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: The Complete ML Pipeline â€” Forgery Detection System\n",
    "\n",
    "*\"Now we build the complete system. We will: (1) prepare the data, (2) select features, (3) choose the best model with cross-validation, and (4) evaluate on a truly held-out test set.\"*  \n",
    "â€” Mink Pavar\n",
    "\n",
    "### The Three-Way Split: Train / Validation / Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: Hold out a final test set BEFORE any model selection\n",
    "# This test set must NEVER be used until the very end\n",
    "\n",
    "# First split: separate final test set (20%)\n",
    "X_trainval, X_test_final, y_trainval, y_test_final = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"Data Split Strategy:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Train+Validation set: {len(X_trainval)} samples ({len(X_trainval)/len(X)*100:.0f}%)\")\n",
    "print(f\"Final Test set: {len(X_test_final)} samples ({len(X_test_final)/len(X)*100:.0f}%)\")\n",
    "print(f\"\\nForgery rate in Train+Val: {y_trainval.mean()*100:.1f}%\")\n",
    "print(f\"Forgery rate in Test: {y_test_final.mean()*100:.1f}%\")\n",
    "print(\"\\nâš ï¸ The final test set will NOT be touched until the very end!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare multiple models using cross-validation on train+val set\n",
    "models_to_compare = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Ridge (Î±=0.1)': Ridge(alpha=0.1),\n",
    "    'Ridge (Î±=1.0)': Ridge(alpha=1.0),\n",
    "    'Lasso (Î±=0.01)': Lasso(alpha=0.01, max_iter=10000),\n",
    "    'Lasso (Î±=0.1)': Lasso(alpha=0.1, max_iter=10000),\n",
    "    'Elastic Net': ElasticNet(alpha=0.05, l1_ratio=0.5, max_iter=10000),\n",
    "}\n",
    "\n",
    "cv_comparison = []\n",
    "print(\"Model Comparison (5-Fold CV on Train+Val Set):\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for name, model in models_to_compare.items():\n",
    "    pipeline = make_pipeline(StandardScaler(), model)\n",
    "    scores = cross_val_score(pipeline, X_trainval, y_trainval, \n",
    "                             cv=5, scoring='neg_mean_squared_error')\n",
    "    cv_comparison.append({\n",
    "        'Model': name,\n",
    "        'Mean MSE': -scores.mean(),\n",
    "        'Std MSE': scores.std()\n",
    "    })\n",
    "    print(f\"{name:25}: MSE = {-scores.mean():.4f} (+/- {scores.std():.4f})\")\n",
    "\n",
    "cv_comparison_df = pd.DataFrame(cv_comparison)\n",
    "best_model_name = cv_comparison_df.loc[cv_comparison_df['Mean MSE'].idxmin(), 'Model']\n",
    "print(f\"\\nðŸ† Best Model: {best_model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "x_pos = np.arange(len(cv_comparison_df))\n",
    "\n",
    "bars = plt.bar(x_pos, cv_comparison_df['Mean MSE'], \n",
    "               yerr=cv_comparison_df['Std MSE'], \n",
    "               capsize=5, color='steelblue', alpha=0.8, edgecolor='black')\n",
    "\n",
    "# Highlight best\n",
    "best_idx = cv_comparison_df['Mean MSE'].idxmin()\n",
    "bars[best_idx].set_color('green')\n",
    "\n",
    "plt.xticks(x_pos, cv_comparison_df['Model'], rotation=45, ha='right')\n",
    "plt.ylabel('Mean CV MSE (+/- std)', fontsize=12)\n",
    "plt.title('Model Comparison for Forgery Detection\\n(Green = Best)', fontsize=13)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final model selection and training\n",
    "# Use the best hyperparameters found through CV\n",
    "\n",
    "# Fine-tune the best model type\n",
    "param_grid_final = {\n",
    "    'ridge__alpha': np.logspace(-3, 1, 20)\n",
    "}\n",
    "\n",
    "pipeline_final = make_pipeline(StandardScaler(), Ridge())\n",
    "\n",
    "grid_search_final = GridSearchCV(\n",
    "    pipeline_final, \n",
    "    param_grid_final, \n",
    "    cv=5, \n",
    "    scoring='neg_mean_squared_error'\n",
    ")\n",
    "\n",
    "grid_search_final.fit(X_trainval, y_trainval)\n",
    "\n",
    "print(\"Fine-tuned Ridge Regression:\")\n",
    "print(f\"Best Î±: {grid_search_final.best_params_['ridge__alpha']:.4f}\")\n",
    "print(f\"Best CV MSE: {-grid_search_final.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Final Evaluation on Held-Out Test Set\n",
    "\n",
    "*\"The moment of truth has arrived. We have selected our model without ever looking at the final test set. Now, and only now, do we evaluate on these truly unseen manuscripts.\"*  \n",
    "â€” Mink Pavar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FINAL EVALUATION - This is the moment of truth!\n",
    "# Train on ALL of train+val, evaluate on final test\n",
    "\n",
    "best_model = grid_search_final.best_estimator_\n",
    "\n",
    "# Make predictions on final test set\n",
    "y_pred_final = best_model.predict(X_test_final)\n",
    "\n",
    "# For classification metrics, threshold at 0.5\n",
    "y_pred_class = (y_pred_final > 0.5).astype(int)\n",
    "\n",
    "# Calculate metrics\n",
    "final_mse = mean_squared_error(y_test_final, y_pred_final)\n",
    "final_accuracy = accuracy_score(y_test_final, y_pred_class)\n",
    "\n",
    "print(\"ðŸŽ¯ FINAL TEST SET EVALUATION ðŸŽ¯\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nModel: Ridge Regression (Î± = {grid_search_final.best_params_['ridge__alpha']:.4f})\")\n",
    "print(f\"\\nFinal Test MSE: {final_mse:.4f}\")\n",
    "print(f\"Final Test Accuracy: {final_accuracy*100:.1f}%\")\n",
    "print(f\"\\nCV MSE (estimate): {-grid_search_final.best_score_:.4f}\")\n",
    "print(f\"Test MSE (actual): {final_mse:.4f}\")\n",
    "print(f\"\\nDifference: {abs(final_mse - (-grid_search_final.best_score_)):.4f}\")\n",
    "print(\"\\nâœ… The CV estimate was close to the true test performance!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(\"=\" * 60)\n",
    "print(classification_report(y_test_final, y_pred_class, \n",
    "                           target_names=['Authentic', 'Forgery']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix visualization\n",
    "cm = confusion_matrix(y_test_final, y_pred_class)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Confusion matrix\n",
    "im = axes[0].imshow(cm, cmap='Blues')\n",
    "axes[0].set_xticks([0, 1])\n",
    "axes[0].set_yticks([0, 1])\n",
    "axes[0].set_xticklabels(['Authentic', 'Forgery'])\n",
    "axes[0].set_yticklabels(['Authentic', 'Forgery'])\n",
    "axes[0].set_xlabel('Predicted', fontsize=12)\n",
    "axes[0].set_ylabel('Actual', fontsize=12)\n",
    "axes[0].set_title('Confusion Matrix', fontsize=13)\n",
    "\n",
    "# Add text annotations\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        text = axes[0].text(j, i, cm[i, j], ha='center', va='center', \n",
    "                           fontsize=20, color='white' if cm[i, j] > cm.max()/2 else 'black')\n",
    "\n",
    "# Predictions vs actual\n",
    "axes[1].scatter(y_test_final, y_pred_final, alpha=0.5, c='steelblue', s=60)\n",
    "axes[1].axhline(0.5, color='red', linestyle='--', label='Decision boundary')\n",
    "axes[1].axvline(0.5, color='gray', linestyle=':')\n",
    "axes[1].set_xlabel('Actual (0=Authentic, 1=Forgery)', fontsize=12)\n",
    "axes[1].set_ylabel('Predicted Probability', fontsize=12)\n",
    "axes[1].set_title('Predictions vs Actual', fontsize=13)\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Feature Importance Analysis\n",
    "\n",
    "*\"The Tribunal wished to understand: which features are most important for detecting forgeries? The model's weights reveal this.\"*  \n",
    "â€” Mink Pavar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract feature importance from final model\n",
    "# Note: weights are for scaled features\n",
    "\n",
    "# Get the Ridge model from the pipeline\n",
    "scaler = best_model.named_steps['standardscaler']\n",
    "ridge = best_model.named_steps['ridge']\n",
    "\n",
    "# Weights for scaled features\n",
    "weights = ridge.coef_\n",
    "\n",
    "# Create importance dataframe\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_cols,\n",
    "    'Weight': weights,\n",
    "    'Abs_Weight': np.abs(weights)\n",
    "}).sort_values('Abs_Weight', ascending=False)\n",
    "\n",
    "print(\"Feature Importance for Forgery Detection:\")\n",
    "print(\"=\" * 50)\n",
    "print(importance_df.to_string(index=False))\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 6))\n",
    "colors = ['green' if w > 0 else 'red' for w in importance_df['Weight']]\n",
    "plt.barh(importance_df['Feature'], importance_df['Weight'], color=colors, alpha=0.8, edgecolor='black')\n",
    "plt.axvline(0, color='black', linewidth=0.5)\n",
    "plt.xlabel('Weight (Positive = More Forgery-Like)', fontsize=12)\n",
    "plt.title('Feature Weights in Forgery Detection Model', fontsize=13)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"- Positive weight: Higher values increase forgery probability\")\n",
    "print(\"- Negative weight: Higher values decrease forgery probability\")\n",
    "print(f\"\\nMost important feature: {importance_df.iloc[0]['Feature']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercises\n",
    "\n",
    "### Exercise 1: Different K Values\n",
    "\n",
    "Compare 3-fold, 5-fold, and 10-fold cross-validation. How does K affect the estimate variance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Compare different K values\n",
    "\n",
    "k_values = [3, 5, 10]\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# for k in k_values:\n",
    "#     pipeline = make_pipeline(StandardScaler(), Ridge(alpha=0.1))\n",
    "#     scores = cross_val_score(pipeline, X_trainval, y_trainval, cv=k, scoring='neg_mean_squared_error')\n",
    "#     print(f\"K={k}: Mean MSE = {-scores.mean():.4f} (+/- {scores.std():.4f})\")\n",
    "\n",
    "# Question: What happens to the variance as K increases?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Leave-One-Out Cross-Validation\n",
    "\n",
    "Implement LOOCV where K = n (number of samples). Compare to 5-fold CV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Leave-One-Out CV\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# loo = LeaveOneOut()\n",
    "# pipeline = make_pipeline(StandardScaler(), Ridge(alpha=0.1))\n",
    "# scores = cross_val_score(pipeline, X_trainval, y_trainval, cv=loo, scoring='neg_mean_squared_error')\n",
    "# print(f\"LOOCV: Mean MSE = {-scores.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Stratified K-Fold\n",
    "\n",
    "When classes are imbalanced, use Stratified K-Fold to ensure each fold has similar class proportions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Stratified K-Fold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# Check class balance in each fold with regular vs stratified\n",
    "# skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "# for train_idx, val_idx in skf.split(X_trainval, y_trainval):\n",
    "#     print(f\"Fold forgery rate: {y_trainval[val_idx].mean()*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Nested Cross-Validation\n",
    "\n",
    "For truly unbiased model selection, use nested CV: inner loop for hyperparameter tuning, outer loop for performance estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4: Nested Cross-Validation\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# Outer loop: 5-fold CV for performance estimation\n",
    "# Inner loop: GridSearchCV for hyperparameter tuning\n",
    "#\n",
    "# outer_cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "# inner_cv = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "#\n",
    "# nested_scores = []\n",
    "# for train_idx, test_idx in outer_cv.split(X_trainval):\n",
    "#     X_train_fold = X_trainval[train_idx]\n",
    "#     y_train_fold = y_trainval[train_idx]\n",
    "#     X_test_fold = X_trainval[test_idx]\n",
    "#     y_test_fold = y_trainval[test_idx]\n",
    "#     \n",
    "#     # Inner CV for hyperparameter tuning\n",
    "#     grid = GridSearchCV(pipeline, param_grid, cv=inner_cv)\n",
    "#     grid.fit(X_train_fold, y_train_fold)\n",
    "#     \n",
    "#     # Evaluate on outer test fold\n",
    "#     score = mean_squared_error(y_test_fold, grid.predict(X_test_fold))\n",
    "#     nested_scores.append(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: Build a Complete Pipeline for Expedition Success\n",
    "\n",
    "Apply everything you've learned to predict expedition success from the expedition_outcomes data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 5: Complete pipeline for expedition success prediction\n",
    "\n",
    "# Features to use\n",
    "exp_features = ['days_in_field', 'crew_size', 'leader_experience_years', \n",
    "                'creature_encounters', 'equipment_condition', 'morale_at_return']\n",
    "\n",
    "# Target: success (binary)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# 1. Prepare data\n",
    "# 2. Split into train+val and final test\n",
    "# 3. Compare models with CV\n",
    "# 4. Tune hyperparameters\n",
    "# 5. Final evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary: The Complete ML Workflow\n",
    "\n",
    "| Step | Purpose | Key Tool |\n",
    "|------|---------|----------|\n",
    "| 1. Data Split | Reserve final test set | `train_test_split` |\n",
    "| 2. Preprocessing | Scale features | `StandardScaler` |\n",
    "| 3. Model Comparison | Find best model type | `cross_val_score` |\n",
    "| 4. Hyperparameter Tuning | Optimize parameters | `GridSearchCV` |\n",
    "| 5. Final Evaluation | Unbiased performance | Held-out test set |\n",
    "\n",
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **A single train/test split is unreliable** â€” results vary with random chance\n",
    "\n",
    "2. **Cross-validation gives robust estimates** â€” average performance across K experiments\n",
    "\n",
    "3. **Use CV for model selection AND hyperparameter tuning** â€” GridSearchCV combines both\n",
    "\n",
    "4. **Hold out a final test set** â€” never touch it until the very end\n",
    "\n",
    "5. **The CV estimate should match final test performance** â€” if not, something is wrong\n",
    "\n",
    "6. **Feature importance reveals what matters** â€” interpret your model's weights\n",
    "\n",
    "---\n",
    "\n",
    "## The Verdict\n",
    "\n",
    "*\"The Tribunal deliberated. They had seen the evidence: linear regression derived from first principles, the bias-variance trade-off explained, regularization demonstrated, and cross-validation proving the model's reliability. On the final day, they rendered their verdict:*\n",
    "\n",
    "*'Mink Pavar's mathematical methods have merit. The system he proposesâ€”trained on known manuscripts, validated through cross-validation, and tested on held-out samplesâ€”provides a principled way to identify forgeries. We accept this approach as evidence in the Archives.'*\n",
    "\n",
    "*The Great Forgery Trial of 912 was concluded. The forger was revealed to be a junior archivist who had studied under Grigsu Haldo himselfâ€”close enough to imitate the master's style, but not perfectly. The stylometric variance and era markers had betrayed him.*\n",
    "\n",
    "*Mink Pavar returned to his studies, but his methods lived on. To this day, the Capital Archives uses mathematical classification to authenticate manuscripts. And scholars speak of the Forgery Trial as the moment when numbers became evidence.\"*\n",
    "\n",
    "---\n",
    "\n",
    "## Congratulations!\n",
    "\n",
    "You have completed **Module 4: Applied ML** and the entire **ML Math with Densworld** course.\n",
    "\n",
    "You now understand:\n",
    "- **Statistics & Probability** â€” uncertainty, distributions, hypothesis testing\n",
    "- **Linear Algebra** â€” vectors, matrices, transformations\n",
    "- **Calculus** â€” derivatives, gradients, optimization\n",
    "- **Applied ML** â€” regression, regularization, model selection\n",
    "\n",
    "These foundations will serve you well as you explore more advanced topics: neural networks, deep learning, and beyond.\n",
    "\n",
    "*\"The mathematics of learning is now yours. Use it wisely.\"*  \n",
    "â€” Mink Pavar"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
