{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/buildLittleWorlds/ml-math-with-densworld/blob/main/modules/04-applied-ml/notebooks/02-bias-variance-tradeoff.ipynb)\n",
    "\n",
    "# Lesson 2: The Bias-Variance Trade-off\n",
    "\n",
    "*\"The Tribunal grew impatient. 'Your simple model makes errors,' they said. 'Give us a more complex one—one that makes no errors on the manuscripts we've shown you.' I warned them: a model that perfectly fits the past may fail spectacularly on the future. This is the deepest truth of prediction.\"*  \n",
    "— Mink Pavar, second day of testimony\n",
    "\n",
    "---\n",
    "\n",
    "## The Most Important Concept in Machine Learning\n",
    "\n",
    "The Forgery Trial took an unexpected turn on the second day. A rival scholar, Eulr Voss, challenged Mink Pavar's method:\n",
    "\n",
    "> *\"Your linear model makes errors! I can construct a model so complex that it perfectly classifies every manuscript you've shown us. Zero errors. Is that not better?\"*\n",
    "\n",
    "The Tribunal murmured in approval. Zero errors sounded impressive.\n",
    "\n",
    "But Mink Pavar shook his head.\n",
    "\n",
    "> *\"Bring me a new manuscript—one your model has never seen. I wager my model will outperform yours. For yours has memorized the training data, while mine has learned the underlying pattern.\"*\n",
    "\n",
    "This challenge revealed the central tension in all of machine learning: **the bias-variance trade-off**.\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this lesson, you will:\n",
    "1. Understand bias (underfitting) and variance (overfitting) intuitively\n",
    "2. Visualize how model complexity affects both\n",
    "3. See why training error alone is misleading\n",
    "4. Learn to diagnose which problem your model has\n",
    "5. Understand the mathematical decomposition of error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Nice plotting defaults\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "# Colab-ready data loading\n",
    "BASE_URL = \"https://raw.githubusercontent.com/buildLittleWorlds/ml-math-with-densworld/main/data/\"\n",
    "\n",
    "# Load datasets\n",
    "manuscripts = pd.read_csv(BASE_URL + \"manuscript_features.csv\")\n",
    "expeditions = pd.read_csv(BASE_URL + \"expedition_outcomes.csv\")\n",
    "\n",
    "print(f\"Loaded {len(manuscripts)} manuscripts\")\n",
    "print(f\"Loaded {len(expeditions)} expeditions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: The Fundamental Problem\n",
    "\n",
    "*\"We have limited data—a sample of manuscripts. From this sample, we must learn a rule that applies to all manuscripts, including those we haven't yet seen. This is the challenge: learning from the particular to predict the general.\"*  \n",
    "— Mink Pavar\n",
    "\n",
    "### The True Function vs. Our Approximation\n",
    "\n",
    "In the real world, there's some true relationship between features and outcomes. We never see this true function—we only see noisy samples from it.\n",
    "\n",
    "Let's create a synthetic example to illustrate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The TRUE function (unknown in practice)\n",
    "def true_function(x):\n",
    "    \"\"\"The true relationship between manuscript age and authenticity score.\"\"\"\n",
    "    return 0.5 + 0.3 * np.sin(2 * x) + 0.1 * x\n",
    "\n",
    "# Generate \"observed\" data with noise\n",
    "np.random.seed(42)\n",
    "n_samples = 30\n",
    "X_train = np.sort(np.random.uniform(0, 4, n_samples))\n",
    "noise = np.random.normal(0, 0.15, n_samples)\n",
    "y_train = true_function(X_train) + noise\n",
    "\n",
    "# Generate test data (new manuscripts)\n",
    "X_test = np.sort(np.random.uniform(0, 4, 20))\n",
    "y_test = true_function(X_test) + np.random.normal(0, 0.15, 20)\n",
    "\n",
    "# Plot\n",
    "x_smooth = np.linspace(0, 4, 200)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x_smooth, true_function(x_smooth), 'g-', linewidth=2, label='True function (unknown)')\n",
    "plt.scatter(X_train, y_train, c='blue', s=60, edgecolor='black', label='Training data', zorder=5)\n",
    "plt.scatter(X_test, y_test, c='orange', s=60, edgecolor='black', marker='s', label='Test data (unseen)', zorder=5)\n",
    "plt.xlabel('Manuscript Feature (e.g., normalized age)', fontsize=12)\n",
    "plt.ylabel('Authenticity Score', fontsize=12)\n",
    "plt.title('The Fundamental Problem: Learning the True Function from Noisy Data', fontsize=13)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"We see the blue points (training data).\")\n",
    "print(\"We must predict the orange points (test data).\")\n",
    "print(\"The green line is the truth—but we never actually see it!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Underfitting (High Bias)\n",
    "\n",
    "*\"Eulr Voss mocked my simple linear model. 'A straight line?' he laughed. 'The world is not so simple!' He was partly right—my model was too simple. It had HIGH BIAS: it assumed a relationship that was simpler than reality.\"*  \n",
    "— Mink Pavar\n",
    "\n",
    "### What is Bias?\n",
    "\n",
    "**Bias** is the error from overly simplistic assumptions. A high-bias model:\n",
    "- Is too simple to capture the true pattern\n",
    "- Makes systematic errors\n",
    "- Underfits both training AND test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a simple linear model (degree 1 polynomial)\n",
    "def fit_polynomial(X, y, degree):\n",
    "    \"\"\"Fit a polynomial of given degree.\"\"\"\n",
    "    poly = PolynomialFeatures(degree=degree, include_bias=True)\n",
    "    X_poly = poly.fit_transform(X.reshape(-1, 1))\n",
    "    model = LinearRegression(fit_intercept=False)\n",
    "    model.fit(X_poly, y)\n",
    "    return model, poly\n",
    "\n",
    "def predict_polynomial(model, poly, X):\n",
    "    \"\"\"Predict using fitted polynomial.\"\"\"\n",
    "    X_poly = poly.transform(X.reshape(-1, 1))\n",
    "    return model.predict(X_poly)\n",
    "\n",
    "# Fit linear model (degree 1) - HIGH BIAS\n",
    "model_linear, poly_linear = fit_polynomial(X_train, y_train, degree=1)\n",
    "y_train_pred_linear = predict_polynomial(model_linear, poly_linear, X_train)\n",
    "y_test_pred_linear = predict_polynomial(model_linear, poly_linear, X_test)\n",
    "\n",
    "# Calculate errors\n",
    "train_mse_linear = mean_squared_error(y_train, y_train_pred_linear)\n",
    "test_mse_linear = mean_squared_error(y_test, y_test_pred_linear)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x_smooth, true_function(x_smooth), 'g-', linewidth=2, alpha=0.5, label='True function')\n",
    "plt.scatter(X_train, y_train, c='blue', s=60, edgecolor='black', label='Training data', zorder=5)\n",
    "y_smooth_linear = predict_polynomial(model_linear, poly_linear, x_smooth)\n",
    "plt.plot(x_smooth, y_smooth_linear, 'r-', linewidth=2, label=f'Linear model (degree 1)')\n",
    "\n",
    "plt.xlabel('Manuscript Feature', fontsize=12)\n",
    "plt.ylabel('Authenticity Score', fontsize=12)\n",
    "plt.title('HIGH BIAS (Underfitting): Model Too Simple', fontsize=13)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"Underfitting (High Bias):\")\n",
    "print(f\"  Training MSE: {train_mse_linear:.4f}\")\n",
    "print(f\"  Test MSE:     {test_mse_linear:.4f}\")\n",
    "print(\"\\nBoth errors are HIGH because the model is too simple.\")\n",
    "print(\"The straight line cannot capture the curved true function.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Overfitting (High Variance)\n",
    "\n",
    "*\"Eulr Voss presented his model—a polynomial of degree 25. It passed through every training point perfectly. 'Zero training error!' he proclaimed. But I asked him: 'What happens when we test it on new manuscripts?'\"*  \n",
    "— Mink Pavar\n",
    "\n",
    "### What is Variance?\n",
    "\n",
    "**Variance** is the error from sensitivity to small fluctuations in the training data. A high-variance model:\n",
    "- Is too complex\n",
    "- Memorizes the noise in the training data\n",
    "- Fits training data perfectly but fails on new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a high-degree polynomial (degree 15) - HIGH VARIANCE\n",
    "model_complex, poly_complex = fit_polynomial(X_train, y_train, degree=15)\n",
    "y_train_pred_complex = predict_polynomial(model_complex, poly_complex, X_train)\n",
    "y_test_pred_complex = predict_polynomial(model_complex, poly_complex, X_test)\n",
    "\n",
    "# Calculate errors\n",
    "train_mse_complex = mean_squared_error(y_train, y_train_pred_complex)\n",
    "test_mse_complex = mean_squared_error(y_test, y_test_pred_complex)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x_smooth, true_function(x_smooth), 'g-', linewidth=2, alpha=0.5, label='True function')\n",
    "plt.scatter(X_train, y_train, c='blue', s=60, edgecolor='black', label='Training data', zorder=5)\n",
    "\n",
    "# Predict on smooth x values (clip extreme predictions for visualization)\n",
    "y_smooth_complex = predict_polynomial(model_complex, poly_complex, x_smooth)\n",
    "y_smooth_complex = np.clip(y_smooth_complex, -1, 3)  # Clip for visualization\n",
    "plt.plot(x_smooth, y_smooth_complex, 'r-', linewidth=2, label=f'Complex model (degree 15)')\n",
    "\n",
    "plt.xlabel('Manuscript Feature', fontsize=12)\n",
    "plt.ylabel('Authenticity Score', fontsize=12)\n",
    "plt.title('HIGH VARIANCE (Overfitting): Model Too Complex', fontsize=13)\n",
    "plt.ylim(-0.5, 2)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"Overfitting (High Variance):\")\n",
    "print(f\"  Training MSE: {train_mse_complex:.4f} (very low!)\")\n",
    "print(f\"  Test MSE:     {test_mse_complex:.4f} (much higher!)\")\n",
    "print(\"\\nThe model memorized the training data (low training error)\")\n",
    "print(\"but fails to generalize (high test error).\")\n",
    "print(\"\\nThe wiggly curve fits the noise, not the signal!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: The Sweet Spot (Balanced Complexity)\n",
    "\n",
    "*\"The Tribunal asked: 'What is the right complexity?' I told them: enough to capture the pattern, but not so much that we fit the noise. We seek the sweet spot.\"*  \n",
    "— Mink Pavar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit models of various complexities\n",
    "degrees = [1, 2, 3, 4, 5, 7, 10, 15]\n",
    "train_errors = []\n",
    "test_errors = []\n",
    "\n",
    "for degree in degrees:\n",
    "    model, poly = fit_polynomial(X_train, y_train, degree)\n",
    "    y_train_pred = predict_polynomial(model, poly, X_train)\n",
    "    y_test_pred = predict_polynomial(model, poly, X_test)\n",
    "    train_errors.append(mean_squared_error(y_train, y_train_pred))\n",
    "    test_errors.append(mean_squared_error(y_test, y_test_pred))\n",
    "\n",
    "# Find best degree\n",
    "best_idx = np.argmin(test_errors)\n",
    "best_degree = degrees[best_idx]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(degrees, train_errors, 'b-o', linewidth=2, markersize=8, label='Training Error')\n",
    "plt.plot(degrees, test_errors, 'r-s', linewidth=2, markersize=8, label='Test Error')\n",
    "plt.axvline(best_degree, color='green', linestyle='--', linewidth=2, label=f'Best degree = {best_degree}')\n",
    "\n",
    "# Annotate regions\n",
    "plt.annotate('UNDERFITTING\\n(High Bias)', xy=(1.5, 0.04), fontsize=11, ha='center',\n",
    "             bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.5))\n",
    "plt.annotate('OVERFITTING\\n(High Variance)', xy=(12, 0.1), fontsize=11, ha='center',\n",
    "             bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.5))\n",
    "plt.annotate('SWEET SPOT', xy=(best_degree, test_errors[best_idx] - 0.015), fontsize=11, ha='center',\n",
    "             bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.7))\n",
    "\n",
    "plt.xlabel('Model Complexity (Polynomial Degree)', fontsize=12)\n",
    "plt.ylabel('Mean Squared Error', fontsize=12)\n",
    "plt.title('The Bias-Variance Trade-off: Finding the Sweet Spot', fontsize=13)\n",
    "plt.legend()\n",
    "plt.ylim(0, 0.2)\n",
    "plt.show()\n",
    "\n",
    "print(\"Key Observations:\")\n",
    "print(\"1. Training error always decreases with complexity\")\n",
    "print(\"2. Test error decreases, then INCREASES (the U-curve)\")\n",
    "print(f\"3. Best model: degree {best_degree} (lowest test error)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the sweet spot model\n",
    "model_best, poly_best = fit_polynomial(X_train, y_train, degree=best_degree)\n",
    "y_smooth_best = predict_polynomial(model_best, poly_best, x_smooth)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Underfitting\n",
    "axes[0].plot(x_smooth, true_function(x_smooth), 'g-', linewidth=2, alpha=0.5, label='True')\n",
    "axes[0].scatter(X_train, y_train, c='blue', s=40, edgecolor='black')\n",
    "axes[0].plot(x_smooth, y_smooth_linear, 'r-', linewidth=2)\n",
    "axes[0].set_title(f'Underfitting (degree 1)\\nTest MSE: {test_mse_linear:.4f}', fontsize=11)\n",
    "axes[0].set_ylim(-0.2, 1.8)\n",
    "\n",
    "# Sweet Spot\n",
    "axes[1].plot(x_smooth, true_function(x_smooth), 'g-', linewidth=2, alpha=0.5, label='True')\n",
    "axes[1].scatter(X_train, y_train, c='blue', s=40, edgecolor='black')\n",
    "axes[1].plot(x_smooth, y_smooth_best, 'r-', linewidth=2)\n",
    "axes[1].set_title(f'Sweet Spot (degree {best_degree})\\nTest MSE: {test_errors[best_idx]:.4f}', fontsize=11)\n",
    "axes[1].set_ylim(-0.2, 1.8)\n",
    "\n",
    "# Overfitting\n",
    "axes[2].plot(x_smooth, true_function(x_smooth), 'g-', linewidth=2, alpha=0.5, label='True')\n",
    "axes[2].scatter(X_train, y_train, c='blue', s=40, edgecolor='black')\n",
    "y_smooth_complex_clipped = np.clip(y_smooth_complex, -0.5, 2)\n",
    "axes[2].plot(x_smooth, y_smooth_complex_clipped, 'r-', linewidth=2)\n",
    "axes[2].set_title(f'Overfitting (degree 15)\\nTest MSE: {test_mse_complex:.4f}', fontsize=11)\n",
    "axes[2].set_ylim(-0.2, 1.8)\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set_xlabel('Feature')\n",
    "    ax.set_ylabel('Score')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: The Mathematical Decomposition\n",
    "\n",
    "*\"The Tribunal demanded proof. I showed them that any prediction error can be decomposed into three parts: bias, variance, and irreducible noise. Only by understanding this decomposition can we make wise choices.\"*  \n",
    "— Mink Pavar\n",
    "\n",
    "### The Bias-Variance Decomposition\n",
    "\n",
    "For any prediction, the expected error can be written as:\n",
    "\n",
    "$$\\text{Expected Error} = \\text{Bias}^2 + \\text{Variance} + \\text{Irreducible Noise}$$\n",
    "\n",
    "Where:\n",
    "- **Bias²**: Error from wrong assumptions (model too simple)\n",
    "- **Variance**: Error from sensitivity to training data (model too complex)\n",
    "- **Irreducible Noise**: Random error we can never eliminate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate the bias-variance decomposition\n",
    "# by training many models on different samples\n",
    "\n",
    "def simulate_bias_variance(degree, n_simulations=100):\n",
    "    \"\"\"Estimate bias and variance by training on many different samples.\"\"\"\n",
    "    predictions = []\n",
    "    \n",
    "    # Fixed test point\n",
    "    x_test_point = np.array([2.0])\n",
    "    true_y = true_function(x_test_point)[0]\n",
    "    \n",
    "    for _ in range(n_simulations):\n",
    "        # Generate new training data\n",
    "        X_sim = np.sort(np.random.uniform(0, 4, 30))\n",
    "        y_sim = true_function(X_sim) + np.random.normal(0, 0.15, 30)\n",
    "        \n",
    "        # Fit model\n",
    "        model, poly = fit_polynomial(X_sim, y_sim, degree)\n",
    "        pred = predict_polynomial(model, poly, x_test_point)[0]\n",
    "        predictions.append(pred)\n",
    "    \n",
    "    predictions = np.array(predictions)\n",
    "    \n",
    "    # Calculate components\n",
    "    mean_pred = np.mean(predictions)\n",
    "    bias = mean_pred - true_y\n",
    "    variance = np.var(predictions)\n",
    "    \n",
    "    return bias**2, variance, predictions\n",
    "\n",
    "# Calculate for different complexities\n",
    "degrees_to_test = [1, 2, 3, 5, 7, 10, 15]\n",
    "biases_sq = []\n",
    "variances = []\n",
    "\n",
    "print(\"Simulating bias-variance for different model complexities...\")\n",
    "for deg in degrees_to_test:\n",
    "    b2, v, _ = simulate_bias_variance(deg)\n",
    "    biases_sq.append(b2)\n",
    "    variances.append(v)\n",
    "    print(f\"  Degree {deg:2d}: Bias² = {b2:.4f}, Variance = {v:.4f}, Total = {b2+v:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the decomposition\n",
    "irreducible_noise = 0.15**2  # Known noise level\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(degrees_to_test, biases_sq, 'b-o', linewidth=2, markersize=8, label='Bias²')\n",
    "plt.plot(degrees_to_test, variances, 'r-s', linewidth=2, markersize=8, label='Variance')\n",
    "total_error = [b + v for b, v in zip(biases_sq, variances)]\n",
    "plt.plot(degrees_to_test, total_error, 'g-^', linewidth=2, markersize=8, label='Bias² + Variance')\n",
    "plt.axhline(irreducible_noise, color='gray', linestyle='--', label=f'Irreducible Noise ({irreducible_noise:.4f})')\n",
    "\n",
    "plt.xlabel('Model Complexity (Polynomial Degree)', fontsize=12)\n",
    "plt.ylabel('Error Component', fontsize=12)\n",
    "plt.title('The Bias-Variance Decomposition', fontsize=13)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key Insight:\")\n",
    "print(\"- Bias DECREASES with complexity (model becomes more flexible)\")\n",
    "print(\"- Variance INCREASES with complexity (model becomes more sensitive)\")\n",
    "print(\"- Total error is minimized at intermediate complexity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Diagnosing Your Model\n",
    "\n",
    "*\"The Tribunal asked: 'How do we know if our model has bias or variance problems?' I gave them this diagnostic table.\"*  \n",
    "— Mink Pavar\n",
    "\n",
    "### Diagnostic Table\n",
    "\n",
    "| Training Error | Test Error | Diagnosis | Solution |\n",
    "|----------------|------------|-----------|----------|\n",
    "| High | High | **Underfitting** (High Bias) | Increase complexity |\n",
    "| Low | High | **Overfitting** (High Variance) | Decrease complexity, add regularization |\n",
    "| Low | Low | **Good fit** | You're done! |\n",
    "| High | Low | **Impossible** | Check for data leakage |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive diagnosis tool\n",
    "def diagnose_model(train_error, test_error, threshold=0.03):\n",
    "    \"\"\"Diagnose a model based on training and test error.\"\"\"\n",
    "    gap = test_error - train_error\n",
    "    \n",
    "    print(f\"Training Error: {train_error:.4f}\")\n",
    "    print(f\"Test Error:     {test_error:.4f}\")\n",
    "    print(f\"Gap:            {gap:.4f}\")\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    \n",
    "    if train_error > threshold and test_error > threshold:\n",
    "        print(\"DIAGNOSIS: UNDERFITTING (High Bias)\")\n",
    "        print(\"\\nBoth training and test errors are high.\")\n",
    "        print(\"The model is too simple to capture the pattern.\")\n",
    "        print(\"\\nSolutions:\")\n",
    "        print(\"  - Increase model complexity\")\n",
    "        print(\"  - Add more features\")\n",
    "        print(\"  - Use a more flexible model family\")\n",
    "        \n",
    "    elif train_error < threshold and test_error > threshold:\n",
    "        print(\"DIAGNOSIS: OVERFITTING (High Variance)\")\n",
    "        print(\"\\nTraining error is low but test error is high.\")\n",
    "        print(\"The model memorized training data but doesn't generalize.\")\n",
    "        print(\"\\nSolutions:\")\n",
    "        print(\"  - Reduce model complexity\")\n",
    "        print(\"  - Add regularization (next lesson!)\")\n",
    "        print(\"  - Get more training data\")\n",
    "        print(\"  - Use dropout (for neural networks)\")\n",
    "        \n",
    "    elif train_error < threshold and test_error < threshold:\n",
    "        print(\"DIAGNOSIS: GOOD FIT\")\n",
    "        print(\"\\nBoth errors are low and similar.\")\n",
    "        print(\"The model has learned the true pattern!\")\n",
    "        \n",
    "    else:\n",
    "        print(\"DIAGNOSIS: CHECK YOUR DATA\")\n",
    "        print(\"\\nTest error lower than training error is suspicious.\")\n",
    "        print(\"Check for data leakage or evaluation errors.\")\n",
    "\n",
    "print(\"Example 1: Linear Model (Underfitting)\")\n",
    "print(\"-\" * 50)\n",
    "diagnose_model(train_mse_linear, test_mse_linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nExample 2: Complex Model (Overfitting)\")\n",
    "print(\"-\" * 50)\n",
    "diagnose_model(train_mse_complex, test_mse_complex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nExample 3: Balanced Model (Good Fit)\")\n",
    "print(\"-\" * 50)\n",
    "train_mse_best = mean_squared_error(y_train, predict_polynomial(model_best, poly_best, X_train))\n",
    "test_mse_best = mean_squared_error(y_test, predict_polynomial(model_best, poly_best, X_test))\n",
    "diagnose_model(train_mse_best, test_mse_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Applying to Manuscript Forgery Detection\n",
    "\n",
    "*\"Let us return to the forgery problem. How complex should our model be?\"*  \n",
    "— Mink Pavar\n",
    "\n",
    "Let's apply bias-variance thinking to the real manuscript data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare manuscript data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Use stylometric_variance and era_marker_score to predict forgery\n",
    "X_ms = manuscripts[['stylometric_variance', 'era_marker_score']].values\n",
    "y_ms = manuscripts['is_forgery'].astype(int).values\n",
    "\n",
    "# Split into train/test\n",
    "X_train_ms, X_test_ms, y_train_ms, y_test_ms = train_test_split(\n",
    "    X_ms, y_ms, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(X_train_ms)}\")\n",
    "print(f\"Test samples: {len(X_test_ms)}\")\n",
    "print(f\"Forgery rate in training: {y_train_ms.mean()*100:.1f}%\")\n",
    "print(f\"Forgery rate in test: {y_test_ms.mean()*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different polynomial complexities\n",
    "degrees_ms = [1, 2, 3, 4, 5, 6]\n",
    "train_errors_ms = []\n",
    "test_errors_ms = []\n",
    "\n",
    "for degree in degrees_ms:\n",
    "    poly = PolynomialFeatures(degree=degree, include_bias=True)\n",
    "    X_train_poly = poly.fit_transform(X_train_ms)\n",
    "    X_test_poly = poly.transform(X_test_ms)\n",
    "    \n",
    "    model = LinearRegression(fit_intercept=False)\n",
    "    model.fit(X_train_poly, y_train_ms)\n",
    "    \n",
    "    y_train_pred = model.predict(X_train_poly)\n",
    "    y_test_pred = model.predict(X_test_poly)\n",
    "    \n",
    "    train_errors_ms.append(mean_squared_error(y_train_ms, y_train_pred))\n",
    "    test_errors_ms.append(mean_squared_error(y_test_ms, y_test_pred))\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(degrees_ms, train_errors_ms, 'b-o', linewidth=2, markersize=8, label='Training Error')\n",
    "plt.plot(degrees_ms, test_errors_ms, 'r-s', linewidth=2, markersize=8, label='Test Error')\n",
    "\n",
    "best_degree_ms = degrees_ms[np.argmin(test_errors_ms)]\n",
    "plt.axvline(best_degree_ms, color='green', linestyle='--', linewidth=2, \n",
    "            label=f'Best degree = {best_degree_ms}')\n",
    "\n",
    "plt.xlabel('Model Complexity (Polynomial Degree)', fontsize=12)\n",
    "plt.ylabel('Mean Squared Error', fontsize=12)\n",
    "plt.title('Bias-Variance Trade-off for Forgery Detection', fontsize=13)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Best polynomial degree for forgery detection: {best_degree_ms}\")\n",
    "print(f\"Test MSE at best degree: {test_errors_ms[best_degree_ms-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercises\n",
    "\n",
    "### Exercise 1: Diagnose These Models\n",
    "\n",
    "For each scenario, diagnose whether the model suffers from bias or variance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Diagnose these models\n",
    "scenarios = [\n",
    "    (\"Model A\", 0.15, 0.16),  # Training MSE, Test MSE\n",
    "    (\"Model B\", 0.02, 0.18),\n",
    "    (\"Model C\", 0.05, 0.06),\n",
    "    (\"Model D\", 0.25, 0.24),\n",
    "]\n",
    "\n",
    "print(\"Diagnose each model:\")\n",
    "print(\"=\"*60)\n",
    "for name, train_err, test_err in scenarios:\n",
    "    print(f\"\\n{name}: Train={train_err:.2f}, Test={test_err:.2f}\")\n",
    "    # YOUR DIAGNOSIS HERE\n",
    "    # Is this underfitting, overfitting, or good fit?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Learning Curves\n",
    "\n",
    "Create learning curves showing how training and test error change with the amount of training data. This is another way to diagnose bias vs variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Learning curves\n",
    "# How do errors change as we add more training data?\n",
    "\n",
    "train_sizes = [5, 10, 15, 20, 25, 30]\n",
    "# For each size, train a model and record train/test error\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# degree = 3  # Choose a complexity\n",
    "# train_errors_lc = []\n",
    "# test_errors_lc = []\n",
    "#\n",
    "# for size in train_sizes:\n",
    "#     # Use first 'size' training points\n",
    "#     # Fit model\n",
    "#     # Record errors\n",
    "#     pass\n",
    "#\n",
    "# plt.plot(train_sizes, train_errors_lc, 'b-o', label='Training')\n",
    "# plt.plot(train_sizes, test_errors_lc, 'r-s', label='Test')\n",
    "# plt.xlabel('Training Set Size')\n",
    "# plt.ylabel('MSE')\n",
    "# plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: The Effect of Noise\n",
    "\n",
    "How does the noise level affect the optimal model complexity? Test with noise levels 0.05, 0.15, and 0.30."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Effect of noise on optimal complexity\n",
    "\n",
    "noise_levels = [0.05, 0.15, 0.30]\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# For each noise level:\n",
    "#   - Generate new training/test data\n",
    "#   - Test different polynomial degrees\n",
    "#   - Find the optimal degree\n",
    "#   - Print results\n",
    "\n",
    "# What pattern do you observe?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Manuscript Complexity Analysis\n",
    "\n",
    "Add more features to the forgery model (vocabulary_richness, avg_sentence_length). Does higher feature count lead to overfitting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4: More features for forgery detection\n",
    "\n",
    "feature_sets = [\n",
    "    ['stylometric_variance'],\n",
    "    ['stylometric_variance', 'era_marker_score'],\n",
    "    ['stylometric_variance', 'era_marker_score', 'vocabulary_richness'],\n",
    "    ['stylometric_variance', 'era_marker_score', 'vocabulary_richness', 'avg_sentence_length'],\n",
    "]\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# For each feature set:\n",
    "#   - Split data\n",
    "#   - Train linear regression\n",
    "#   - Record train/test error\n",
    "#   - Does adding features help or hurt?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: Eulr Voss's Challenge\n",
    "\n",
    "Eulr Voss claims his degree-20 polynomial achieves \"perfect training accuracy\" on the manuscript data. Show him why this is misleading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 5: Debunk Eulr Voss's perfect model\n",
    "\n",
    "# Fit a degree-20 polynomial on manuscript data\n",
    "# Show the training vs test error\n",
    "# Explain why \"perfect training accuracy\" is not the goal\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "| Concept | Definition | Symptom |\n",
    "|---------|------------|----------|\n",
    "| **Bias** | Error from simplistic assumptions | High train error, high test error |\n",
    "| **Variance** | Error from sensitivity to training data | Low train error, high test error |\n",
    "| **Underfitting** | Model too simple | Cannot capture the pattern |\n",
    "| **Overfitting** | Model too complex | Memorizes noise instead of signal |\n",
    "| **Sweet Spot** | Balanced complexity | Minimizes total error |\n",
    "\n",
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Training error alone is misleading** — a model with zero training error might be terrible on new data\n",
    "\n",
    "2. **The bias-variance trade-off is fundamental** — reducing one often increases the other\n",
    "\n",
    "3. **Model complexity must be chosen carefully** — not too simple (underfitting), not too complex (overfitting)\n",
    "\n",
    "4. **Always evaluate on held-out test data** — this is how we detect overfitting\n",
    "\n",
    "5. **The goal is generalization** — we want to predict new data, not memorize old data\n",
    "\n",
    "---\n",
    "\n",
    "## Next Lesson\n",
    "\n",
    "In **Lesson 3: Regularization — Taming Complexity**, we'll learn a powerful technique to prevent overfitting without reducing model complexity.\n",
    "\n",
    "*\"The Tribunal asked: 'Is there a way to have a complex model that doesn't overfit?' I told them yes—but we must add a penalty for complexity. This is regularization: we tell the model that large weights are expensive. The model must justify its complexity with better predictions.\"*  \n",
    "— Mink Pavar"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
