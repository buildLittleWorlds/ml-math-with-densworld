{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/buildLittleWorlds/ml-math-with-densworld/blob/main/modules/04-applied-ml/notebooks/03-regularization.ipynb)\n",
    "\n",
    "# Lesson 3: Regularization — Taming Complexity\n",
    "\n",
    "*\"Eulr Voss demanded a complex model. The Tribunal wanted protection against overfitting. I offered them a compromise: keep the complexity, but penalize it. Make the model pay a price for every parameter it uses. This is regularization—the art of controlled complexity.\"*  \n",
    "— Mink Pavar, third day of testimony\n",
    "\n",
    "---\n",
    "\n",
    "## The Controlled Complexity\n",
    "\n",
    "The Forgery Trial had reached an impasse. Mink Pavar's simple model was too biased. Eulr Voss's complex model was too variable. The Tribunal demanded a solution that preserved flexibility while preventing memorization.\n",
    "\n",
    "Mink Pavar proposed a radical idea:\n",
    "\n",
    "> *\"We can keep a complex model—with many parameters—but we must tax its complexity. For every weight that grows large, we add a penalty to our loss function. The model must earn its complexity by making better predictions.\"*\n",
    "\n",
    "This is **regularization**: adding a penalty term to the loss function that discourages overly complex solutions.\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this lesson, you will:\n",
    "1. Understand why regularization works (shrinking weights)\n",
    "2. Implement L2 (Ridge) regularization and interpret its effects\n",
    "3. Implement L1 (Lasso) regularization and understand feature selection\n",
    "4. Know when to use Ridge vs Lasso\n",
    "5. Tune the regularization strength hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Nice plotting defaults\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "# Colab-ready data loading\n",
    "BASE_URL = \"https://raw.githubusercontent.com/buildLittleWorlds/ml-math-with-densworld/main/data/\"\n",
    "\n",
    "# Load datasets\n",
    "manuscripts = pd.read_csv(BASE_URL + \"manuscript_features.csv\")\n",
    "expeditions = pd.read_csv(BASE_URL + \"expedition_outcomes.csv\")\n",
    "\n",
    "print(f\"Loaded {len(manuscripts)} manuscripts\")\n",
    "print(f\"Loaded {len(expeditions)} expeditions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: The Problem — Large Weights Cause Overfitting\n",
    "\n",
    "*\"The overfitting model had enormous weights—some positive, some negative, fighting each other to fit every wiggle in the training data. I realized: if we force the weights to stay small, the model cannot overfit.\"*  \n",
    "— Mink Pavar\n",
    "\n",
    "Let's revisit the overfitting problem and examine the weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic data\n",
    "def true_function(x):\n",
    "    return 0.5 + 0.3 * np.sin(2 * x) + 0.1 * x\n",
    "\n",
    "np.random.seed(42)\n",
    "n_samples = 30\n",
    "X_train = np.sort(np.random.uniform(0, 4, n_samples))\n",
    "y_train = true_function(X_train) + np.random.normal(0, 0.15, n_samples)\n",
    "\n",
    "X_test = np.sort(np.random.uniform(0, 4, 20))\n",
    "y_test = true_function(X_test) + np.random.normal(0, 0.15, 20)\n",
    "\n",
    "# Fit a high-degree polynomial WITHOUT regularization\n",
    "degree = 10\n",
    "poly = PolynomialFeatures(degree=degree, include_bias=True)\n",
    "X_train_poly = poly.fit_transform(X_train.reshape(-1, 1))\n",
    "X_test_poly = poly.transform(X_test.reshape(-1, 1))\n",
    "\n",
    "# Standard linear regression (no regularization)\n",
    "model_unregularized = LinearRegression(fit_intercept=False)\n",
    "model_unregularized.fit(X_train_poly, y_train)\n",
    "\n",
    "# Look at the weights\n",
    "print(\"Weights of degree-10 polynomial (NO regularization):\")\n",
    "print(\"=\" * 50)\n",
    "for i, coef in enumerate(model_unregularized.coef_):\n",
    "    print(f\"  x^{i}: {coef:12.2f}\")\n",
    "\n",
    "print(f\"\\nSum of absolute weights: {np.abs(model_unregularized.coef_).sum():.2f}\")\n",
    "print(f\"Max absolute weight: {np.abs(model_unregularized.coef_).max():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the overfitting\n",
    "x_smooth = np.linspace(0, 4, 200)\n",
    "X_smooth_poly = poly.transform(x_smooth.reshape(-1, 1))\n",
    "y_pred_unreg = model_unregularized.predict(X_smooth_poly)\n",
    "y_pred_unreg_clipped = np.clip(y_pred_unreg, -1, 3)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x_smooth, true_function(x_smooth), 'g-', linewidth=2, alpha=0.5, label='True function')\n",
    "plt.scatter(X_train, y_train, c='blue', s=60, edgecolor='black', label='Training data', zorder=5)\n",
    "plt.plot(x_smooth, y_pred_unreg_clipped, 'r-', linewidth=2, label='Unregularized (overfitting)')\n",
    "\n",
    "plt.xlabel('Manuscript Feature', fontsize=12)\n",
    "plt.ylabel('Authenticity Score', fontsize=12)\n",
    "plt.title('Large Weights → Wild Oscillations → Overfitting', fontsize=13)\n",
    "plt.legend()\n",
    "plt.ylim(-0.5, 2)\n",
    "plt.show()\n",
    "\n",
    "train_mse = mean_squared_error(y_train, model_unregularized.predict(X_train_poly))\n",
    "test_mse = mean_squared_error(y_test, model_unregularized.predict(X_test_poly))\n",
    "print(f\"Training MSE: {train_mse:.4f}\")\n",
    "print(f\"Test MSE: {test_mse:.4f}\")\n",
    "print(\"\\nThe large weights cause wild oscillations between data points!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: L2 Regularization (Ridge Regression)\n",
    "\n",
    "*\"I proposed a simple penalty: add the sum of squared weights to the loss function. The model must now minimize BOTH the prediction error AND the size of its weights.\"*  \n",
    "— Mink Pavar\n",
    "\n",
    "### The Ridge Loss Function\n",
    "\n",
    "$$L_{\\text{Ridge}} = \\underbrace{\\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2}_{\\text{MSE}} + \\underbrace{\\lambda \\sum_{j=1}^{p} w_j^2}_{\\text{L2 penalty}}$$\n",
    "\n",
    "Where:\n",
    "- $\\lambda$ (lambda) controls the regularization strength\n",
    "- Higher $\\lambda$ = more penalty = smaller weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features (important for regularization!)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_poly)\n",
    "X_test_scaled = scaler.transform(X_test_poly)\n",
    "X_smooth_scaled = scaler.transform(X_smooth_poly)\n",
    "\n",
    "# Fit Ridge regression with different lambda values\n",
    "lambdas = [0, 0.001, 0.01, 0.1, 1, 10]\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, lam in enumerate(lambdas):\n",
    "    if lam == 0:\n",
    "        # No regularization\n",
    "        model = LinearRegression(fit_intercept=False)\n",
    "    else:\n",
    "        model = Ridge(alpha=lam, fit_intercept=False)\n",
    "    \n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    y_smooth_pred = model.predict(X_smooth_scaled)\n",
    "    y_smooth_pred_clipped = np.clip(y_smooth_pred, -1, 3)\n",
    "    \n",
    "    test_mse = mean_squared_error(y_test, model.predict(X_test_scaled))\n",
    "    weight_sum = np.sum(model.coef_**2)\n",
    "    \n",
    "    ax = axes[idx]\n",
    "    ax.plot(x_smooth, true_function(x_smooth), 'g-', linewidth=2, alpha=0.5)\n",
    "    ax.scatter(X_train, y_train, c='blue', s=30, edgecolor='black')\n",
    "    ax.plot(x_smooth, y_smooth_pred_clipped, 'r-', linewidth=2)\n",
    "    ax.set_ylim(-0.5, 2)\n",
    "    ax.set_title(f'λ = {lam}\\nTest MSE: {test_mse:.4f}\\nΣw²: {weight_sum:.2f}')\n",
    "\n",
    "plt.suptitle('Ridge Regression: Effect of Regularization Strength', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare weights at different regularization strengths\n",
    "print(\"Weight comparison: Unregularized vs Ridge (λ=1)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "model_ridge = Ridge(alpha=1.0, fit_intercept=False)\n",
    "model_ridge.fit(X_train_scaled, y_train)\n",
    "\n",
    "model_unreg = LinearRegression(fit_intercept=False)\n",
    "model_unreg.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"{'Term':>10} | {'Unregularized':>15} | {'Ridge (λ=1)':>15}\")\n",
    "print(\"-\" * 50)\n",
    "for i in range(len(model_unreg.coef_)):\n",
    "    print(f\"{'x^'+str(i):>10} | {model_unreg.coef_[i]:>15.4f} | {model_ridge.coef_[i]:>15.4f}\")\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'Σ|w|':>10} | {np.abs(model_unreg.coef_).sum():>15.4f} | {np.abs(model_ridge.coef_).sum():>15.4f}\")\n",
    "print(f\"{'Σw²':>10} | {np.sum(model_unreg.coef_**2):>15.4f} | {np.sum(model_ridge.coef_**2):>15.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: L1 Regularization (Lasso Regression)\n",
    "\n",
    "*\"The Tribunal asked: 'What if some features are useless for detecting forgeries? Can the model learn to ignore them?' I showed them Lasso—a regularization that forces useless weights to exactly zero.\"*  \n",
    "— Mink Pavar\n",
    "\n",
    "### The Lasso Loss Function\n",
    "\n",
    "$$L_{\\text{Lasso}} = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2 + \\lambda \\sum_{j=1}^{p} |w_j|$$\n",
    "\n",
    "The key difference: L1 uses **absolute values**, not squares. This creates a different geometry that pushes weights to exactly zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Ridge vs Lasso\n",
    "lambda_val = 0.1\n",
    "\n",
    "model_ridge = Ridge(alpha=lambda_val, fit_intercept=False)\n",
    "model_lasso = Lasso(alpha=lambda_val, fit_intercept=False, max_iter=10000)\n",
    "\n",
    "model_ridge.fit(X_train_scaled, y_train)\n",
    "model_lasso.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"Weight comparison: Ridge vs Lasso (λ={lambda_val})\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Term':>10} | {'Ridge':>15} | {'Lasso':>15}\")\n",
    "print(\"-\" * 50)\n",
    "for i in range(len(model_ridge.coef_)):\n",
    "    ridge_w = model_ridge.coef_[i]\n",
    "    lasso_w = model_lasso.coef_[i]\n",
    "    zero_marker = \" ← ZERO\" if abs(lasso_w) < 1e-6 else \"\"\n",
    "    print(f\"{'x^'+str(i):>10} | {ridge_w:>15.4f} | {lasso_w:>15.4f}{zero_marker}\")\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(f\"Non-zero weights: Ridge={np.sum(np.abs(model_ridge.coef_) > 1e-6)}, Lasso={np.sum(np.abs(model_lasso.coef_) > 1e-6)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Ridge vs Lasso\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Ridge\n",
    "y_smooth_ridge = np.clip(model_ridge.predict(X_smooth_scaled), -1, 3)\n",
    "axes[0].plot(x_smooth, true_function(x_smooth), 'g-', linewidth=2, alpha=0.5, label='True')\n",
    "axes[0].scatter(X_train, y_train, c='blue', s=40, edgecolor='black')\n",
    "axes[0].plot(x_smooth, y_smooth_ridge, 'r-', linewidth=2, label='Ridge')\n",
    "test_mse_ridge = mean_squared_error(y_test, model_ridge.predict(X_test_scaled))\n",
    "axes[0].set_title(f'Ridge (L2): λ={lambda_val}\\nTest MSE: {test_mse_ridge:.4f}\\nAll weights non-zero', fontsize=11)\n",
    "axes[0].set_ylim(-0.2, 1.8)\n",
    "axes[0].legend()\n",
    "\n",
    "# Lasso\n",
    "y_smooth_lasso = np.clip(model_lasso.predict(X_smooth_scaled), -1, 3)\n",
    "axes[1].plot(x_smooth, true_function(x_smooth), 'g-', linewidth=2, alpha=0.5, label='True')\n",
    "axes[1].scatter(X_train, y_train, c='blue', s=40, edgecolor='black')\n",
    "axes[1].plot(x_smooth, y_smooth_lasso, 'r-', linewidth=2, label='Lasso')\n",
    "test_mse_lasso = mean_squared_error(y_test, model_lasso.predict(X_test_scaled))\n",
    "n_nonzero = np.sum(np.abs(model_lasso.coef_) > 1e-6)\n",
    "axes[1].set_title(f'Lasso (L1): λ={lambda_val}\\nTest MSE: {test_mse_lasso:.4f}\\nOnly {n_nonzero}/{len(model_lasso.coef_)} weights non-zero', fontsize=11)\n",
    "axes[1].set_ylim(-0.2, 1.8)\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: The Geometry of Regularization\n",
    "\n",
    "*\"Why does L1 push weights to zero while L2 merely shrinks them? The answer lies in geometry. Let me draw you the constraint regions...\"*  \n",
    "— Mink Pavar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the geometry of L1 vs L2\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Create loss contours (ellipses centered at optimal point)\n",
    "w1_range = np.linspace(-2, 4, 100)\n",
    "w2_range = np.linspace(-2, 4, 100)\n",
    "W1, W2 = np.meshgrid(w1_range, w2_range)\n",
    "\n",
    "# Simulated loss function (centered at w1=2, w2=1)\n",
    "optimal_w1, optimal_w2 = 2.0, 1.0\n",
    "Loss = 0.5 * (W1 - optimal_w1)**2 + 2 * (W2 - optimal_w2)**2\n",
    "\n",
    "# L2 constraint region (circle)\n",
    "theta = np.linspace(0, 2*np.pi, 100)\n",
    "r_l2 = 1.5\n",
    "l2_w1 = r_l2 * np.cos(theta)\n",
    "l2_w2 = r_l2 * np.sin(theta)\n",
    "\n",
    "# L1 constraint region (diamond)\n",
    "r_l1 = 1.5\n",
    "l1_w1 = [r_l1, 0, -r_l1, 0, r_l1]\n",
    "l1_w2 = [0, r_l1, 0, -r_l1, 0]\n",
    "\n",
    "# L2 (Ridge) plot\n",
    "axes[0].contour(W1, W2, Loss, levels=20, cmap='Greys', alpha=0.7)\n",
    "axes[0].fill(l2_w1, l2_w2, alpha=0.3, color='blue')\n",
    "axes[0].plot(l2_w1, l2_w2, 'b-', linewidth=2, label='L2 constraint')\n",
    "axes[0].plot(optimal_w1, optimal_w2, 'r*', markersize=15, label='Unconstrained optimum')\n",
    "# Ridge solution (on the circle)\n",
    "ridge_w1, ridge_w2 = 1.3, 0.7\n",
    "axes[0].plot(ridge_w1, ridge_w2, 'g*', markersize=15, label='Ridge solution')\n",
    "axes[0].axhline(0, color='black', linewidth=0.5)\n",
    "axes[0].axvline(0, color='black', linewidth=0.5)\n",
    "axes[0].set_xlabel('w₁', fontsize=12)\n",
    "axes[0].set_ylabel('w₂', fontsize=12)\n",
    "axes[0].set_title('L2 (Ridge): Solution on Circle\\nWeights shrink but stay non-zero', fontsize=12)\n",
    "axes[0].legend(loc='upper left')\n",
    "axes[0].set_xlim(-2.5, 4)\n",
    "axes[0].set_ylim(-2.5, 4)\n",
    "axes[0].set_aspect('equal')\n",
    "\n",
    "# L1 (Lasso) plot\n",
    "axes[1].contour(W1, W2, Loss, levels=20, cmap='Greys', alpha=0.7)\n",
    "axes[1].fill(l1_w1, l1_w2, alpha=0.3, color='red')\n",
    "axes[1].plot(l1_w1, l1_w2, 'r-', linewidth=2, label='L1 constraint')\n",
    "axes[1].plot(optimal_w1, optimal_w2, 'r*', markersize=15, label='Unconstrained optimum')\n",
    "# Lasso solution (on a corner!)\n",
    "lasso_w1, lasso_w2 = 1.5, 0  # On the corner → w2 is exactly 0!\n",
    "axes[1].plot(lasso_w1, lasso_w2, 'g*', markersize=15, label='Lasso solution')\n",
    "axes[1].axhline(0, color='black', linewidth=0.5)\n",
    "axes[1].axvline(0, color='black', linewidth=0.5)\n",
    "axes[1].set_xlabel('w₁', fontsize=12)\n",
    "axes[1].set_ylabel('w₂', fontsize=12)\n",
    "axes[1].set_title('L1 (Lasso): Solution on Diamond Corner\\nWeights hit exactly zero!', fontsize=12)\n",
    "axes[1].legend(loc='upper left')\n",
    "axes[1].set_xlim(-2.5, 4)\n",
    "axes[1].set_ylim(-2.5, 4)\n",
    "axes[1].set_aspect('equal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key Insight:\")\n",
    "print(\"- L2 constraint is a CIRCLE → solutions land on the smooth edge\")\n",
    "print(\"- L1 constraint is a DIAMOND → solutions land on CORNERS\")\n",
    "print(\"- Corners have coordinates equal to zero → AUTOMATIC FEATURE SELECTION\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Choosing λ — The Regularization Path\n",
    "\n",
    "*\"The Tribunal asked: 'How much penalty is right?' I showed them the regularization path—how weights change as we increase λ from 0 to infinity.\"*  \n",
    "— Mink Pavar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot regularization paths\n",
    "lambdas = np.logspace(-4, 2, 100)\n",
    "\n",
    "# Store weights at each lambda\n",
    "ridge_weights = []\n",
    "lasso_weights = []\n",
    "\n",
    "for lam in lambdas:\n",
    "    ridge = Ridge(alpha=lam, fit_intercept=False)\n",
    "    ridge.fit(X_train_scaled, y_train)\n",
    "    ridge_weights.append(ridge.coef_.copy())\n",
    "    \n",
    "    lasso = Lasso(alpha=lam, fit_intercept=False, max_iter=10000)\n",
    "    lasso.fit(X_train_scaled, y_train)\n",
    "    lasso_weights.append(lasso.coef_.copy())\n",
    "\n",
    "ridge_weights = np.array(ridge_weights)\n",
    "lasso_weights = np.array(lasso_weights)\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Ridge path\n",
    "for i in range(ridge_weights.shape[1]):\n",
    "    axes[0].plot(lambdas, ridge_weights[:, i], linewidth=1.5)\n",
    "axes[0].set_xscale('log')\n",
    "axes[0].set_xlabel('λ (regularization strength)', fontsize=12)\n",
    "axes[0].set_ylabel('Weight value', fontsize=12)\n",
    "axes[0].set_title('Ridge Regularization Path\\n(Weights shrink smoothly toward zero)', fontsize=12)\n",
    "axes[0].axhline(0, color='black', linewidth=0.5)\n",
    "\n",
    "# Lasso path\n",
    "for i in range(lasso_weights.shape[1]):\n",
    "    axes[1].plot(lambdas, lasso_weights[:, i], linewidth=1.5)\n",
    "axes[1].set_xscale('log')\n",
    "axes[1].set_xlabel('λ (regularization strength)', fontsize=12)\n",
    "axes[1].set_ylabel('Weight value', fontsize=12)\n",
    "axes[1].set_title('Lasso Regularization Path\\n(Weights hit zero at different λ values)', fontsize=12)\n",
    "axes[1].axhline(0, color='black', linewidth=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Observation:\")\n",
    "print(\"- Ridge: All weights approach zero asymptotically but never reach it\")\n",
    "print(\"- Lasso: Weights drop to exactly zero at different λ values\")\n",
    "print(\"- Lasso performs AUTOMATIC FEATURE SELECTION\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find optimal lambda using validation error\n",
    "train_errors_ridge = []\n",
    "test_errors_ridge = []\n",
    "train_errors_lasso = []\n",
    "test_errors_lasso = []\n",
    "\n",
    "for lam in lambdas:\n",
    "    ridge = Ridge(alpha=lam, fit_intercept=False)\n",
    "    ridge.fit(X_train_scaled, y_train)\n",
    "    train_errors_ridge.append(mean_squared_error(y_train, ridge.predict(X_train_scaled)))\n",
    "    test_errors_ridge.append(mean_squared_error(y_test, ridge.predict(X_test_scaled)))\n",
    "    \n",
    "    lasso = Lasso(alpha=lam, fit_intercept=False, max_iter=10000)\n",
    "    lasso.fit(X_train_scaled, y_train)\n",
    "    train_errors_lasso.append(mean_squared_error(y_train, lasso.predict(X_train_scaled)))\n",
    "    test_errors_lasso.append(mean_squared_error(y_test, lasso.predict(X_test_scaled)))\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Ridge\n",
    "axes[0].plot(lambdas, train_errors_ridge, 'b-', linewidth=2, label='Training')\n",
    "axes[0].plot(lambdas, test_errors_ridge, 'r-', linewidth=2, label='Test')\n",
    "best_idx_ridge = np.argmin(test_errors_ridge)\n",
    "axes[0].axvline(lambdas[best_idx_ridge], color='green', linestyle='--', \n",
    "                label=f'Best λ = {lambdas[best_idx_ridge]:.4f}')\n",
    "axes[0].set_xscale('log')\n",
    "axes[0].set_xlabel('λ', fontsize=12)\n",
    "axes[0].set_ylabel('MSE', fontsize=12)\n",
    "axes[0].set_title('Ridge: Finding Optimal λ', fontsize=12)\n",
    "axes[0].legend()\n",
    "\n",
    "# Lasso\n",
    "axes[1].plot(lambdas, train_errors_lasso, 'b-', linewidth=2, label='Training')\n",
    "axes[1].plot(lambdas, test_errors_lasso, 'r-', linewidth=2, label='Test')\n",
    "best_idx_lasso = np.argmin(test_errors_lasso)\n",
    "axes[1].axvline(lambdas[best_idx_lasso], color='green', linestyle='--', \n",
    "                label=f'Best λ = {lambdas[best_idx_lasso]:.4f}')\n",
    "axes[1].set_xscale('log')\n",
    "axes[1].set_xlabel('λ', fontsize=12)\n",
    "axes[1].set_ylabel('MSE', fontsize=12)\n",
    "axes[1].set_title('Lasso: Finding Optimal λ', fontsize=12)\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Best Ridge λ: {lambdas[best_idx_ridge]:.4f} (Test MSE: {test_errors_ridge[best_idx_ridge]:.4f})\")\n",
    "print(f\"Best Lasso λ: {lambdas[best_idx_lasso]:.4f} (Test MSE: {test_errors_lasso[best_idx_lasso]:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Applying to Manuscript Forgery Detection\n",
    "\n",
    "*\"Now let us apply regularization to the forgery problem. Which features truly matter? Lasso will tell us.\"*  \n",
    "— Mink Pavar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare manuscript data with multiple features\n",
    "feature_cols = ['stylometric_variance', 'era_marker_score', 'vocabulary_richness', \n",
    "                'avg_sentence_length', 'philosophical_term_density', 'word_count']\n",
    "\n",
    "X_ms = manuscripts[feature_cols].values\n",
    "y_ms = manuscripts['is_forgery'].astype(int).values\n",
    "\n",
    "# Split and scale\n",
    "X_train_ms, X_test_ms, y_train_ms, y_test_ms = train_test_split(\n",
    "    X_ms, y_ms, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "scaler_ms = StandardScaler()\n",
    "X_train_ms_scaled = scaler_ms.fit_transform(X_train_ms)\n",
    "X_test_ms_scaled = scaler_ms.transform(X_test_ms)\n",
    "\n",
    "print(f\"Features: {feature_cols}\")\n",
    "print(f\"Training samples: {len(X_train_ms)}\")\n",
    "print(f\"Test samples: {len(X_test_ms)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare unregularized, Ridge, and Lasso\n",
    "models = {\n",
    "    'Unregularized': LinearRegression(),\n",
    "    'Ridge (λ=0.1)': Ridge(alpha=0.1),\n",
    "    'Lasso (λ=0.01)': Lasso(alpha=0.01, max_iter=10000),\n",
    "}\n",
    "\n",
    "print(\"Feature Importance Comparison for Forgery Detection\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "results = []\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_ms_scaled, y_train_ms)\n",
    "    train_mse = mean_squared_error(y_train_ms, model.predict(X_train_ms_scaled))\n",
    "    test_mse = mean_squared_error(y_test_ms, model.predict(X_test_ms_scaled))\n",
    "    results.append((name, train_mse, test_mse, model.coef_))\n",
    "\n",
    "# Print weights\n",
    "print(f\"\\n{'Feature':<30} | {'Unreg':>10} | {'Ridge':>10} | {'Lasso':>10}\")\n",
    "print(\"-\" * 75)\n",
    "for i, feat in enumerate(feature_cols):\n",
    "    unreg_w = results[0][3][i]\n",
    "    ridge_w = results[1][3][i]\n",
    "    lasso_w = results[2][3][i]\n",
    "    zero_marker = \"*\" if abs(lasso_w) < 1e-6 else \"\"\n",
    "    print(f\"{feat:<30} | {unreg_w:>10.4f} | {ridge_w:>10.4f} | {lasso_w:>10.4f}{zero_marker}\")\n",
    "\n",
    "print(\"-\" * 75)\n",
    "print(\"* = weight set to zero by Lasso\")\n",
    "\n",
    "print(f\"\\n{'Model':<20} | {'Train MSE':>12} | {'Test MSE':>12}\")\n",
    "print(\"-\" * 50)\n",
    "for name, train_mse, test_mse, _ in results:\n",
    "    print(f\"{name:<20} | {train_mse:>12.4f} | {test_mse:>12.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature importance\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "x_pos = np.arange(len(feature_cols))\n",
    "width = 0.25\n",
    "\n",
    "colors = ['steelblue', 'coral', 'green']\n",
    "for i, (name, _, _, weights) in enumerate(results):\n",
    "    ax.bar(x_pos + i*width, np.abs(weights), width, label=name, color=colors[i], alpha=0.8)\n",
    "\n",
    "ax.set_xticks(x_pos + width)\n",
    "ax.set_xticklabels([f.replace('_', '\\n') for f in feature_cols], fontsize=9)\n",
    "ax.set_ylabel('|Weight| (Absolute Value)', fontsize=12)\n",
    "ax.set_title('Feature Importance for Forgery Detection\\n(Lasso Performs Automatic Feature Selection)', fontsize=13)\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find selected features\n",
    "lasso_weights = results[2][3]\n",
    "selected_features = [f for f, w in zip(feature_cols, lasso_weights) if abs(w) > 1e-6]\n",
    "print(f\"\\nLasso selected features: {selected_features}\")\n",
    "print(f\"Lasso eliminated features: {[f for f in feature_cols if f not in selected_features]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: When to Use Ridge vs Lasso\n",
    "\n",
    "*\"The Tribunal asked: 'Which is better—Ridge or Lasso?' I told them: it depends on your beliefs about the problem.\"*  \n",
    "— Mink Pavar\n",
    "\n",
    "### Decision Guide\n",
    "\n",
    "| Situation | Use Ridge (L2) | Use Lasso (L1) |\n",
    "|-----------|----------------|----------------|\n",
    "| All features likely relevant | ✓ | |\n",
    "| Many irrelevant features | | ✓ |\n",
    "| Feature selection needed | | ✓ |\n",
    "| Correlated features | ✓ | |\n",
    "| Interpretability important | | ✓ |\n",
    "| Numerical stability needed | ✓ | |\n",
    "\n",
    "### Elastic Net: The Best of Both Worlds\n",
    "\n",
    "$$L_{\\text{Elastic}} = \\text{MSE} + \\lambda_1 \\sum |w_j| + \\lambda_2 \\sum w_j^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "# Compare all three on manuscript data\n",
    "elastic = ElasticNet(alpha=0.01, l1_ratio=0.5, max_iter=10000)  # 50% L1, 50% L2\n",
    "elastic.fit(X_train_ms_scaled, y_train_ms)\n",
    "\n",
    "print(\"Elastic Net Weights (50% L1 + 50% L2):\")\n",
    "print(\"=\" * 50)\n",
    "for feat, w in zip(feature_cols, elastic.coef_):\n",
    "    zero_marker = \"← ZERO\" if abs(w) < 1e-6 else \"\"\n",
    "    print(f\"  {feat:30}: {w:8.4f} {zero_marker}\")\n",
    "\n",
    "test_mse_elastic = mean_squared_error(y_test_ms, elastic.predict(X_test_ms_scaled))\n",
    "print(f\"\\nElastic Net Test MSE: {test_mse_elastic:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercises\n",
    "\n",
    "### Exercise 1: Optimal Lambda Search\n",
    "\n",
    "Use cross-validation to find the optimal λ for Ridge regression on the manuscript data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Find optimal lambda using cross-validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "lambdas_to_try = [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# for lam in lambdas_to_try:\n",
    "#     ridge = Ridge(alpha=lam)\n",
    "#     scores = cross_val_score(ridge, X_train_ms_scaled, y_train_ms, \n",
    "#                              cv=5, scoring='neg_mean_squared_error')\n",
    "#     print(f\"λ={lam}: Mean CV MSE = {-scores.mean():.4f} (+/- {scores.std():.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Feature Selection with Lasso\n",
    "\n",
    "Find the λ value where Lasso selects exactly 3 features for forgery detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Find lambda for exactly 3 features\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# Test different lambda values and count non-zero weights\n",
    "# for lam in [0.001, 0.005, 0.01, 0.02, 0.05, 0.1]:\n",
    "#     lasso = Lasso(alpha=lam, max_iter=10000)\n",
    "#     lasso.fit(X_train_ms_scaled, y_train_ms)\n",
    "#     n_features = np.sum(np.abs(lasso.coef_) > 1e-6)\n",
    "#     print(f\"λ={lam}: {n_features} non-zero features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Regularization for Expedition Data\n",
    "\n",
    "Apply Ridge and Lasso to predict expedition catch_value from multiple features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Expedition catch value prediction\n",
    "\n",
    "exp_features = ['days_in_field', 'crew_size', 'leader_experience_years', 'creature_encounters']\n",
    "# Note: You'll need to handle missing values if any\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# X_exp = expeditions[exp_features].values\n",
    "# y_exp = expeditions['catch_value'].values\n",
    "# \n",
    "# Split, scale, fit Ridge and Lasso, compare results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: The Effect of Scaling\n",
    "\n",
    "What happens if you apply regularization WITHOUT scaling the features first? Compare results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4: Regularization without scaling\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# Fit Ridge on scaled vs unscaled data\n",
    "# Compare the weights - what do you observe?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: Elastic Net Tuning\n",
    "\n",
    "Tune both α (overall strength) and l1_ratio (L1 vs L2 balance) for Elastic Net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 5: Elastic Net parameter tuning\n",
    "\n",
    "alphas = [0.001, 0.01, 0.1]\n",
    "l1_ratios = [0.1, 0.5, 0.9]  # 0.1 = mostly L2, 0.9 = mostly L1\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# for alpha in alphas:\n",
    "#     for l1_ratio in l1_ratios:\n",
    "#         elastic = ElasticNet(alpha=alpha, l1_ratio=l1_ratio, max_iter=10000)\n",
    "#         elastic.fit(X_train_ms_scaled, y_train_ms)\n",
    "#         test_mse = mean_squared_error(y_test_ms, elastic.predict(X_test_ms_scaled))\n",
    "#         n_features = np.sum(np.abs(elastic.coef_) > 1e-6)\n",
    "#         print(f\"α={alpha}, L1_ratio={l1_ratio}: MSE={test_mse:.4f}, features={n_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "| Method | Penalty | Effect | Use When |\n",
    "|--------|---------|--------|----------|\n",
    "| **Ridge (L2)** | $\\lambda \\sum w_j^2$ | Shrinks all weights | All features relevant, correlated features |\n",
    "| **Lasso (L1)** | $\\lambda \\sum |w_j|$ | Zeros some weights | Feature selection needed |\n",
    "| **Elastic Net** | Both L1 + L2 | Best of both | Groups of correlated features |\n",
    "\n",
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Regularization prevents overfitting** by penalizing large weights\n",
    "\n",
    "2. **L2 (Ridge) shrinks weights smoothly** — all features remain, but with smaller influence\n",
    "\n",
    "3. **L1 (Lasso) creates sparse solutions** — automatic feature selection\n",
    "\n",
    "4. **The geometry explains the difference** — L2 constraint is a circle, L1 is a diamond with corners\n",
    "\n",
    "5. **λ controls the trade-off** — use validation data to find the optimal value\n",
    "\n",
    "6. **Always scale features** before applying regularization\n",
    "\n",
    "---\n",
    "\n",
    "## Next Lesson\n",
    "\n",
    "In **Lesson 4: Model Selection & Cross-Validation (Capstone)**, we'll bring everything together. We'll learn how to properly evaluate models, tune hyperparameters, and build a complete ML pipeline for the forgery detection challenge.\n",
    "\n",
    "*\"The Tribunal has heard about loss functions, bias-variance trade-offs, and regularization. Now comes the final test: building a complete system that can be trusted to classify manuscripts we have never seen. This is the true challenge of machine learning—generalization.\"*  \n",
    "— Mink Pavar, preparing for the final day of testimony"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
