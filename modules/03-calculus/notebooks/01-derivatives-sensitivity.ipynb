{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/buildLittleWorlds/ml-math-with-densworld/blob/main/modules/03-calculus/notebooks/01-derivatives-sensitivity.ipynb)\n",
    "\n",
    "# Lesson 1: Derivatives as Sensitivity\n",
    "\n",
    "*\"If I adjust my stratagem by the smallest fraction, how much closer—or further—does the Tower's fall become? This is the question that haunts my nights. Every siege is a function; every choice, a variable. I must learn to feel the gradient of war.\"*  \n",
    "— The Colonel, private journals, Year 15 of the Siege\n",
    "\n",
    "---\n",
    "\n",
    "## The Core Reframe\n",
    "\n",
    "In school, you learned:\n",
    "> \"The derivative is the slope of the tangent line.\"\n",
    "\n",
    "That's geometrically true but **not useful** for machine learning—or for understanding the Colonel's siege of the Tower of Mirado.\n",
    "\n",
    "Here's the ML interpretation:\n",
    "> \"The derivative measures **sensitivity** — how much the output changes when we nudge the input.\"\n",
    "\n",
    "For the Colonel besieging the Tower:\n",
    "> \"If I adjust my attack strategy slightly, how much does my progress toward breaching the Tower change?\"\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this lesson, you will:\n",
    "1. Think of derivatives as sensitivity measures, not just slopes\n",
    "2. Understand what \"high\" vs \"low\" derivatives mean practically\n",
    "3. See why this matters for training ML models—and for the Colonel's siege\n",
    "4. Compute numerical derivatives and interpret their meaning\n",
    "5. Connect derivatives to the concept of optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Nice plotting defaults\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "# Colab-ready data loading\n",
    "BASE_URL = \"https://raw.githubusercontent.com/buildLittleWorlds/ml-math-with-densworld/main/data/\"\n",
    "\n",
    "# Load the siege progress data\n",
    "siege = pd.read_csv(BASE_URL + \"siege_progress.csv\")\n",
    "stratagem = pd.read_csv(BASE_URL + \"stratagem_details.csv\")\n",
    "\n",
    "print(f\"Loaded {len(siege)} months of siege records\")\n",
    "print(f\"Loaded {len(stratagem)} individual stratagem attempts\")\n",
    "print(f\"Siege duration: {siege['year'].max()} years\")\n",
    "siege.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## The Colonel's Optimization Problem\n",
    "\n",
    "*\"Twenty years I have spent beneath the shadow of this Tower. Twenty years of trial and failure, of blood and patience. The Tower does not negotiate. It simply waits. But I have learned something in these decades: there is a landscape beneath the landscape—a terrain of cause and effect that I navigate blind, feeling for the downhill slope.\"*  \n",
    "— The Colonel, addressing his officers, Year 19\n",
    "\n",
    "The Colonel has been besieging the Tower of Mirado for 20 years. His **loss function** is simple: the distance from breaching the Tower (1 - progress_score). His **parameters** are his stratagems—ladder assaults, grappling hooks, tunneling, parley, waiting.\n",
    "\n",
    "**The Question**: If I change my approach, how much does my loss change?\n",
    "\n",
    "- If the answer is \"a lot\" → High sensitivity → Large derivative\n",
    "- If the answer is \"barely at all\" → Low sensitivity → Small derivative\n",
    "\n",
    "---\n",
    "\n",
    "## Part 1: The Sensitivity Intuition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize progress over time\n",
    "fig, axes = plt.subplots(2, 1, figsize=(12, 8), sharex=True)\n",
    "\n",
    "# Progress score over time\n",
    "axes[0].plot(siege['month_total'], siege['progress_score'], 'b-', linewidth=1.5)\n",
    "axes[0].fill_between(siege['month_total'], 0, siege['progress_score'], alpha=0.3)\n",
    "axes[0].set_ylabel('Progress Score', fontsize=11)\n",
    "axes[0].set_title('The Colonel\\'s Siege: 20 Years of Effort', fontsize=13)\n",
    "axes[0].set_ylim(0, 0.3)\n",
    "\n",
    "# Loss (1 - progress) over time\n",
    "axes[1].plot(siege['month_total'], siege['loss'], 'r-', linewidth=1.5)\n",
    "axes[1].fill_between(siege['month_total'], siege['loss'], 1, alpha=0.3, color='red')\n",
    "axes[1].set_ylabel('Loss (1 - Progress)', fontsize=11)\n",
    "axes[1].set_xlabel('Month of Siege', fontsize=11)\n",
    "axes[1].set_title('What We Want to Minimize: Distance from Breaching the Tower', fontsize=13)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Starting progress: {siege['progress_score'].iloc[0]:.1%}\")\n",
    "print(f\"Final progress: {siege['progress_score'].iloc[-1]:.1%}\")\n",
    "print(f\"\\nThe Colonel has made {siege['progress_score'].iloc[-1]:.1%} progress in 20 years.\")\n",
    "print(\"The Tower of Mirado does not fall easily.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Part 2: Sensitivity Changes Over Time\n",
    "\n",
    "For non-linear systems, the sensitivity **depends on where you are**.\n",
    "\n",
    "Early in the siege, when progress is low, the Tower's defenses may respond differently than later. The same change in strategy might have dramatically different effects depending on the current state.\n",
    "\n",
    "*\"In the early years, every assault seemed to matter. A ladder gained or lost could shift the momentum. Now, in Year 15, the increments grow smaller. The Tower has adapted to us, as we have adapted to it.\"*  \n",
    "— The Colonel\n",
    "\n",
    "Let's look at how sensitivity (the derivative of progress with respect to effort) changes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The 'progress_delta' column shows how much progress changed each month\n",
    "# This is approximately the derivative!\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(12, 8), sharex=True)\n",
    "\n",
    "# Progress delta (the \"derivative\")\n",
    "colors = ['green' if d > 0 else 'red' for d in siege['progress_delta']]\n",
    "axes[0].bar(siege['month_total'], siege['progress_delta'], color=colors, alpha=0.7, width=1)\n",
    "axes[0].axhline(0, color='black', linewidth=0.5)\n",
    "axes[0].set_ylabel('Progress Change (Δ)', fontsize=11)\n",
    "axes[0].set_title('Monthly Change in Progress\\n(Green = Progress, Red = Regression)', fontsize=13)\n",
    "axes[0].set_ylim(-0.03, 0.08)\n",
    "\n",
    "# Rolling average of progress delta (smoothed derivative)\n",
    "rolling_avg = siege['progress_delta'].rolling(window=12).mean()\n",
    "axes[1].plot(siege['month_total'], rolling_avg, 'purple', linewidth=2)\n",
    "axes[1].axhline(0, color='black', linewidth=0.5)\n",
    "axes[1].fill_between(siege['month_total'], 0, rolling_avg, \n",
    "                     where=rolling_avg > 0, alpha=0.3, color='green')\n",
    "axes[1].fill_between(siege['month_total'], rolling_avg, 0, \n",
    "                     where=rolling_avg < 0, alpha=0.3, color='red')\n",
    "axes[1].set_ylabel('12-Month Rolling Average', fontsize=11)\n",
    "axes[1].set_xlabel('Month of Siege', fontsize=11)\n",
    "axes[1].set_title('Smoothed Sensitivity: How Effective is the Colonel\\'s Effort?', fontsize=13)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Notice how the derivative (rate of progress) varies over time.\")\n",
    "print(\"Some periods show high sensitivity—small changes yield big results.\")\n",
    "print(\"Other periods show near-zero sensitivity—the Colonel is stuck.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## Part 3: Why ML Cares About Sensitivity\n",
    "\n",
    "In machine learning, we have:\n",
    "- **Parameters** (weights): the knobs we can turn (like choosing a stratagem)\n",
    "- **Loss** (error): what we want to minimize (like distance from breaching the Tower)\n",
    "\n",
    "The derivative tells us: **\"If I adjust this parameter a tiny bit, how much will the error change?\"**\n",
    "\n",
    "This is exactly what the Colonel needs to know!\n",
    "\n",
    "### Connecting to the Siege\n",
    "\n",
    "The Colonel has been recording an `estimated_gradient`—his sense of which direction reduces loss. But his estimates are noisy. He can't see the true loss landscape; he navigates by feel.\n",
    "\n",
    "*\"I cannot see the true gradient. The Tower reveals nothing. I estimate, I guess, I intuit. Some months my intuition is sharp; others, I am blind. This is the curse of optimization in the dark.\"*  \n",
    "— The Colonel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare actual progress to the Colonel's estimated gradient\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "scatter = ax.scatter(siege['estimated_gradient'], siege['progress_delta'], \n",
    "                     alpha=0.5, c=siege['month_total'], cmap='viridis', s=40)\n",
    "ax.axhline(0, color='black', linewidth=0.5)\n",
    "ax.axvline(0, color='black', linewidth=0.5)\n",
    "\n",
    "# Add regression line\n",
    "z = np.polyfit(siege['estimated_gradient'], siege['progress_delta'], 1)\n",
    "p = np.poly1d(z)\n",
    "x_line = np.linspace(siege['estimated_gradient'].min(), siege['estimated_gradient'].max(), 100)\n",
    "ax.plot(x_line, p(x_line), 'r--', linewidth=2, label='Trend')\n",
    "\n",
    "ax.set_xlabel('Colonel\\'s Estimated Gradient (his sense of direction)', fontsize=11)\n",
    "ax.set_ylabel('Actual Progress Delta', fontsize=11)\n",
    "ax.set_title('How Well Does the Colonel Estimate the True Gradient?\\n(Color = Month of Siege)', fontsize=13)\n",
    "ax.legend()\n",
    "plt.colorbar(scatter, label='Month')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "correlation = siege['estimated_gradient'].corr(siege['progress_delta'])\n",
    "print(f\"Correlation between estimated and actual gradient: {correlation:.3f}\")\n",
    "print(\"\\nThe Colonel's estimates are noisy—he can't see the true loss landscape.\")\n",
    "print(\"This is exactly the challenge faced by stochastic gradient descent!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## Part 4: The Sign of the Derivative\n",
    "\n",
    "The **sign** of the derivative is crucial:\n",
    "\n",
    "| Derivative Sign | Meaning | What to do |\n",
    "|-----------------|---------|------------|\n",
    "| Positive (+) | More effort → More loss | **Reduce** effort in this direction |\n",
    "| Negative (-) | More effort → Less loss | **Increase** effort in this direction |\n",
    "| Zero (0) | At a minimum (or maximum) | You might be done! |\n",
    "\n",
    "For the Colonel: if a stratagem consistently leads to negative progress, he should try something else.\n",
    "\n",
    "Let's analyze which stratagems work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze effectiveness by stratagem\n",
    "stratagem_analysis = siege.groupby('stratagem_attempted').agg({\n",
    "    'progress_delta': ['mean', 'std', 'count'],\n",
    "    'morale_index': 'mean',\n",
    "    'supply_level': 'mean'\n",
    "}).round(4)\n",
    "\n",
    "stratagem_analysis.columns = ['avg_progress', 'std_progress', 'count', 'avg_morale', 'avg_supplies']\n",
    "stratagem_analysis = stratagem_analysis.sort_values('avg_progress', ascending=False)\n",
    "\n",
    "print(\"Stratagem Effectiveness Analysis:\")\n",
    "print(\"=\" * 70)\n",
    "print(stratagem_analysis.to_string())\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Interpretation:\")\n",
    "print(\"- Positive avg_progress = stratagem tends to help (negative derivative of loss)\")\n",
    "print(\"- Negative avg_progress = stratagem tends to hurt (positive derivative of loss)\")\n",
    "print(\"- High std = unpredictable results—noisy gradient estimates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize stratagem effectiveness\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "stratagems = stratagem_analysis.index\n",
    "progress = stratagem_analysis['avg_progress']\n",
    "errors = stratagem_analysis['std_progress']\n",
    "counts = stratagem_analysis['count']\n",
    "\n",
    "colors = ['green' if p > 0 else 'red' for p in progress]\n",
    "bars = ax.bar(stratagems, progress, color=colors, alpha=0.7, edgecolor='black')\n",
    "ax.errorbar(stratagems, progress, yerr=errors, fmt='none', color='black', capsize=5)\n",
    "\n",
    "# Add count labels\n",
    "for bar, count in zip(bars, counts):\n",
    "    height = bar.get_height()\n",
    "    ax.annotate(f'n={count}',\n",
    "                xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                xytext=(0, 3 if height > 0 else -15),\n",
    "                textcoords=\"offset points\",\n",
    "                ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "ax.axhline(0, color='black', linewidth=1)\n",
    "ax.set_ylabel('Average Progress per Month', fontsize=11)\n",
    "ax.set_xlabel('Stratagem', fontsize=11)\n",
    "ax.set_title('Which Stratagems Move the Colonel Toward His Goal?\\n(Error bars = standard deviation)', fontsize=13)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## Part 5: Computing Derivatives Numerically\n",
    "\n",
    "You don't always need the mathematical formula for a derivative.\n",
    "\n",
    "The **numerical derivative** approximates it by actually nudging the input and measuring the change:\n",
    "\n",
    "$$f'(x) \\approx \\frac{f(x + h) - f(x)}{h}$$\n",
    "\n",
    "where $h$ is a tiny number (like 0.0001).\n",
    "\n",
    "This is literally: \"nudge x by h, see how much f changes, divide by the nudge size.\"\n",
    "\n",
    "*\"I do this instinctively. I try a slight variation—one more ladder, one fewer sapper—and observe the result. The Tower is my function; my choices are the input; progress is the output. I am computing gradients without knowing the formula.\"*  \n",
    "— The Colonel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_derivative(f, x, h=1e-5):\n",
    "    \"\"\"Compute derivative of f at point x using finite differences.\"\"\"\n",
    "    return (f(x + h) - f(x)) / h\n",
    "\n",
    "def numerical_derivative_centered(f, x, h=1e-5):\n",
    "    \"\"\"Compute derivative using centered differences (more accurate).\"\"\"\n",
    "    return (f(x + h) - f(x - h)) / (2 * h)\n",
    "\n",
    "# Example: A simple loss function (like the Colonel's distance from the Tower)\n",
    "def loss_function(effort):\n",
    "    \"\"\"Simulated loss: decreases with effort, but with diminishing returns.\"\"\"\n",
    "    return 1 / (1 + 0.1 * effort)  # Starts at 1, approaches 0\n",
    "\n",
    "# Test the numerical derivative at different effort levels\n",
    "effort_levels = [0, 5, 10, 20, 50, 100]\n",
    "print(\"Numerical Derivatives of Loss Function:\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Effort':>10} | {'Loss':>10} | {'Derivative':>12} | Interpretation\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for effort in effort_levels:\n",
    "    loss = loss_function(effort)\n",
    "    deriv = numerical_derivative(loss_function, effort)\n",
    "    interp = \"High sensitivity\" if abs(deriv) > 0.005 else \"Low sensitivity\"\n",
    "    print(f\"{effort:>10} | {loss:>10.4f} | {deriv:>12.6f} | {interp}\")\n",
    "\n",
    "print(\"\\nNote: All derivatives are negative (more effort → less loss)\")\n",
    "print(\"But the magnitude decreases—diminishing returns!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the loss function and its derivative\n",
    "effort_range = np.linspace(0, 100, 200)\n",
    "losses = [loss_function(e) for e in effort_range]\n",
    "derivatives = [numerical_derivative(loss_function, e) for e in effort_range]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss function\n",
    "axes[0].plot(effort_range, losses, 'b-', linewidth=2)\n",
    "axes[0].set_xlabel('Effort Level', fontsize=11)\n",
    "axes[0].set_ylabel('Loss (Distance from Goal)', fontsize=11)\n",
    "axes[0].set_title('The Loss Function\\n(What the Colonel wants to minimize)', fontsize=12)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Derivative (sensitivity)\n",
    "axes[1].plot(effort_range, derivatives, 'r-', linewidth=2)\n",
    "axes[1].axhline(0, color='black', linewidth=0.5)\n",
    "axes[1].set_xlabel('Effort Level', fontsize=11)\n",
    "axes[1].set_ylabel('Derivative (Sensitivity)', fontsize=11)\n",
    "axes[1].set_title('The Derivative\\n(How much does effort reduce loss?)', fontsize=12)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Left: The loss decreases with effort, but flattens out.\")\n",
    "print(\"Right: The derivative (sensitivity) is always negative but approaches zero.\")\n",
    "print(\"\\nThis is diminishing returns: early effort is highly effective, later effort less so.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## Part 6: The Stratagem Details — Gradient Estimation in Practice\n",
    "\n",
    "The Colonel's records include detailed stratagem-level data with both his **estimated gradient** and the **actual gradient** (computed after the fact). This lets us see how well he navigates the loss landscape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the stratagem details\n",
    "print(\"Stratagem Details — The Colonel's Gradient Estimation Record:\")\n",
    "print(\"=\" * 90)\n",
    "cols = ['stratagem_id', 'stratagem_type', 'estimated_gradient', 'actual_gradient', \n",
    "        'gradient_error', 'progress_delta', 'outcome_category']\n",
    "print(stratagem[cols].head(15).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How accurate are the Colonel's gradient estimates?\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Scatter: estimated vs actual\n",
    "axes[0].scatter(stratagem['estimated_gradient'], stratagem['actual_gradient'], \n",
    "                alpha=0.5, c='steelblue', edgecolor='white', s=50)\n",
    "# Perfect estimation line\n",
    "lims = [min(stratagem['estimated_gradient'].min(), stratagem['actual_gradient'].min()),\n",
    "        max(stratagem['estimated_gradient'].max(), stratagem['actual_gradient'].max())]\n",
    "axes[0].plot(lims, lims, 'r--', linewidth=2, label='Perfect estimation')\n",
    "axes[0].set_xlabel('Colonel\\'s Estimated Gradient', fontsize=11)\n",
    "axes[0].set_ylabel('Actual Gradient', fontsize=11)\n",
    "axes[0].set_title('Gradient Estimation Accuracy', fontsize=12)\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Distribution of gradient errors\n",
    "axes[1].hist(stratagem['gradient_error'], bins=30, color='coral', edgecolor='black', alpha=0.7)\n",
    "axes[1].axvline(0, color='black', linewidth=1, linestyle='--')\n",
    "axes[1].axvline(stratagem['gradient_error'].mean(), color='red', linewidth=2, \n",
    "                label=f'Mean error: {stratagem[\"gradient_error\"].mean():.4f}')\n",
    "axes[1].set_xlabel('Gradient Error (Estimated - Actual)', fontsize=11)\n",
    "axes[1].set_ylabel('Frequency', fontsize=11)\n",
    "axes[1].set_title('Distribution of the Colonel\\'s Estimation Errors', fontsize=12)\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "correlation = stratagem['estimated_gradient'].corr(stratagem['actual_gradient'])\n",
    "mean_abs_error = stratagem['gradient_error'].abs().mean()\n",
    "print(f\"Correlation between estimated and actual: {correlation:.3f}\")\n",
    "print(f\"Mean absolute error: {mean_abs_error:.4f}\")\n",
    "print(\"\\nThe Colonel's estimates are correlated with truth, but noisy.\")\n",
    "print(\"This is stochastic gradient descent in action!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## Part 7: What Makes a Good Gradient Estimate?\n",
    "\n",
    "Looking at which stratagems give the Colonel better gradient estimates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze gradient estimation quality by stratagem type\n",
    "gradient_quality = stratagem.groupby('stratagem_type').agg({\n",
    "    'gradient_error': ['mean', 'std', 'count'],\n",
    "    'was_optimal_direction': 'mean'  # How often did he get the direction right?\n",
    "}).round(4)\n",
    "\n",
    "gradient_quality.columns = ['mean_error', 'std_error', 'count', 'correct_direction_%']\n",
    "gradient_quality['correct_direction_%'] = (gradient_quality['correct_direction_%'] * 100).round(1)\n",
    "gradient_quality = gradient_quality.sort_values('correct_direction_%', ascending=False)\n",
    "\n",
    "print(\"Gradient Estimation Quality by Stratagem Type:\")\n",
    "print(\"=\" * 70)\n",
    "print(gradient_quality.to_string())\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"- 'correct_direction_%' = how often the Colonel correctly identified uphill/downhill\")\n",
    "print(\"- Some stratagems give clearer feedback than others\")\n",
    "print(\"- Reconnaissance-type actions tend to improve gradient estimation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercises\n",
    "\n",
    "### Exercise 1: Stratagem Analysis\n",
    "\n",
    "The Colonel used 'parley' many times with near-zero average progress. Using the concept of derivatives, explain why he might have kept trying it despite poor results.\n",
    "\n",
    "*Hint: Consider the variance and the cost of the stratagem.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Why did the Colonel keep trying parley?\n",
    "# Analyze parley's characteristics\n",
    "\n",
    "parley_data = stratagem[stratagem['stratagem_type'] == 'parley']\n",
    "\n",
    "print(\"Parley Stratagem Analysis:\")\n",
    "print(f\"Number of attempts: {len(parley_data)}\")\n",
    "print(f\"Mean progress delta: {parley_data['progress_delta'].mean():.4f}\")\n",
    "print(f\"Std progress delta: {parley_data['progress_delta'].std():.4f}\")\n",
    "print(f\"Risk level (mean): {parley_data['risk_level'].mean():.2f}\")\n",
    "print(f\"Personnel committed (mean): {parley_data['personnel_committed'].mean():.1f}\")\n",
    "print(f\"Supply cost (mean): {parley_data['supply_cost'].mean():.1f}\")\n",
    "print(f\"Casualties (total): {parley_data['casualties'].sum()}\")\n",
    "\n",
    "# Your interpretation:\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"INTERPRETATION:\")\n",
    "print(\"Parley has near-zero expected gradient (no progress),\")\n",
    "print(\"but it has LOW COST and LOW RISK.\")\n",
    "print(\"The Colonel uses it when other options are too expensive.\")\n",
    "print(\"It's like a 'safe' step in gradient descent—no progress, but no regression either.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "### Exercise 2: Numerical Derivatives from Siege Data\n",
    "\n",
    "Compute the numerical derivative of the actual siege progress score at months 50, 100, and 200. How does sensitivity change over time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Compute numerical derivatives from siege data\n",
    "# The progress_delta column IS the discrete derivative!\n",
    "\n",
    "months_to_analyze = [50, 100, 150, 200]\n",
    "\n",
    "print(\"Sensitivity Analysis at Different Time Points:\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Month':>8} | {'Progress':>10} | {'Delta (Deriv)':>14} | {'12-mo Avg':>12}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for month in months_to_analyze:\n",
    "    if month < len(siege):\n",
    "        row = siege[siege['month_total'] == month].iloc[0]\n",
    "        # 12-month rolling average around this point\n",
    "        start = max(0, month - 6)\n",
    "        end = min(len(siege), month + 6)\n",
    "        rolling = siege.iloc[start:end]['progress_delta'].mean()\n",
    "        print(f\"{month:>8} | {row['progress_score']:>10.4f} | {row['progress_delta']:>14.4f} | {rolling:>12.4f}\")\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"The derivative fluctuates month-to-month (noise),\")\n",
    "print(\"but the rolling average shows the underlying trend.\")\n",
    "print(\"Sensitivity tends to decrease as the siege progresses—\")\n",
    "print(\"the Tower becomes harder to crack with each year.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "### Exercise 3: Perfect Information\n",
    "\n",
    "If the Colonel had perfect knowledge of the gradient (no estimation error), how might his strategy have been different? Analyze the cases where his estimate was most wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: What if the Colonel had perfect gradient knowledge?\n",
    "\n",
    "# Find the stratagems with the largest gradient errors\n",
    "stratagem_sorted = stratagem.sort_values('gradient_error', key=abs, ascending=False)\n",
    "\n",
    "print(\"Top 10 Largest Gradient Estimation Errors:\")\n",
    "print(\"=\" * 100)\n",
    "cols = ['stratagem_id', 'stratagem_type', 'estimated_gradient', 'actual_gradient', \n",
    "        'gradient_error', 'outcome_category', 'casualties']\n",
    "print(stratagem_sorted[cols].head(10).to_string(index=False))\n",
    "\n",
    "# How many disasters could have been avoided?\n",
    "disasters = stratagem[stratagem['outcome_category'] == 'disaster']\n",
    "wrong_direction_disasters = disasters[~disasters['was_optimal_direction']]\n",
    "\n",
    "print(f\"\\n\\nTotal disasters: {len(disasters)}\")\n",
    "print(f\"Disasters where Colonel went wrong direction: {len(wrong_direction_disasters)}\")\n",
    "print(f\"Total casualties in wrong-direction disasters: {wrong_direction_disasters['casualties'].sum()}\")\n",
    "print(\"\\nWith perfect gradient information, the Colonel could have avoided\")\n",
    "print(f\"approximately {len(wrong_direction_disasters)} disasters and {wrong_direction_disasters['casualties'].sum()} casualties.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": [
    "### Exercise 4: Design Your Own Stratagem\n",
    "\n",
    "Based on the gradient analysis, propose an optimal stratagem mix for the Colonel. Which stratagems should he favor? Which should he avoid?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4: Design optimal stratagem mix\n",
    "\n",
    "# Calculate efficiency: progress per casualty and per supply\n",
    "strat_efficiency = stratagem.groupby('stratagem_type').agg({\n",
    "    'progress_delta': 'mean',\n",
    "    'casualties': 'mean',\n",
    "    'supply_cost': 'mean',\n",
    "    'gradient_error': lambda x: x.abs().mean(),  # Mean absolute error\n",
    "    'was_optimal_direction': 'mean'\n",
    "}).round(4)\n",
    "\n",
    "strat_efficiency.columns = ['avg_progress', 'avg_casualties', 'avg_supply', 'mae_gradient', 'direction_accuracy']\n",
    "\n",
    "# Calculate efficiency metrics\n",
    "strat_efficiency['progress_per_casualty'] = (\n",
    "    strat_efficiency['avg_progress'] / (strat_efficiency['avg_casualties'] + 0.1)\n",
    ").round(4)\n",
    "strat_efficiency['progress_per_supply'] = (\n",
    "    strat_efficiency['avg_progress'] / (strat_efficiency['avg_supply'] + 1)\n",
    ").round(4)\n",
    "\n",
    "print(\"Stratagem Efficiency Analysis:\")\n",
    "print(\"=\" * 100)\n",
    "print(strat_efficiency.to_string())\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"OPTIMAL STRATEGY RECOMMENDATION:\")\n",
    "print(\"=\"*50)\n",
    "best_progress = strat_efficiency['avg_progress'].idxmax()\n",
    "best_efficiency = strat_efficiency['progress_per_casualty'].idxmax()\n",
    "best_gradient = strat_efficiency['direction_accuracy'].idxmax()\n",
    "\n",
    "print(f\"Highest average progress: {best_progress}\")\n",
    "print(f\"Best progress per casualty: {best_efficiency}\")\n",
    "print(f\"Best gradient estimation: {best_gradient}\")\n",
    "print(\"\\nRecommendation: Mix high-progress stratagems with low-cost\")\n",
    "print(\"reconnaissance to maintain accurate gradient estimates.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-27",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "| Concept | Key Insight | Colonel's Siege Example |\n",
    "|---------|-------------|------------------------|\n",
    "| **Derivative** | Measures sensitivity: how output changes per unit input change | How much does progress change when the Colonel adjusts his stratagem? |\n",
    "| **Sign of Derivative** | Tells direction: positive = uphill, negative = downhill | Negative derivative of loss means the stratagem helps |\n",
    "| **Magnitude** | Tells intensity: large = high sensitivity, small = diminishing returns | Early siege: high sensitivity; Late siege: low sensitivity |\n",
    "| **Numerical Derivative** | Approximate by nudging: f'(x) ≈ (f(x+h) - f(x)) / h | Try a variation, observe the result, estimate the gradient |\n",
    "| **Gradient Estimation** | In practice, we estimate gradients with noise | The Colonel can't see the true loss landscape |\n",
    "| **Diminishing Returns** | Sensitivity decreases as you approach the optimum | The Tower becomes harder to crack with each year |\n",
    "\n",
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Derivatives measure sensitivity**: \"If I nudge input, how much does output change?\"\n",
    "\n",
    "2. **The magnitude tells you how sensitive**: Large derivative = small input changes cause big output changes\n",
    "\n",
    "3. **The sign tells you the direction**: Negative derivative of loss = more effort helps!\n",
    "\n",
    "4. **In ML, we use this to improve models**: The derivative of loss with respect to parameters tells us how to adjust them\n",
    "\n",
    "5. **The Colonel's challenge is our challenge**: He can't see the true loss landscape, only noisy estimates—just like stochastic gradient descent.\n",
    "\n",
    "---\n",
    "\n",
    "## Next Lesson\n",
    "\n",
    "In **Lesson 2: The Gradient — Your Compass**, we'll extend this idea to functions with multiple inputs. The Colonel must choose between multiple stratagems simultaneously—he needs a whole *vector* of sensitivities. That's the gradient, his compass in the fog of war.\n",
    "\n",
    "*\"One derivative tells me how hard to push. But I have many levers to pull—ladders, tunnels, grapples, siege engines. I need a compass that points in n dimensions. I need the gradient.\"*  \n",
    "— The Colonel"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
