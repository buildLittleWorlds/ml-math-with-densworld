{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/buildLittleWorlds/ml-math-with-densworld/blob/main/modules/03-calculus/notebooks/02-gradient-compass.ipynb)\n",
    "\n",
    "# Lesson 2: The Gradient — Your Compass in the Fog\n",
    "\n",
    "*\"I have many levers to pull—ladders, tunnels, grapples, siege engines, the morale of my men, the timing of supplies. Each affects the outcome differently. When I change one, the others do not stand still. I need not a single direction, but a compass that points in n dimensions. I need the gradient.\"*  \n",
    "— The Colonel, strategic notes, Year 17 of the Siege\n",
    "\n",
    "---\n",
    "\n",
    "## The Core Insight\n",
    "\n",
    "In Lesson 1, we learned that a derivative measures sensitivity: how much the output changes when we nudge one input.\n",
    "\n",
    "But what if we have **multiple inputs**?\n",
    "\n",
    "The Colonel doesn't just choose \"more effort\" or \"less effort.\" He chooses:\n",
    "- How many personnel to commit\n",
    "- Which stratagem to use\n",
    "- How many supplies to expend\n",
    "- What risk level to accept\n",
    "\n",
    "Each input has its own sensitivity. The **gradient** is simply the collection of all these sensitivities—a vector that tells you the direction of steepest ascent (or descent).\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this lesson, you will:\n",
    "1. Understand partial derivatives as \"hold everything else constant\"\n",
    "2. See the gradient as a vector of partial derivatives\n",
    "3. Interpret the gradient as a compass pointing uphill\n",
    "4. Calculate gradients numerically and interpret their meaning\n",
    "5. Connect gradients to the Colonel's multi-dimensional optimization problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Nice plotting defaults\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "# Colab-ready data loading\n",
    "BASE_URL = \"https://raw.githubusercontent.com/buildLittleWorlds/ml-math-with-densworld/main/data/\"\n",
    "\n",
    "# Load the siege data\n",
    "siege = pd.read_csv(BASE_URL + \"siege_progress.csv\")\n",
    "stratagem = pd.read_csv(BASE_URL + \"stratagem_details.csv\")\n",
    "\n",
    "print(f\"Loaded {len(siege)} months of siege records\")\n",
    "print(f\"Loaded {len(stratagem)} individual stratagem attempts\")\n",
    "print(f\"\\nStratagem columns: {stratagem.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## The Foggy Mountain Analogy\n",
    "\n",
    "*\"Imagine you are lost on a mountain in dense fog. You cannot see the summit. You cannot see the valley. All you can do is feel the slope beneath your feet. Which way is down?\"*  \n",
    "— Vagabu Olt, teaching optimization to young cartographers\n",
    "\n",
    "The gradient tells you:\n",
    "- **The direction of steepest ascent** (uphill)\n",
    "- **The magnitude of the slope** (how steep)\n",
    "\n",
    "To go downhill (minimize loss), you walk in the **opposite direction** of the gradient.\n",
    "\n",
    "---\n",
    "\n",
    "## Part 1: From One Variable to Many\n",
    "\n",
    "### Single Variable (Review)\n",
    "\n",
    "For a function of one variable, the derivative tells us the sensitivity:\n",
    "\n",
    "$$f'(x) = \\frac{df}{dx}$$\n",
    "\n",
    "### Multiple Variables (New)\n",
    "\n",
    "For a function of many variables, we have **partial derivatives**—one for each input:\n",
    "\n",
    "$$\\frac{\\partial f}{\\partial x_1}, \\frac{\\partial f}{\\partial x_2}, \\ldots, \\frac{\\partial f}{\\partial x_n}$$\n",
    "\n",
    "Each partial derivative asks: \"If I nudge *this one input* while holding all others constant, how does the output change?\"\n",
    "\n",
    "The **gradient** is the vector of all partial derivatives:\n",
    "\n",
    "$$\\nabla f = \\left[ \\frac{\\partial f}{\\partial x_1}, \\frac{\\partial f}{\\partial x_2}, \\ldots, \\frac{\\partial f}{\\partial x_n} \\right]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: A simple 2D loss function\n",
    "# Loss = (personnel - optimal_personnel)^2 + (supplies - optimal_supplies)^2\n",
    "\n",
    "def loss_function_2d(personnel, supplies, optimal_p=50, optimal_s=30):\n",
    "    \"\"\"A simple quadratic loss function in 2D.\n",
    "    Minimum is at (optimal_p, optimal_s).\n",
    "    \"\"\"\n",
    "    return (personnel - optimal_p)**2 + (supplies - optimal_s)**2\n",
    "\n",
    "# Partial derivatives (analytically)\n",
    "# ∂L/∂personnel = 2 * (personnel - optimal_p)\n",
    "# ∂L/∂supplies = 2 * (supplies - optimal_s)\n",
    "\n",
    "def gradient_2d(personnel, supplies, optimal_p=50, optimal_s=30):\n",
    "    \"\"\"Gradient of the 2D loss function.\"\"\"\n",
    "    dL_dp = 2 * (personnel - optimal_p)\n",
    "    dL_ds = 2 * (supplies - optimal_s)\n",
    "    return np.array([dL_dp, dL_ds])\n",
    "\n",
    "# Test at a few points\n",
    "test_points = [(20, 10), (50, 30), (80, 50), (30, 60)]\n",
    "\n",
    "print(\"Gradient at Various Points:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Point (P, S)':>15} | {'Loss':>10} | {'Gradient':>20} | Interpretation\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for p, s in test_points:\n",
    "    loss = loss_function_2d(p, s)\n",
    "    grad = gradient_2d(p, s)\n",
    "    if np.allclose(grad, 0):\n",
    "        interp = \"At minimum!\"\n",
    "    else:\n",
    "        interp = f\"Move toward ({50 - np.sign(grad[0]):.0f}P, {30 - np.sign(grad[1]):.0f}S)\"\n",
    "    print(f\"({p:>3}, {s:>3})       | {loss:>10.1f} | [{grad[0]:>8.1f}, {grad[1]:>8.1f}] | {interp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Part 2: Visualizing the Gradient\n",
    "\n",
    "Let's visualize the loss landscape and the gradient field. The gradient vectors point in the direction of steepest **ascent** (uphill). To minimize loss, we go in the **opposite** direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a meshgrid for visualization\n",
    "p_range = np.linspace(0, 100, 50)\n",
    "s_range = np.linspace(0, 60, 50)\n",
    "P, S = np.meshgrid(p_range, s_range)\n",
    "Z = loss_function_2d(P, S)\n",
    "\n",
    "# Create figure with two subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Left: Contour plot with gradient vectors\n",
    "ax1 = axes[0]\n",
    "contour = ax1.contour(P, S, Z, levels=20, cmap='viridis')\n",
    "ax1.clabel(contour, inline=True, fontsize=8)\n",
    "\n",
    "# Add gradient vectors at sample points\n",
    "p_arrows = np.linspace(10, 90, 7)\n",
    "s_arrows = np.linspace(5, 55, 6)\n",
    "for p in p_arrows:\n",
    "    for s in s_arrows:\n",
    "        grad = gradient_2d(p, s)\n",
    "        # Normalize for visualization\n",
    "        magnitude = np.sqrt(grad[0]**2 + grad[1]**2)\n",
    "        if magnitude > 0:\n",
    "            grad_norm = grad / magnitude * 5  # Scale for visibility\n",
    "            # Gradient points uphill; we show it in red\n",
    "            ax1.arrow(p, s, grad_norm[0], grad_norm[1], \n",
    "                     head_width=2, head_length=1, fc='red', ec='red', alpha=0.7)\n",
    "\n",
    "# Mark the minimum\n",
    "ax1.plot(50, 30, 'g*', markersize=20, label='Minimum (50, 30)')\n",
    "ax1.set_xlabel('Personnel Committed', fontsize=11)\n",
    "ax1.set_ylabel('Supply Level', fontsize=11)\n",
    "ax1.set_title('Loss Landscape with Gradient Vectors\\n(Red arrows point UPHILL)', fontsize=12)\n",
    "ax1.legend()\n",
    "\n",
    "# Right: 3D surface\n",
    "ax2 = fig.add_subplot(122, projection='3d')\n",
    "surf = ax2.plot_surface(P, S, Z, cmap='viridis', alpha=0.8, edgecolor='none')\n",
    "ax2.scatter([50], [30], [0], color='green', s=100, marker='*', label='Minimum')\n",
    "ax2.set_xlabel('Personnel')\n",
    "ax2.set_ylabel('Supplies')\n",
    "ax2.set_zlabel('Loss')\n",
    "ax2.set_title('The Colonel\\'s Loss Landscape\\n(He wants to reach the bottom)', fontsize=12)\n",
    "\n",
    "# Remove the duplicate subplot\n",
    "axes[1].remove()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"The gradient vectors (red arrows) point UPHILL.\")\n",
    "print(\"To minimize loss, walk in the OPPOSITE direction (toward the green star).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## Part 3: Numerical Gradients\n",
    "\n",
    "Just like we computed numerical derivatives in Lesson 1, we can compute **numerical gradients** by nudging each input one at a time:\n",
    "\n",
    "$$\\frac{\\partial f}{\\partial x_i} \\approx \\frac{f(x_1, \\ldots, x_i + h, \\ldots, x_n) - f(x_1, \\ldots, x_i, \\ldots, x_n)}{h}$$\n",
    "\n",
    "This is how neural networks compute gradients in practice (though with the more efficient backpropagation algorithm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_gradient(f, x, h=1e-5):\n",
    "    \"\"\"\n",
    "    Compute the numerical gradient of f at point x.\n",
    "    \n",
    "    Parameters:\n",
    "    - f: function that takes a numpy array and returns a scalar\n",
    "    - x: numpy array, the point at which to compute the gradient\n",
    "    - h: step size for finite differences\n",
    "    \n",
    "    Returns:\n",
    "    - gradient: numpy array of partial derivatives\n",
    "    \"\"\"\n",
    "    x = np.array(x, dtype=float)\n",
    "    gradient = np.zeros_like(x)\n",
    "    \n",
    "    for i in range(len(x)):\n",
    "        # Perturb the i-th component\n",
    "        x_plus = x.copy()\n",
    "        x_plus[i] += h\n",
    "        \n",
    "        x_minus = x.copy()\n",
    "        x_minus[i] -= h\n",
    "        \n",
    "        # Centered difference for better accuracy\n",
    "        gradient[i] = (f(x_plus) - f(x_minus)) / (2 * h)\n",
    "    \n",
    "    return gradient\n",
    "\n",
    "# Test our numerical gradient\n",
    "def loss_wrapper(x):\n",
    "    return loss_function_2d(x[0], x[1])\n",
    "\n",
    "test_points = [(20, 10), (50, 30), (80, 50)]\n",
    "\n",
    "print(\"Comparing Analytical vs Numerical Gradients:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Point':>15} | {'Analytical':>20} | {'Numerical':>20} | Match?\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for p, s in test_points:\n",
    "    analytical = gradient_2d(p, s)\n",
    "    numerical = numerical_gradient(loss_wrapper, np.array([p, s]))\n",
    "    match = \"Yes\" if np.allclose(analytical, numerical) else \"No\"\n",
    "    print(f\"({p:>3}, {s:>3})       | [{analytical[0]:>7.2f}, {analytical[1]:>7.2f}] | [{numerical[0]:>7.2f}, {numerical[1]:>7.2f}] | {match}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## Part 4: The Colonel's Multi-Dimensional Problem\n",
    "\n",
    "The Colonel's siege is not a simple 2D problem. Each stratagem involves multiple factors:\n",
    "- Personnel committed\n",
    "- Supply cost\n",
    "- Risk level\n",
    "- His confidence in the approach\n",
    "\n",
    "Let's examine how these factors relate to progress and build a gradient intuition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the relationships in the stratagem data\n",
    "print(\"Stratagem Features and Their Relationships to Progress:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "features = ['personnel_committed', 'supply_cost', 'risk_level', 'colonel_confidence']\n",
    "\n",
    "for feature in features:\n",
    "    correlation = stratagem[feature].corr(stratagem['progress_delta'])\n",
    "    print(f\"{feature:25} → progress_delta correlation: {correlation:>7.3f}\")\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"- Positive correlation: increasing this tends to increase progress\")\n",
    "print(\"- Negative correlation: increasing this tends to decrease progress\")\n",
    "print(\"- Near-zero correlation: weak or nonlinear relationship\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the relationships\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "for ax, feature in zip(axes.flat, features):\n",
    "    # Color by outcome\n",
    "    colors = {'success': 'green', 'partial': 'orange', 'failure': 'gray', 'disaster': 'red'}\n",
    "    for outcome in colors:\n",
    "        mask = stratagem['outcome_category'] == outcome\n",
    "        ax.scatter(stratagem.loc[mask, feature], \n",
    "                  stratagem.loc[mask, 'progress_delta'],\n",
    "                  c=colors[outcome], label=outcome, alpha=0.6, s=40)\n",
    "    \n",
    "    ax.axhline(0, color='black', linestyle='--', linewidth=0.5)\n",
    "    ax.set_xlabel(feature.replace('_', ' ').title(), fontsize=10)\n",
    "    ax.set_ylabel('Progress Delta', fontsize=10)\n",
    "    \n",
    "    # Add correlation in title\n",
    "    corr = stratagem[feature].corr(stratagem['progress_delta'])\n",
    "    ax.set_title(f'{feature.replace(\"_\", \" \").title()}\\n(correlation: {corr:.3f})', fontsize=11)\n",
    "\n",
    "axes[0, 0].legend(loc='upper right', fontsize=8)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## Part 5: Building a Gradient From Data\n",
    "\n",
    "Let's fit a simple linear model to the stratagem data and extract the coefficients—these are essentially the partial derivatives (gradients) telling us how each factor affects progress.\n",
    "\n",
    "*\"Each factor pulls the outcome in its own direction. The gradient is the sum of all these pulls, pointing toward the optimal configuration.\"*  \n",
    "— The Colonel, reflecting on decades of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Prepare features\n",
    "features = ['personnel_committed', 'supply_cost', 'risk_level', 'colonel_confidence', 'step_size']\n",
    "X = stratagem[features].values\n",
    "y = stratagem['progress_delta'].values\n",
    "\n",
    "# Standardize for comparable coefficients\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Fit linear model\n",
    "model = LinearRegression()\n",
    "model.fit(X_scaled, y)\n",
    "\n",
    "print(\"Linear Model Coefficients (Standardized):\")\n",
    "print(\"These represent the 'gradient' of progress with respect to each feature.\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "coef_df = pd.DataFrame({\n",
    "    'Feature': features,\n",
    "    'Coefficient': model.coef_,\n",
    "    'Abs Value': np.abs(model.coef_)\n",
    "}).sort_values('Abs Value', ascending=False)\n",
    "\n",
    "print(coef_df.to_string(index=False))\n",
    "print(f\"\\nR² Score: {model.score(X_scaled, y):.4f}\")\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"- Positive coefficient: increasing this feature increases progress\")\n",
    "print(\"- Negative coefficient: increasing this feature decreases progress\")\n",
    "print(\"- Larger absolute value: stronger effect\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the gradient (coefficients)\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "colors = ['green' if c > 0 else 'red' for c in model.coef_]\n",
    "bars = ax.barh(features, model.coef_, color=colors, alpha=0.7, edgecolor='black')\n",
    "ax.axvline(0, color='black', linewidth=1)\n",
    "\n",
    "ax.set_xlabel('Gradient Component (Effect on Progress)', fontsize=11)\n",
    "ax.set_ylabel('Feature', fontsize=11)\n",
    "ax.set_title('The Gradient of the Colonel\\'s Optimization Problem\\n(What should he adjust to maximize progress?)', fontsize=12)\n",
    "\n",
    "# Add value labels\n",
    "for bar, coef in zip(bars, model.coef_):\n",
    "    width = bar.get_width()\n",
    "    ax.annotate(f'{coef:.4f}',\n",
    "                xy=(width, bar.get_y() + bar.get_height()/2),\n",
    "                xytext=(5 if width > 0 else -5, 0),\n",
    "                textcoords=\"offset points\",\n",
    "                ha='left' if width > 0 else 'right',\n",
    "                va='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"This bar chart IS the gradient (in the linear approximation).\")\n",
    "print(\"It tells the Colonel which levers to pull and in which direction.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## Part 6: The Gradient as a Compass\n",
    "\n",
    "The gradient gives us a direction. But how do we use it?\n",
    "\n",
    "**Key insight**: The gradient points in the direction of **steepest ascent** of the function. If we want to **minimize** loss (which we do!), we should move in the **opposite direction**.\n",
    "\n",
    "$$\\text{new position} = \\text{old position} - \\alpha \\cdot \\nabla f$$\n",
    "\n",
    "where $\\alpha$ is the **learning rate** (step size).\n",
    "\n",
    "*\"The compass points to higher ground. But I seek the valley—the place where loss is minimized. So I walk against the needle.\"*  \n",
    "— The Colonel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate gradient descent on the 2D loss function\n",
    "def gradient_descent_2d(start, learning_rate=0.1, num_steps=20):\n",
    "    \"\"\"Perform gradient descent on our 2D loss function.\"\"\"\n",
    "    path = [start]\n",
    "    current = np.array(start, dtype=float)\n",
    "    \n",
    "    for _ in range(num_steps):\n",
    "        grad = gradient_2d(current[0], current[1])\n",
    "        current = current - learning_rate * grad  # Move against the gradient\n",
    "        path.append(current.copy())\n",
    "    \n",
    "    return np.array(path)\n",
    "\n",
    "# Run from different starting points\n",
    "starts = [(10, 10), (90, 50), (30, 55), (80, 5)]\n",
    "colors = ['blue', 'red', 'green', 'purple']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "# Plot contours\n",
    "contour = ax.contour(P, S, Z, levels=20, cmap='gray', alpha=0.5)\n",
    "\n",
    "# Plot paths from each starting point\n",
    "for start, color in zip(starts, colors):\n",
    "    path = gradient_descent_2d(start, learning_rate=0.05, num_steps=30)\n",
    "    ax.plot(path[:, 0], path[:, 1], 'o-', color=color, markersize=4, \n",
    "            linewidth=1.5, label=f'Start: {start}')\n",
    "    ax.plot(path[0, 0], path[0, 1], 's', color=color, markersize=10)  # Start\n",
    "    ax.plot(path[-1, 0], path[-1, 1], '*', color=color, markersize=15)  # End\n",
    "\n",
    "# Mark the true minimum\n",
    "ax.plot(50, 30, 'k*', markersize=25, label='Minimum (50, 30)')\n",
    "\n",
    "ax.set_xlabel('Personnel Committed', fontsize=11)\n",
    "ax.set_ylabel('Supply Level', fontsize=11)\n",
    "ax.set_title('Gradient Descent in Action\\n(Following the compass downhill)', fontsize=12)\n",
    "ax.legend(loc='upper right')\n",
    "ax.set_xlim(0, 100)\n",
    "ax.set_ylim(0, 60)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"From any starting point, following the negative gradient leads to the minimum.\")\n",
    "print(\"This is gradient descent—the core algorithm of machine learning optimization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## Part 7: Gradient Magnitude — How Steep Is the Slope?\n",
    "\n",
    "The gradient is a vector. Its **magnitude** (length) tells us how steep the terrain is:\n",
    "\n",
    "$$\\|\\nabla f\\| = \\sqrt{\\left(\\frac{\\partial f}{\\partial x_1}\\right)^2 + \\left(\\frac{\\partial f}{\\partial x_2}\\right)^2 + \\cdots}$$\n",
    "\n",
    "- **Large magnitude**: We're on a steep slope; small steps cause big changes\n",
    "- **Small magnitude**: We're on a gentle slope; we might be near a minimum\n",
    "- **Zero magnitude**: We're at a critical point (minimum, maximum, or saddle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute gradient magnitude across the landscape\n",
    "gradient_magnitude = np.zeros_like(Z)\n",
    "\n",
    "for i in range(len(p_range)):\n",
    "    for j in range(len(s_range)):\n",
    "        grad = gradient_2d(p_range[i], s_range[j])\n",
    "        gradient_magnitude[j, i] = np.sqrt(grad[0]**2 + grad[1]**2)\n",
    "\n",
    "# Plot gradient magnitude\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: Gradient magnitude heatmap\n",
    "im = axes[0].imshow(gradient_magnitude, extent=[0, 100, 0, 60], \n",
    "                     origin='lower', cmap='hot', aspect='auto')\n",
    "axes[0].plot(50, 30, 'g*', markersize=20)\n",
    "axes[0].set_xlabel('Personnel Committed', fontsize=11)\n",
    "axes[0].set_ylabel('Supply Level', fontsize=11)\n",
    "axes[0].set_title('Gradient Magnitude\\n(Brighter = Steeper Slope)', fontsize=12)\n",
    "plt.colorbar(im, ax=axes[0], label='|∇f|')\n",
    "\n",
    "# Right: Cross-section showing gradient magnitude along a path\n",
    "path_points = np.linspace(0, 100, 100)\n",
    "grad_along_path = [np.sqrt(np.sum(gradient_2d(p, 30)**2)) for p in path_points]\n",
    "\n",
    "axes[1].plot(path_points, grad_along_path, 'b-', linewidth=2)\n",
    "axes[1].axvline(50, color='green', linestyle='--', label='Minimum')\n",
    "axes[1].set_xlabel('Personnel Committed (at Supply = 30)', fontsize=11)\n",
    "axes[1].set_ylabel('Gradient Magnitude', fontsize=11)\n",
    "axes[1].set_title('Gradient Magnitude Along a Slice\\n(Zero at the minimum!)', fontsize=12)\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"The gradient magnitude is zero at the minimum (50, 30).\")\n",
    "print(\"This is how we know we've found an optimum!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercises\n",
    "\n",
    "### Exercise 1: Computing Gradients by Hand\n",
    "\n",
    "For the function $f(x, y) = x^2 + 2xy + 3y^2$:\n",
    "\n",
    "1. Compute the partial derivatives $\\frac{\\partial f}{\\partial x}$ and $\\frac{\\partial f}{\\partial y}$\n",
    "2. Write the gradient $\\nabla f$\n",
    "3. Evaluate the gradient at point (1, 2)\n",
    "4. Which direction should you move to decrease f?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Computing gradients\n",
    "\n",
    "def f(x, y):\n",
    "    return x**2 + 2*x*y + 3*y**2\n",
    "\n",
    "def gradient_f(x, y):\n",
    "    # df/dx = 2x + 2y\n",
    "    # df/dy = 2x + 6y\n",
    "    df_dx = 2*x + 2*y\n",
    "    df_dy = 2*x + 6*y\n",
    "    return np.array([df_dx, df_dy])\n",
    "\n",
    "# Test at (1, 2)\n",
    "point = (1, 2)\n",
    "grad = gradient_f(*point)\n",
    "\n",
    "print(\"Exercise 1 Solution:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"∂f/∂x = 2x + 2y\")\n",
    "print(f\"∂f/∂y = 2x + 6y\")\n",
    "print(f\"\\n∇f = [2x + 2y, 2x + 6y]\")\n",
    "print(f\"\\nAt point (1, 2):\")\n",
    "print(f\"  ∇f(1, 2) = [{grad[0]}, {grad[1]}]\")\n",
    "print(f\"\\nTo decrease f, move in direction: [{-grad[0]}, {-grad[1]}]\")\n",
    "\n",
    "# Verify numerically\n",
    "numerical = numerical_gradient(lambda x: f(x[0], x[1]), np.array([1.0, 2.0]))\n",
    "print(f\"\\nNumerical verification: [{numerical[0]:.4f}, {numerical[1]:.4f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "### Exercise 2: The Colonel's Dilemma\n",
    "\n",
    "Using the stratagem data, analyze which combination of factors leads to the highest gradient toward success. If the Colonel could only improve two factors, which should he prioritize?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Prioritizing factors\n",
    "\n",
    "# From our earlier linear model, the coefficients tell us the gradient\n",
    "print(\"Factor Priorities Based on Gradient Analysis:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Sort by absolute coefficient value\n",
    "priorities = sorted(zip(features, model.coef_), key=lambda x: abs(x[1]), reverse=True)\n",
    "\n",
    "for i, (feature, coef) in enumerate(priorities, 1):\n",
    "    direction = \"INCREASE\" if coef > 0 else \"DECREASE\"\n",
    "    print(f\"{i}. {feature:25} (coef: {coef:>8.4f}) → {direction}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RECOMMENDATION:\")\n",
    "print(f\"The Colonel should focus on:\")\n",
    "print(f\"  1. {priorities[0][0]} (strongest effect)\")\n",
    "print(f\"  2. {priorities[1][0]} (second strongest effect)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "### Exercise 3: Non-Convex Landscapes\n",
    "\n",
    "Real optimization problems often have multiple minima. Create a loss function with two minima and show how gradient descent might get stuck in a local minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Non-convex landscape\n",
    "\n",
    "def multi_modal_loss(x):\n",
    "    \"\"\"A loss function with multiple minima.\"\"\"\n",
    "    return np.sin(x) + 0.1 * x**2 - 2\n",
    "\n",
    "def multi_modal_gradient(x):\n",
    "    return np.cos(x) + 0.2 * x\n",
    "\n",
    "# Visualize\n",
    "x_range = np.linspace(-6, 6, 200)\n",
    "y_values = [multi_modal_loss(x) for x in x_range]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "ax.plot(x_range, y_values, 'b-', linewidth=2, label='Loss function')\n",
    "\n",
    "# Run gradient descent from different starting points\n",
    "starts = [-5, -2, 0, 3]\n",
    "colors = ['red', 'green', 'orange', 'purple']\n",
    "\n",
    "for start, color in zip(starts, colors):\n",
    "    x = start\n",
    "    path = [x]\n",
    "    for _ in range(50):\n",
    "        grad = multi_modal_gradient(x)\n",
    "        x = x - 0.1 * grad\n",
    "        path.append(x)\n",
    "    \n",
    "    path = np.array(path)\n",
    "    y_path = [multi_modal_loss(p) for p in path]\n",
    "    ax.plot(path, y_path, 'o-', color=color, markersize=3, alpha=0.7, label=f'Start: {start}')\n",
    "    ax.plot(path[0], y_path[0], 's', color=color, markersize=10)\n",
    "    ax.plot(path[-1], y_path[-1], '*', color=color, markersize=15)\n",
    "\n",
    "ax.set_xlabel('x', fontsize=11)\n",
    "ax.set_ylabel('Loss', fontsize=11)\n",
    "ax.set_title('Non-Convex Loss Landscape\\n(Different starting points lead to different minima!)', fontsize=12)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Starting from x=-5 leads to the leftmost minimum.\")\n",
    "print(\"Starting from x=3 leads to a different minimum.\")\n",
    "print(\"\\nThis is why initialization matters in machine learning!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-24",
   "metadata": {},
   "source": [
    "### Exercise 4: Gradient Estimation Quality\n",
    "\n",
    "Analyze how well the Colonel's estimated gradients match the actual gradients in the stratagem data. When are his estimates most accurate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4: Gradient estimation analysis\n",
    "\n",
    "# Group by stratagem type and analyze gradient estimation quality\n",
    "estimation_quality = stratagem.groupby('stratagem_type').agg({\n",
    "    'estimated_gradient': 'mean',\n",
    "    'actual_gradient': 'mean',\n",
    "    'gradient_error': ['mean', 'std'],\n",
    "    'was_optimal_direction': 'mean'\n",
    "}).round(4)\n",
    "\n",
    "estimation_quality.columns = ['est_grad_mean', 'actual_grad_mean', \n",
    "                               'error_mean', 'error_std', 'direction_accuracy']\n",
    "estimation_quality['direction_accuracy'] = (estimation_quality['direction_accuracy'] * 100).round(1)\n",
    "\n",
    "print(\"Gradient Estimation Quality by Stratagem Type:\")\n",
    "print(\"=\" * 80)\n",
    "print(estimation_quality.sort_values('direction_accuracy', ascending=False).to_string())\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINDINGS:\")\n",
    "best = estimation_quality['direction_accuracy'].idxmax()\n",
    "worst = estimation_quality['direction_accuracy'].idxmin()\n",
    "print(f\"Best gradient estimation: {best} ({estimation_quality.loc[best, 'direction_accuracy']}% correct direction)\")\n",
    "print(f\"Worst gradient estimation: {worst} ({estimation_quality.loc[worst, 'direction_accuracy']}% correct direction)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-26",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "| Concept | Key Insight | Colonel's Siege Example |\n",
    "|---------|-------------|------------------------|\n",
    "| **Partial Derivative** | Sensitivity of output to one input (holding others constant) | How much does progress change if we add more personnel? |\n",
    "| **Gradient** | Vector of all partial derivatives | [∂progress/∂personnel, ∂progress/∂supplies, ∂progress/∂risk, ...] |\n",
    "| **Gradient Direction** | Points toward steepest ascent | The combination of adjustments that most increases loss |\n",
    "| **Negative Gradient** | Points toward steepest descent | The direction the Colonel should move to maximize progress |\n",
    "| **Gradient Magnitude** | How steep the slope is | Large = big changes imminent; Small = near optimum |\n",
    "| **Numerical Gradient** | Compute by nudging each input | Try small variations, measure effects |\n",
    "\n",
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **The gradient generalizes the derivative to multiple dimensions**: It's a vector of sensitivities.\n",
    "\n",
    "2. **The gradient points uphill**: To minimize, move in the opposite direction.\n",
    "\n",
    "3. **Each component tells a story**: \"Increase personnel\" vs \"Decrease risk\" are encoded in the gradient.\n",
    "\n",
    "4. **The magnitude tells you how far from optimal**: Near-zero gradient means you're close to a minimum (or maximum, or saddle).\n",
    "\n",
    "5. **Computing gradients is the key bottleneck in ML**: Backpropagation solves this efficiently for neural networks.\n",
    "\n",
    "---\n",
    "\n",
    "## Next Lesson\n",
    "\n",
    "In **Lesson 3: Gradient Descent — Walking Downhill**, we'll put the gradient to work. We'll implement the full gradient descent algorithm, explore learning rates, and watch the Colonel optimize his siege strategy step by step.\n",
    "\n",
    "*\"Now I have my compass. But knowing the direction is not enough—I must also know how far to step. Too timid, and I waste years. Too bold, and I overshoot and regress. The learning rate is the measure of my courage.\"*  \n",
    "— The Colonel"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
