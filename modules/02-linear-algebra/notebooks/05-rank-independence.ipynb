{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/buildLittleWorlds/ml-math-with-densworld/blob/main/modules/02-linear-algebra/notebooks/05-rank-independence.ipynb)\n",
    "\n",
    "# Lesson 5: Rank and Linear Independence\n",
    "\n",
    "*\"The Archives contain ten thousand manuscripts, but only three philosophical schools. Most variation is redundant—knowing the Stone School alignment almost determines the others. The true dimension of the data is far smaller than it appears.\"*  \n",
    "— Archivist's note, Capital Archives\n",
    "\n",
    "---\n",
    "\n",
    "## The Core Question\n",
    "\n",
    "When we add a new feature to our dataset, are we actually adding *new information*? Or is the new feature just a combination of what we already have?\n",
    "\n",
    "Consider the manuscript school alignments:\n",
    "- Stone School alignment\n",
    "- Water School alignment  \n",
    "- Pebble School alignment\n",
    "\n",
    "If these three always sum to 1.0 (a manuscript must align with *some* school), then the third is completely determined by the first two. We have **3 columns but only 2 independent dimensions**.\n",
    "\n",
    "This matters because:\n",
    "- **Multicollinearity** breaks linear regression\n",
    "- **Dimensionality reduction** exploits redundancy\n",
    "- **Feature selection** removes useless variables\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this lesson, you will:\n",
    "1. Understand linear independence and dependence\n",
    "2. Calculate and interpret matrix rank\n",
    "3. Detect multicollinearity in real datasets\n",
    "4. See why low rank causes problems in regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.linalg import matrix_rank, svd\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Nice plotting defaults\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "# Colab-ready data loading\n",
    "BASE_URL = \"https://raw.githubusercontent.com/buildLittleWorlds/ml-math-with-densworld/main/data/\"\n",
    "\n",
    "# Load our datasets\n",
    "creature_vectors = pd.read_csv(BASE_URL + \"creature_vectors.csv\")\n",
    "manuscripts = pd.read_csv(BASE_URL + \"manuscript_features.csv\")\n",
    "expeditions = pd.read_csv(BASE_URL + \"expedition_outcomes.csv\")\n",
    "\n",
    "print(f\"Loaded {len(creature_vectors)} creatures\")\n",
    "print(f\"Loaded {len(manuscripts)} manuscripts\")\n",
    "print(f\"Loaded {len(expeditions)} expedition records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## Part 1: Linear Independence — The Intuition\n",
    "\n",
    "A set of vectors is **linearly independent** if no vector can be written as a combination of the others.\n",
    "\n",
    "**Independent example:**\n",
    "- $\\mathbf{v}_1 = [1, 0]$ (points east)\n",
    "- $\\mathbf{v}_2 = [0, 1]$ (points north)\n",
    "\n",
    "No combination of east can give you north. These span the full 2D plane.\n",
    "\n",
    "**Dependent example:**\n",
    "- $\\mathbf{v}_1 = [1, 0]$ (points east)\n",
    "- $\\mathbf{v}_2 = [2, 0]$ (also points east, just longer)\n",
    "\n",
    "Both point the same direction! $\\mathbf{v}_2 = 2 \\cdot \\mathbf{v}_1$. They only span a 1D line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize linear independence vs dependence\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Independent vectors\n",
    "ax = axes[0]\n",
    "v1 = np.array([1, 0])\n",
    "v2 = np.array([0, 1])\n",
    "\n",
    "ax.arrow(0, 0, v1[0]*0.9, v1[1], head_width=0.05, fc='blue', ec='blue', linewidth=2, label='v₁ = [1, 0]')\n",
    "ax.arrow(0, 0, v2[0], v2[1]*0.9, head_width=0.05, fc='red', ec='red', linewidth=2, label='v₂ = [0, 1]')\n",
    "\n",
    "# Show span (the whole plane)\n",
    "ax.fill([-1.2, 1.2, 1.2, -1.2], [-1.2, -1.2, 1.2, 1.2], alpha=0.1, color='green')\n",
    "ax.text(0.7, 0.7, 'Span = entire 2D plane', fontsize=11, style='italic')\n",
    "\n",
    "ax.set_xlim(-1.2, 1.2)\n",
    "ax.set_ylim(-1.2, 1.2)\n",
    "ax.set_aspect('equal')\n",
    "ax.axhline(0, color='black', linewidth=0.5)\n",
    "ax.axvline(0, color='black', linewidth=0.5)\n",
    "ax.set_title('Linearly INDEPENDENT\\n(Span 2D space)', fontsize=13)\n",
    "ax.legend(loc='lower right')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Dependent vectors\n",
    "ax = axes[1]\n",
    "v1 = np.array([1, 0])\n",
    "v2 = np.array([2, 0])\n",
    "\n",
    "ax.arrow(0, 0, v1[0]*0.9, v1[1], head_width=0.05, fc='blue', ec='blue', linewidth=2, label='v₁ = [1, 0]')\n",
    "ax.arrow(0, 0, v2[0]*0.45, v2[1], head_width=0.05, fc='red', ec='red', linewidth=2, label='v₂ = [2, 0]')\n",
    "\n",
    "# Show span (just the x-axis)\n",
    "ax.axhline(0, color='green', linewidth=4, alpha=0.3)\n",
    "ax.text(0.5, 0.3, 'Span = only x-axis!', fontsize=11, style='italic')\n",
    "ax.text(0.5, -0.3, '(v₂ = 2·v₁)', fontsize=10, color='gray')\n",
    "\n",
    "ax.set_xlim(-0.5, 2.5)\n",
    "ax.set_ylim(-1.2, 1.2)\n",
    "ax.set_aspect('equal')\n",
    "ax.axhline(0, color='black', linewidth=0.5)\n",
    "ax.axvline(0, color='black', linewidth=0.5)\n",
    "ax.set_title('Linearly DEPENDENT\\n(Only span 1D line)', fontsize=13)\n",
    "ax.legend(loc='upper right')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Part 2: Matrix Rank — Counting Independent Dimensions\n",
    "\n",
    "The **rank** of a matrix is the number of linearly independent columns (or rows—they're equal!).\n",
    "\n",
    "- If a 5×5 matrix has rank 5: **full rank** (all columns independent)\n",
    "- If a 5×5 matrix has rank 3: **rank deficient** (only 3 independent columns, 2 are redundant)\n",
    "\n",
    "Rank tells you the \"true dimensionality\" of your data, regardless of how many columns you have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate rank calculation\n",
    "print(\"Matrix Rank Examples:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Full rank matrix (3x3 with rank 3)\n",
    "A_full = np.array([[1, 0, 0],\n",
    "                   [0, 1, 0],\n",
    "                   [0, 0, 1]])\n",
    "print(f\"\\nIdentity Matrix (3×3):\")\n",
    "print(A_full)\n",
    "print(f\"Rank: {matrix_rank(A_full)}  (Full rank: 3 independent columns)\")\n",
    "\n",
    "# Rank-deficient matrix (3x3 with rank 2)\n",
    "A_deficient = np.array([[1, 2, 3],\n",
    "                        [4, 5, 6],\n",
    "                        [7, 8, 9]])\n",
    "print(f\"\\nMagic-like Matrix (3×3):\")\n",
    "print(A_deficient)\n",
    "print(f\"Rank: {matrix_rank(A_deficient)}  (Column 3 = -col1 + 2*col2... deficient!)\")\n",
    "\n",
    "# Obvious dependency\n",
    "A_obvious = np.array([[1, 2],\n",
    "                      [2, 4],\n",
    "                      [3, 6]])\n",
    "print(f\"\\nObvious Dependency (3×2):\")\n",
    "print(A_obvious)\n",
    "print(f\"Rank: {matrix_rank(A_obvious)}  (Column 2 = 2 × Column 1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## Part 3: Manuscript School Alignments — A Real Example\n",
    "\n",
    "In the Archives, manuscripts are rated for alignment to three philosophical schools. But do we really have 3 independent pieces of information?\n",
    "\n",
    "Let's investigate..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract school alignment features\n",
    "school_features = ['school_alignment_stone', 'school_alignment_water', 'school_alignment_pebble']\n",
    "school_matrix = manuscripts[school_features].values\n",
    "\n",
    "print(\"School Alignment Matrix (first 10 manuscripts):\")\n",
    "print(\"=\"*60)\n",
    "print(manuscripts[['manuscript_id'] + school_features].head(10).to_string(index=False))\n",
    "\n",
    "# Check if they sum to a constant\n",
    "row_sums = school_matrix.sum(axis=1)\n",
    "print(f\"\\nRow sums (should be constant if dependent):\")\n",
    "print(f\"  Min: {row_sums.min():.4f}\")\n",
    "print(f\"  Max: {row_sums.max():.4f}\")\n",
    "print(f\"  Mean: {row_sums.mean():.4f}\")\n",
    "print(f\"  Std: {row_sums.std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate rank of school alignment matrix\n",
    "print(\"\\nRank Analysis of School Alignments:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "rank = matrix_rank(school_matrix)\n",
    "print(f\"\\nMatrix shape: {school_matrix.shape}\")\n",
    "print(f\"Matrix rank: {rank}\")\n",
    "\n",
    "if rank < school_matrix.shape[1]:\n",
    "    print(f\"\\n⚠️  RANK DEFICIENT!\")\n",
    "    print(f\"   We have {school_matrix.shape[1]} columns but only {rank} independent dimensions.\")\n",
    "    print(f\"   {school_matrix.shape[1] - rank} column(s) are redundant.\")\n",
    "else:\n",
    "    print(f\"\\n✓ Full rank: all {rank} columns are independent.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the near-dependency\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# 3D scatter - if truly 3D, points fill space; if 2D, they lie on a plane\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "ax = fig.add_subplot(121, projection='3d')\n",
    "ax.scatter(school_matrix[:, 0], school_matrix[:, 1], school_matrix[:, 2], alpha=0.5)\n",
    "ax.set_xlabel('Stone')\n",
    "ax.set_ylabel('Water')\n",
    "ax.set_zlabel('Pebble')\n",
    "ax.set_title('Manuscripts in 3D School Space\\n(Do they fill 3D or lie on a plane?)')\n",
    "\n",
    "# Pairwise correlations\n",
    "ax = axes[1]\n",
    "corr_matrix = np.corrcoef(school_matrix.T)\n",
    "im = ax.imshow(corr_matrix, cmap='RdBu_r', vmin=-1, vmax=1)\n",
    "ax.set_xticks([0, 1, 2])\n",
    "ax.set_yticks([0, 1, 2])\n",
    "ax.set_xticklabels(['Stone', 'Water', 'Pebble'])\n",
    "ax.set_yticklabels(['Stone', 'Water', 'Pebble'])\n",
    "ax.set_title('Correlation Matrix\\n(High |correlation| = dependence)')\n",
    "\n",
    "# Add correlation values\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        ax.text(j, i, f'{corr_matrix[i,j]:.2f}', ha='center', va='center', fontsize=12)\n",
    "\n",
    "plt.colorbar(im, ax=ax, shrink=0.8)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## Part 4: Singular Value Decomposition (SVD) — Finding the True Dimensions\n",
    "\n",
    "The **Singular Value Decomposition** reveals the \"true\" dimensions of your data:\n",
    "\n",
    "$$\\mathbf{A} = \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}^T$$\n",
    "\n",
    "The **singular values** (diagonal of $\\Sigma$) tell you how much \"energy\" is in each dimension:\n",
    "- Large singular values = important, real dimensions\n",
    "- Tiny singular values = noise or redundant dimensions\n",
    "\n",
    "If some singular values are near zero, those dimensions are effectively empty—the data doesn't vary in those directions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform SVD on school alignments\n",
    "U, S, Vt = svd(school_matrix, full_matrices=False)\n",
    "\n",
    "print(\"Singular Value Decomposition:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nSingular values: {S}\")\n",
    "print(f\"\\nExplained variance ratio:\")\n",
    "\n",
    "variance_explained = (S ** 2) / np.sum(S ** 2)\n",
    "cumulative_variance = np.cumsum(variance_explained)\n",
    "\n",
    "for i, (s, var, cum) in enumerate(zip(S, variance_explained, cumulative_variance)):\n",
    "    print(f\"  Dimension {i+1}: singular value = {s:.4f}, variance = {var:.2%}, cumulative = {cum:.2%}\")\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ax = axes[0]\n",
    "ax.bar(range(1, len(S)+1), S, color='steelblue')\n",
    "ax.set_xlabel('Dimension')\n",
    "ax.set_ylabel('Singular Value')\n",
    "ax.set_title('Singular Values\\n(Drop-off indicates effective dimensionality)')\n",
    "ax.set_xticks(range(1, len(S)+1))\n",
    "\n",
    "ax = axes[1]\n",
    "ax.bar(range(1, len(variance_explained)+1), variance_explained, color='steelblue', label='Individual')\n",
    "ax.plot(range(1, len(cumulative_variance)+1), cumulative_variance, 'ro-', label='Cumulative')\n",
    "ax.axhline(0.95, color='green', linestyle='--', label='95% threshold')\n",
    "ax.set_xlabel('Dimension')\n",
    "ax.set_ylabel('Variance Explained')\n",
    "ax.set_title('Variance Explained by Each Dimension')\n",
    "ax.set_xticks(range(1, len(S)+1))\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## Part 5: Multicollinearity — When Regression Breaks\n",
    "\n",
    "If features are linearly dependent (or nearly so), **linear regression fails**. The model can't decide which feature to \"credit\" for the effect.\n",
    "\n",
    "Let's create a scenario where this happens and see the consequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic example: predicting expedition success\n",
    "# with nearly collinear features\n",
    "\n",
    "np.random.seed(42)\n",
    "n = 100\n",
    "\n",
    "# Feature 1: crew_size\n",
    "crew_size = np.random.uniform(10, 50, n)\n",
    "\n",
    "# Feature 2: supplies (highly correlated with crew_size!)\n",
    "supplies = 2 * crew_size + np.random.normal(0, 2, n)\n",
    "\n",
    "# Feature 3: independent feature\n",
    "weather_score = np.random.uniform(0, 1, n)\n",
    "\n",
    "# Target: expedition success (truly depends on crew_size and weather)\n",
    "true_success = 0.5 * crew_size + 20 * weather_score + np.random.normal(0, 5, n)\n",
    "\n",
    "# Combine into matrix\n",
    "X = np.column_stack([crew_size, supplies, weather_score])\n",
    "y = true_success\n",
    "\n",
    "print(\"Synthetic Expedition Data:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Features: crew_size, supplies (≈2×crew_size), weather_score\")\n",
    "print(f\"True relationship: success = 0.5×crew + 20×weather + noise\")\n",
    "print(f\"\")\n",
    "print(f\"Correlation matrix:\")\n",
    "corr = np.corrcoef(X.T)\n",
    "print(f\"                crew_size  supplies  weather\")\n",
    "print(f\"  crew_size     {corr[0,0]:.3f}      {corr[0,1]:.3f}     {corr[0,2]:.3f}\")\n",
    "print(f\"  supplies      {corr[1,0]:.3f}      {corr[1,1]:.3f}     {corr[1,2]:.3f}\")\n",
    "print(f\"  weather       {corr[2,0]:.3f}      {corr[2,1]:.3f}     {corr[2,2]:.3f}\")\n",
    "print(f\"\")\n",
    "print(f\"⚠️  crew_size and supplies have correlation {corr[0,1]:.3f}!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit linear regression and examine coefficients\n",
    "from numpy.linalg import lstsq, cond\n",
    "\n",
    "# Add intercept\n",
    "X_with_intercept = np.column_stack([np.ones(n), X])\n",
    "\n",
    "# Fit model\n",
    "coefficients, residuals, rank, singular_values = lstsq(X_with_intercept, y, rcond=None)\n",
    "\n",
    "print(\"Linear Regression with Collinear Features:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nFitted coefficients:\")\n",
    "print(f\"  Intercept:    {coefficients[0]:>10.4f}\")\n",
    "print(f\"  crew_size:    {coefficients[1]:>10.4f}  (true: 0.5)\")\n",
    "print(f\"  supplies:     {coefficients[2]:>10.4f}  (true: 0.0)\")\n",
    "print(f\"  weather:      {coefficients[3]:>10.4f}  (true: 20.0)\")\n",
    "\n",
    "print(f\"\\nMatrix condition number: {cond(X_with_intercept):.2f}\")\n",
    "print(\"  (High condition number = unstable coefficients)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show coefficient instability: small changes in data → big changes in coefficients\n",
    "print(\"Coefficient Instability Test:\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nFitting on different random subsets of data...\\n\")\n",
    "\n",
    "bootstrap_coeffs = []\n",
    "for i in range(10):\n",
    "    # Random subsample\n",
    "    idx = np.random.choice(n, size=n//2, replace=False)\n",
    "    X_sub = X_with_intercept[idx]\n",
    "    y_sub = y[idx]\n",
    "    \n",
    "    coeffs, _, _, _ = lstsq(X_sub, y_sub, rcond=None)\n",
    "    bootstrap_coeffs.append(coeffs)\n",
    "    print(f\"  Sample {i+1}: crew_size={coeffs[1]:>7.3f}, supplies={coeffs[2]:>7.3f}, weather={coeffs[3]:>7.3f}\")\n",
    "\n",
    "bootstrap_coeffs = np.array(bootstrap_coeffs)\n",
    "print(f\"\\nCoefficient standard deviations:\")\n",
    "print(f\"  crew_size:  {bootstrap_coeffs[:,1].std():.4f}\")\n",
    "print(f\"  supplies:   {bootstrap_coeffs[:,2].std():.4f}\")\n",
    "print(f\"  weather:    {bootstrap_coeffs[:,3].std():.4f}\")\n",
    "print(f\"\\n⚠️  crew_size and supplies swap their weights wildly!\")\n",
    "print(f\"   The model can't tell which one matters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## Part 6: Detecting Collinearity in Creature Data\n",
    "\n",
    "Let's check if our creature behavioral features have any hidden dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze creature behavioral features\n",
    "behavioral_features = ['aggression', 'sociality', 'nocturnality', 'territoriality', 'hunting_strategy']\n",
    "X_creatures = creature_vectors[behavioral_features].values\n",
    "\n",
    "print(\"Creature Behavioral Features — Collinearity Check:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Matrix rank\n",
    "print(f\"\\nMatrix shape: {X_creatures.shape}\")\n",
    "print(f\"Matrix rank: {matrix_rank(X_creatures)}\")\n",
    "\n",
    "# Correlation matrix\n",
    "print(f\"\\nCorrelation matrix:\")\n",
    "corr = np.corrcoef(X_creatures.T)\n",
    "print(f\"{'':>15}\", end='')\n",
    "for f in behavioral_features:\n",
    "    print(f\"{f[:8]:>10}\", end='')\n",
    "print()\n",
    "for i, f in enumerate(behavioral_features):\n",
    "    print(f\"{f:>15}\", end='')\n",
    "    for j in range(len(behavioral_features)):\n",
    "        print(f\"{corr[i,j]:>10.2f}\", end='')\n",
    "    print()\n",
    "\n",
    "# SVD to find effective dimensionality\n",
    "_, S, _ = svd(X_creatures - X_creatures.mean(axis=0), full_matrices=False)  # Center first\n",
    "variance_explained = (S ** 2) / np.sum(S ** 2)\n",
    "cumulative = np.cumsum(variance_explained)\n",
    "\n",
    "print(f\"\\nSingular value analysis:\")\n",
    "for i, (s, var, cum) in enumerate(zip(S, variance_explained, cumulative)):\n",
    "    print(f\"  Dim {i+1}: σ={s:.3f}, var={var:.1%}, cumulative={cum:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize correlation heatmap\n",
    "fig, ax = plt.subplots(figsize=(9, 7))\n",
    "\n",
    "im = ax.imshow(corr, cmap='RdBu_r', vmin=-1, vmax=1)\n",
    "ax.set_xticks(range(len(behavioral_features)))\n",
    "ax.set_yticks(range(len(behavioral_features)))\n",
    "ax.set_xticklabels([f.replace('_', '\\n') for f in behavioral_features], fontsize=10)\n",
    "ax.set_yticklabels(behavioral_features, fontsize=10)\n",
    "\n",
    "# Add correlation values\n",
    "for i in range(len(behavioral_features)):\n",
    "    for j in range(len(behavioral_features)):\n",
    "        color = 'white' if abs(corr[i,j]) > 0.5 else 'black'\n",
    "        ax.text(j, i, f'{corr[i,j]:.2f}', ha='center', va='center', fontsize=11, color=color)\n",
    "\n",
    "ax.set_title('Creature Behavioral Feature Correlations', fontsize=13)\n",
    "plt.colorbar(im, ax=ax, shrink=0.8)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"- Aggression and territoriality are positively correlated (0.50)\")\n",
    "print(\"- Sociality is negatively correlated with aggression (-0.45)\")\n",
    "print(\"- These correlations are meaningful but not perfect collinearity\")\n",
    "print(\"- Full rank of 5 means all features contribute unique information\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "## Part 7: Variance Inflation Factor (VIF) — The Standard Tool\n",
    "\n",
    "The **Variance Inflation Factor** measures how much a feature's regression coefficient variance is \"inflated\" due to collinearity with other features.\n",
    "\n",
    "- VIF = 1: No collinearity\n",
    "- VIF = 5: Moderate (coefficient variance 5× higher than if independent)\n",
    "- VIF > 10: Severe collinearity problem!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_vif(X):\n",
    "    \"\"\"Calculate Variance Inflation Factor for each feature.\"\"\"\n",
    "    vifs = []\n",
    "    for i in range(X.shape[1]):\n",
    "        # Regress feature i on all other features\n",
    "        y_i = X[:, i]\n",
    "        X_others = np.delete(X, i, axis=1)\n",
    "        X_others = np.column_stack([np.ones(len(X_others)), X_others])  # Add intercept\n",
    "        \n",
    "        # Get predictions\n",
    "        coeffs, _, _, _ = np.linalg.lstsq(X_others, y_i, rcond=None)\n",
    "        y_pred = X_others @ coeffs\n",
    "        \n",
    "        # Calculate R²\n",
    "        ss_res = np.sum((y_i - y_pred) ** 2)\n",
    "        ss_tot = np.sum((y_i - y_i.mean()) ** 2)\n",
    "        r_squared = 1 - ss_res / ss_tot if ss_tot > 0 else 0\n",
    "        \n",
    "        # VIF = 1 / (1 - R²)\n",
    "        vif = 1 / (1 - r_squared) if r_squared < 1 else np.inf\n",
    "        vifs.append(vif)\n",
    "    \n",
    "    return np.array(vifs)\n",
    "\n",
    "# Calculate VIF for creature behavioral features\n",
    "vifs = calculate_vif(X_creatures)\n",
    "\n",
    "print(\"Variance Inflation Factors (VIF) for Creature Features:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\n{'Feature':<20} {'VIF':>10} {'Interpretation':>25}\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "for feature, vif in zip(behavioral_features, vifs):\n",
    "    if vif < 2:\n",
    "        interp = \"Low collinearity\"\n",
    "    elif vif < 5:\n",
    "        interp = \"Moderate\"\n",
    "    elif vif < 10:\n",
    "        interp = \"High - investigate\"\n",
    "    else:\n",
    "        interp = \"⚠️ SEVERE\"\n",
    "    print(f\"{feature:<20} {vif:>10.2f} {interp:>25}\")\n",
    "\n",
    "print(f\"\\n✓ All VIFs < 5: No severe multicollinearity in creature features!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare to our synthetic collinear data\n",
    "print(\"\\nVIF for Synthetic Data (crew_size, supplies, weather):\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "vifs_synthetic = calculate_vif(X)\n",
    "synthetic_features = ['crew_size', 'supplies', 'weather_score']\n",
    "\n",
    "print(f\"\\n{'Feature':<15} {'VIF':>10}\")\n",
    "print(\"-\"*30)\n",
    "for feature, vif in zip(synthetic_features, vifs_synthetic):\n",
    "    flag = \"⚠️ COLLINEAR!\" if vif > 10 else \"\"\n",
    "    print(f\"{feature:<15} {vif:>10.2f}  {flag}\")\n",
    "\n",
    "print(f\"\\n⚠️ crew_size and supplies both have VIF > 10!\")\n",
    "print(f\"   This confirms the near-perfect collinearity we created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "| Concept | Key Insight | Densworld Example |\n",
    "|---------|-------------|-------------------|\n",
    "| **Linear Independence** | Can't express one vector from others | East and North directions |\n",
    "| **Linear Dependence** | Redundant information | supplies ≈ 2 × crew_size |\n",
    "| **Matrix Rank** | Count of independent columns | 3 school columns → rank 2 |\n",
    "| **SVD** | Reveals true dimensionality | Singular values show real dims |\n",
    "| **Multicollinearity** | Correlated features break regression | Unstable coefficients |\n",
    "| **VIF** | Standard diagnostic for collinearity | VIF > 10 = problem |\n",
    "\n",
    "---\n",
    "\n",
    "## Exercises\n",
    "\n",
    "### Exercise 1: Habitat Feature Independence\n",
    "\n",
    "Calculate the rank and VIF for the creature habitat features. Are there any hidden dependencies?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Your code here\n",
    "# habitat_features = ['depth_preference', 'moisture_preference', 'light_tolerance', \n",
    "#                     'cave_affinity', 'surface_affinity']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-24",
   "metadata": {},
   "source": [
    "### Exercise 2: Creating Perfect Collinearity\n",
    "\n",
    "Create a new feature that is an exact linear combination of existing behavioral features (e.g., `threat_score = aggression + territoriality`). Add it to the feature matrix and verify that the rank doesn't increase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-26",
   "metadata": {},
   "source": [
    "### Exercise 3: Dimensionality Reduction Preview\n",
    "\n",
    "Using SVD, project the 5 behavioral features down to 2 dimensions (keeping only the first 2 principal components). Plot all creatures in this 2D space. Do similar creatures cluster together?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Your code here\n",
    "# Hint: U[:, :2] @ np.diag(S[:2]) gives the 2D coordinates\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-28",
   "metadata": {},
   "source": [
    "### Exercise 4: Fixing Multicollinearity\n",
    "\n",
    "In the synthetic expedition data, we have collinear features (crew_size and supplies). Try these fixes:\n",
    "1. Remove one of the collinear features\n",
    "2. Create a new feature: ratio = supplies / crew_size\n",
    "\n",
    "Verify that VIF decreases and regression coefficients become stable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4: Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-30",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Module 2 Complete!\n",
    "\n",
    "You've completed the Linear Algebra module. You now understand:\n",
    "\n",
    "1. **Vectors** as both data records and points in space\n",
    "2. **Norms** for measuring size and distance (L1, L2)\n",
    "3. **Dot products** for measuring alignment and similarity\n",
    "4. **Matrix transformations** as functions on vectors\n",
    "5. **Rank and independence** for detecting redundancy\n",
    "\n",
    "These concepts are the foundation of:\n",
    "- Principal Component Analysis (PCA)\n",
    "- Neural network layers\n",
    "- Regularization (Ridge and Lasso)\n",
    "- Recommendation systems\n",
    "- And much more!\n",
    "\n",
    "*\"The Archives are built on vectors. Every creature, every manuscript, every expedition—all are points in vast spaces. To understand these spaces is to understand the hidden structure of the world.\"*  \n",
    "— Boffa Trent"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
